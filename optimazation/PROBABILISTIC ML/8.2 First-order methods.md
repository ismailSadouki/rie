All of these algorithms require that the user specify a starting point $θ_0$ . Then at each iteration t, they perform an update of the following form:
![](https://i.imgur.com/5s2FPUn.png)
where $η_t$ is known as the step size or learning rate, and dt is a descent direction, such as the negative of the gradient, given by $g_t = ∇_θ L(θ)|_{θ_t}$ .These update steps are continued until the method reaches a stationary point, where the gradient is zero
## 8.2.1 Descent direction
We say that a direction d is a descent direction if there is a small enough (but nonzero) amount η we can move in direction d and be guaranteed to decrease the function value. Formally, we require that there exists an $η_{max} > 0$ such that
![](https://i.imgur.com/EknkTyC.png)
![](https://i.imgur.com/yBlcQt4.png)
points in the direction of maximal increase in f , so the negative gradient is a descent direction.
It can be shown that any direction d is also a descent direction if the angle θ between d and $−g_t$ is less than 90 degrees and satisfies
![](https://i.imgur.com/uZmihg6.png)
It seems that the best choice would be to pick $d_t = −g_t$ . This is known as the direction of steepest descent. However, this can be quite slow. We consider faster versions later
# 8.2.2 Step size (learning rate)
In machine learning, the sequence of step sizes ${η_t }$ is called the learning rate schedule.
## 8.2.2.1 Constant step size
The simplest method is to use a constant step size, ηt = η. However, if it is too large, the method may fail to converge, and if it is too small, the method will converge but very slowly.
![](https://i.imgur.com/RBaEw5n.png)

![](https://i.imgur.com/46dUVOp.png)
The intuitive reason for this can be understood by thinking of a ball rolling down a valley. We want to make sure it doesn’t take a step that is larger than the slope of the steepest direction, which is what the largest eigenvalue measures
More generally, setting η < 2/L, where L is the Lipschitz constant of the gradient (Section 8.1.4), ensures convergence. Since this constant is generally unknown, we usually need to adapt the step size, as we discuss below.

## 8.2.2.2 Line search
The optimal step size can be found by finding the value that maximally decreases the objective along the chosen direction by solving the 1d minimization problem
![](https://i.imgur.com/xEqFq7P.png)
This is known as line search, since we are searching along the line defined by $d_t$ .
If the loss is convex, this subproblem is also convex, because $φ_t (η) = L(θ_t + ηd_t )$ is a convex function of an affine function of η, for fixed $θ_t$ and $d_t$ .
For example, consider the quadratic loss
![](https://i.imgur.com/yjVekrp.png)
Using the optimal step size is known as **exact line search**.
However, it is not usually necessary to be so precise.
such as the Armijo backtracking method, that try to ensure sufficient reduction in the objective function without spending too much time trying to solve Equation (8.21).
In particular, we can start with the current stepsize (or some maximum value),
and then reduce it by a factor 0 < β < 1 at each step until we satisfy the following condition, known as the Armijo-Goldstein test:
![](https://i.imgur.com/lFeHPhM.png)
![](https://i.imgur.com/t136iXW.png)

# 8.2.3 Convergence rates
We want to find optimization algorithms that converge quickly to a (local) optimum.For certain convex problems, with a gradient with bounded Lipschitz constant, one can show that gradient descent converges at a linear rate. This means that there exists a number 0 < µ < 1 such that
![](https://i.imgur.com/peCYPPu.png)
![](https://i.imgur.com/hEQiXUW.png)
![](https://i.imgur.com/kBf5LGH.png)
In the more general case of non-quadratic functions, the objective will often be locally quadratic around a local optimum.Hence the convergence rate depends on the condition number of the Hessian $κ(H)$, at that point.We can often improve the convergence speed by optimizing a surrogate objective (or model) at each step which has a Hessian that is close to the Hessian of the objective function as
we discuss in Section 8.3.
Although line search works well, we see from Figure 8.12 that the path of steepest descent with an exact line-search exhibits a characteristic zig-zag behavior, which is inefficient. This problem can be overcome using a method called conjugate gradient descent

## 8.2.4 Momentum methods
One simple heuristic, is to move faster along directions that were previously good, and to slow down along directions where the gradient has suddenly changed, just like a ball rolling downhill. This can be implemented as follows:
![](https://i.imgur.com/bJ6VjJ0.png)

where mt is the momentum (mass times velocity) and 0 < β < 1. A typical value of β is 0.9. For β = 0, the method reduces to gradient descent.
We see that mt is like an exponentially weighted moving average of the past gradients
![](https://i.imgur.com/lwhtYG4.png)
If all the past gradients are a constant, say g, this simplifies to
![](https://i.imgur.com/DKcwNwh.png)
![](https://i.imgur.com/uwPBe0E.png)
Thus in the limit, we multiply the gradient by 1/(1 − β). For example, if β = 0.9, we scale the gradient up by 10
Since we update the parameters using the gradient average mt−1 , rather than just the most recent gradient, gt−1 , we see that past gradients can exhibit some influence on the present. Furthermore, when momentum is combined with SGD, discussed in Section 8.4, we will see that it can simulate the effects of a larger minibatch, without the computational cost.

## 8.2.4.2 Nesterov momentum
One problem with the standard momentum method is that it may not slow down enough at the bottom of a valley, causing oscillation.
The Nesterov accelerated gradient method instead modifies the gradient descent to include an extrapolation step, as follows:
![](https://i.imgur.com/UoVljmX.png)
This is essentially a form of one-step “look ahead”, that can reduce the amount of oscillation
![](https://i.imgur.com/4i3rTCc.png)

Nesterov accelerated gradient can also be rewritten in the same format as standard momentum. In this case, the momentum term is updated using the gradient at the predicted new location,
![](https://i.imgur.com/y6GQxnm.png)
It also shows how this method can be faster than standard momentum: the momentum vector is already roughly pointing in the right direction, so measuring the gradient at the new location, θt + βmt , rather than the current location, θt , can be more accurate.
The Nesterov accelerated gradient method is provably faster than steepest descent for convex functions when β and ηt are chosen appropriately. It is called “accelerated” because of this improved convergence rate, which is optimal for gradient-based methods using only first-order information
when the objective function is convex and has Lipschitz-continuous gradients. In practice, however, using Nesterov momentum can be slower than steepest descent, and can even unstable if β or ηt are misspecified.