we consider stochastic optimization, where the goal is to minimize the average
value of a function:
![](https://i.imgur.com/5WavWqO.png)
where z is a random input to the objective. This could be a “noise” term, coming from the environment, or it could be a training example drawn randomly from the training set.

At each iteration, we assume we observe $L_t(θ)=L(θ, z_t)$ , where $z_t ∼ q$. We also assume a way to compute an unbiased estimate of the gradient of L. If the distribution q(z) is independent of the parameters we are optimizing, we can use $g_t = ∇_θ L_t (θ_t )$. In this case, The resulting algorithm can be written as follows:
![](https://i.imgur.com/YVrusmK.png)
This method is known as stochastic gradient descent or SGD. As long as the gradient estimate is unbiased, then this method will converge to a stationary point, providing we decay the step size ηt at a certain rate

## 8.4.1 Application to finite sum problems
## 8.4.2 Example: SGD for fitting linear regression
the objective has the form
![](https://i.imgur.com/8coZ6Y7.png)
![](https://i.imgur.com/8spMoR6.png)
Now consider using SGD with a minibatch size of B = 1. The update becomes
![](https://i.imgur.com/oZMCIYV.png)
where n = n(t) is the index of the example chosen at iteration t.
The overall algorithm is called the least mean squares (LMS) algorithm, and is also known as the delta rule, or the Widrow-Hoff rule
![](https://i.imgur.com/VUnELHq.png)
Note that SGD (and hence LMS) may require multiple passes through the
data to find the optimum.

# 8.4.3 Choosing the step size (learning rate)
![](https://i.imgur.com/Yx2CCNN.png)
We see a U-shaped curve, where an overly small learning rate results in underfitting, and overly large learning rate results in instability of the model.

One heuristic for choosing a good learning rate, is to start with a small
learning rate and gradually increase it, evaluating performance using a small number of minibatches. We then make a plot like the one in Figure 8.17, and pick the learning rate with the lowest loss.(In practice, it is better to pick a rate that is slightly smaller than (i.e., to the left of) the one with the lowest loss, to ensure stability).

Rather than choosing a single constant learning rate, we can use a **learning rate schedule**, in which we adjust the step size over time, Theoretically, a sufficient condition for SGD to achieve convergence is if the learning rate schedule satisfies the Robbins-Monro conditions:
![](https://i.imgur.com/92IH6MI.png)
![](https://i.imgur.com/F4nUKoQ.png)
In the deep learning community, another common schedule is to quickly increase the learning rate and then gradually decrease it again, as shown in Figure 8.19a.
![](https://i.imgur.com/9dCNj6M.png)
This is called learning rate warmup,or the one-cycle learning rate schedule.The motivation for this is the following: initially the parameters may be in a part of the loss landscape that is poorly conditioned, so a large step size will “bounce around” too much (c.f., Figure 8.11(b)) and fail to make progress downhill. However, with a slow learning rate, the algorithm can discover flatter regions of space, where a larger step size can be used. Once there, fast progress can be made. However, to ensure convergence to a point, we must reduce the learning rate to 0.

It is also possible to increase and decrease the learning rate multiple times, in a cyclical fashion. This is called a cyclical learning rate.See
Figure 8.19b for an illustration using triangular shapes.The motivation behind this approach is to escape local minima. The minimum and maximum learning rates can be found based on the initial “dry run” described above, and the half-cycle can be chosen based on how many restarts you want to do with your training budget. 

An alternative to using heuristics for estimating the learning rate is to use line search, This is tricky when using SGD, because the noisy gradients make the computation of the Armijo condition difficult However, [Vas+19] show that it can be made to work if the variance of the gradient noise goes to zero over time. This can happen if the model is sufficiently flexible that it can perfectly interpolate the training set.

# 8.4.4 Iterate averaging
The parameter estimates produced by SGD can be very unstable over time. To reduce the variance of the estimate, we can compute the average using
![](https://i.imgur.com/aXIZtA2.png)
where θt are the usual SGD iterates. This is called iterate averaging or Polyak-Ruppert averaging
they prove that the estimate $\bar{θ_t}$ achieves the best possible asymptotic convergence rate among SGD algorithms, matching that of variants using second-order information, such as Hessians. 
This averaging can also have statistical benefits. For example, in [NR18], they prove that, in the case of linear regression, this method is equivalent to $l_2$ regularization (i.e., ridge regression).
Rather than an exponential moving average of SGD iterates, Stochastic Weight Averaging (SWA) [Izm+18] uses an equal average in conjunction with a modified learning rate schedule. In contrast to standard Polyak-Ruppert averaging, which was motivated for faster convergence rates, SWA exploits the flatness in objectives used to train deep neural networks, to find solutions which provide better generalization.

# 8.4.5 Variance reduction *
we discuss various ways to reduce the variance in SGD. In some cases, this can
improve the theoretical convergence rate from sublinear to linear (i.e., the same as full-batch gradient descent)
These methods reduce the variance of the gradients, rather than
the parameters themselves and are designed to work for finite sum problems.

## 8.4.5.1 SVRG
The basic idea of stochastic variance reduced gradient (SVRG) is to use a control
variate, in which we estimate a baseline value of the gradient based on the full batch, which we then use to compare the stochastic gradients to.
More precisely, ever so often (e.g., once per epoch), we compute the full gradient at a “snapshot” of the model parameters $\tilde{θ}$; the corresponding “exact” gradient is therefore $∇L(\tildeθ)$. At step t, we compute the usual stochastic gradient at the current parameters, $∇L_t (θ_t )$, but also at the snapshot paramete
![](https://i.imgur.com/3Saedm6.png)
involves two gradient computations, since we can compute ∇L(θ̃) once per epoch. At the end of the epoch, we update the snapshot parameters, $\tilde{θ̃}$, based on the most recent value of θt , or a running average of the iterates, and update the expected baseline. (We can compute snapshots less often, but then the baseline will not be correlated with the objective and can hurt performance, as shown in
[DB18].)
Iterations of SVRG are computationally faster than those of full-batch GD, but SVRG can still match the theoretical convergence rate of GD.
## 8.4.5.2 SAGA
![](https://i.imgur.com/rOzTfxu.png)
## 8.4.5.3 Application to deep learning
<mark> Note completed</mark>
# 8.4.6 Preconditioned SGD
## 8.4.6.1 AdaGrad
## 8.4.6.2 RMSProp and AdaDelta
## 8.4.6.3 Adam
## 8.4.6.4 Issues with adaptive learning rates
## 8.4.6.5 Non-diagonal preconditioning matrices
