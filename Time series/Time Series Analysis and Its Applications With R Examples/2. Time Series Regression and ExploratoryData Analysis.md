![](https://i.imgur.com/4rnddTI.png)
![](https://i.imgur.com/XYo1LDb.png)
![](https://i.imgur.com/Z2lIqVf.png)
These results are often summarized in an Analysis of Variance (ANOVA) table as
given in Table 2.1 for this particular case.
![](https://i.imgur.com/9RAcEk2.png)
The null hypothesis is rejected at level α if $F > F_{n−q−1}^{q-r} (α)$, the 1 − α percentile of the F distribution with q − r numerator and n − q − 1 denominator degrees of freedom.
A special case of interest is the null hypothesis H0 : β1 = · · · = βq = 0. In this
case r = 0, and the model in (2.10) becomes $$ x_t=\beta_0+w_t$$
![](https://i.imgur.com/tKTP3tu.png)
In this case SSE0 is the sum of squared deviations from the mean x̄ and is otherwise known as the adjusted total sum of squares. The measure R2 is called the coeﬃcient of determination.


Suppose we consider a normal regression model with k coeﬃcients and denote the maximum likelihood estimator for the variance as
$$σ̂_k^2=SSE(k)/n \;(2.14)$$
where SSE(k) denotes the residual sum of squares under the model with k regression coeﬃcients. Then, Akaike [1–3] suggested measuring the goodness of ﬁt for this particular model by balancing the error of the ﬁt against the number of parameters in the model; we deﬁne the following.
![](https://i.imgur.com/310rXKC.png)
The value of k yielding the minimum AIC speciﬁes the best model.The idea is
roughly that minimizing σ̂k2 would be a reasonable objective, except that it decreases monotonically as k increases. Therefore, we ought to penalize the error variance by a term proportional to the number of parameters.
The corrected form is deﬁned as follows.
![](https://i.imgur.com/zbWyDn3.png)
where σ̂k2 is given by (2.14), k is the number of parameters in the model, and n is
the sample size.
![](https://i.imgur.com/JBksibH.png)
Notice that the penalty term in BIC is much larger than in AIC,
consequently, BIC tends to choose smaller models. Various simulation studies have tended to verify that BIC does well at getting the correct order in large samples,whereas AICc tends to be superior in smaller samples where the relative number of parameters is large; 

# 2.2 Exploratory Data Analysis
Perhaps the easiest form of nonstationarity to work with is the trend stationary
model wherein the process has stationary behavior around a trend. We may write this type of model as
$$
x_t=\mu_t+y_t\;\;(2.24)
$$
where xt are the observations, μt denotes the trend, and yt is a stationary process.
Quite often, strong trend will obscure the behavior of the stationary process, yt , as we shall see in numerous examples. Hence, there is some advantage to removing the trend as a ﬁrst step in an exploratory analysis of such time series. 
The steps involved are to obtain a reasonable estimate of the trend component, say μ̂t , and then work with the residuals
$$ ŷ_t = x_t − μ̂_t .$$

One advantage of diﬀerencing over detrending to remove trend is that no param-
eters are estimated in the diﬀerencing operation. One disadvantage, however, is that diﬀerencing does not yield an estimate of the stationary process yt.
If an estimate of yt is essential, then detrending may be more appropriate. If
the goal is to coerce the data to stationarity, then diﬀerencing may be more appropriate.

Diﬀerencing is also a viable tool if the trend is ﬁxed

![](https://i.imgur.com/pOP69nO.png)
