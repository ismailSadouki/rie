### Example 3.10 The ACF of an AR(2) Process
![](https://i.imgur.com/9ek593B.png)
### **Many Models Naturally Have Zero Mean**

- **AR (AutoRegressive) Models**: If an AR process has a constant term, we can **remove it by centering** the data.
    
- **White Noise**: The noise term wt∼N(0,σ2)w_t \sim N(0, \sigma^2)wt​∼N(0,σ2) in many models is assumed to have zero mean by default.
    
- **Differenced Data**: When we difference a time series to remove trends, the result often has a mean close to zero.

Using the results for the homogeneous diﬀerence equation of order two, let z1
and z2 be the roots of the associated polynomial, $φ(z) = 1 − φ_1 z − φ_2 z^2$ . Because the model is causal, we know the roots are outside the unit circle: |z1 | > 1 and |z2 | > 1. Now, consider the solution for three cases:
![](https://i.imgur.com/U2VSjGL.png)
![](https://i.imgur.com/RCR0R3B.png)


### Example 3.12 The ψ-weights for an ARMA Model
In time series analysis, **ψ-weights** describe how past white noise terms influence the present value in an **ARMA** (AutoRegressive Moving Average) model. These weights help express the ARMA process in an **infinite moving average (MA) representation**, which is useful for understanding impulse responses, forecasting, and model stability.
- ψ-weights express an **ARMA(p, q)** model as an **infinite MA process**.
    
- They help understand the impact of **past shocks**.
    
- They are computed using recursive formulas and are crucial for **model analysis and forecasting**.

To solve for the ψ-weights in general, we must match the coeﬃcients in $φ(z)ψ(z) = θ(z)$:
![](https://i.imgur.com/YIRkEv8.png)


# 3.3 Autocorrelation and Partial Autocorrelation
![](https://i.imgur.com/NZt3A0Z.png)
![](https://i.imgur.com/cOantGI.png)
![](https://i.imgur.com/i7HtjDw.png)


### Example 3.13 The ACF of an AR(p)
![](https://i.imgur.com/rU5aUXW.png)
Recall that for a causal model, all of the roots are outside the unit circle, |zi | > 1,
for i = 1, . . . , r. If all the roots are real, then ρ(h) dampens exponentially fast to
zero as h → ∞. If some of the roots are complex, then they will be in conjugate
pairs and ρ(h) will dampen, in a sinusoidal fashion, exponentially fast to zero as
h → ∞. In the case of complex roots, the time series will appear to be cyclic in
nature. This, of course, is also true for ARMA models in which the AR part has
complex roots.

### Example 3.14 The ACF of an ARMA(1, 1)
![](https://i.imgur.com/edP7Ie2.png)
Notice that the general pattern of ρ(h) versus h in (3.52) is not diﬀerent from
that of an AR(1) given in (3.8). Hence, it is unlikely that we will be able to tell the
diﬀerence between an ARMA(1,1) and an AR(1) based solely on an ACF estimated
from a sample. This consideration will lead us to the partial autocorrelation function.

# The Partial Autocorrelation Function (PACF)
Recall that if X, Y , and Z are random variables, then the partial correlation
between X and Y given Z is obtained by regressing X on Z to obtain X̂, regressing Y on Z to obtain Ŷ, and then calculating
$$ρ_{XY |Z} = corr\{X − \hat{X}, Y − \hat{Y} \}.$$
measures the correlation between X and Y with the linear
eﬀect of Z removed (or partialled out).
If the variables are multivariate normal, then 
this deﬁnition coincides with $ρ_{XY |Z} = corr(X, Y | Z)$.
the tool we need is partial autocorrelation, which is the correlation between
xs and xt with the linear eﬀect of everything “in the middle” removed.
![](https://i.imgur.com/YR7ilqB.png)
the linear dependence of {xt+1, . . . , xt+h−1 } on each, removed.
If the process xt is Gaussian, then $φ_{hh} = corr(x_{t+h}, x_t | x_{t+1}, . . . , x_{t+h−1} )$; that is, $φ_{hh}$ is the correlation coeﬃcient between $x_{t+h}$ and $x_t$ in the bivariate distribution of $(x_{t+h}, x_t )$ conditional on {xt+1, . . . , xt+h−1 }.recall equa-
tion (3.54). When h ≤ p, φ pp is not zero, and φ11, . . . , φ p−1, p−1 are not necessarily zero.


![](https://i.imgur.com/V3f1ZLx.png)
![](https://i.imgur.com/t3mOsBB.png)
because, by causality, xt − x̂t depends only on {wt+h−1, wt+h−2, . . .}

### Example 3.17 The PACF of an Invertible MA(q)
![](https://i.imgur.com/AaO5ROS.png)
In the next section, we will discuss methods of calculating the PACF. The PACF
for MA models behaves much like the ACF for AR models. Also, the PACF for AR
models behaves much like the ACF for MA models. Because an invertible ARMA
model has an inﬁnite AR representation, the PACF will not cut oﬀ. We may summarize these results in Table 3.1.
![](https://i.imgur.com/EujZi51.png)
