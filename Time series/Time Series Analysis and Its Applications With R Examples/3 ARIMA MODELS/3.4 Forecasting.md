Throughout this section, we will assume xt is stationary and the model parameters are known.
The minimum mean square error predictor of $x_{n+m}$ is

![](https://i.imgur.com/hdMG8ti.png)
because the conditional expectation minimizes the mean square error
![](https://i.imgur.com/IqThjI7.png)
where g(x1:n ) is a function of the observations x1:n
First, we will restrict attention to predictors that are linear functions of the data,
that is, predictors of the form
![](https://i.imgur.com/zbGTJ8i.png)
We note that the αs depend on n and m, but for now we drop the dependence from the notation.
![](https://i.imgur.com/ZddmI1v.png)
Linear predictors of the form (3.59) that minimize the mean square prediction
error (3.58) are called best linear predictors (BLPs).
linear prediction depends only on the second-order moments of the process, which are easy to estimate from the data.
![](https://i.imgur.com/N7If6DP.png)
The equations speciﬁed in (3.60) are called the prediction equations, and they
are used to solve for the coeﬃcients {α0, α1, . . . , αn }
The results of Property 3.3 can also be obtained via least squares; i.e., to minimize $Q = E(x_{n+m} − \sum_{k=0}^n\alpha_kx_k)^2$ with respect to the αs

![](https://i.imgur.com/Ig4Zm5G.png)

First, consider one-step-ahead prediction. That is, given {x1, . . . , xn }, we wish to
forecast the value of the time series at the next time point, xn+1 . The BLP of xn+1 is of the form
![](https://i.imgur.com/ZpuviJT.png)
where we now display the dependence of the coeﬃcients on n;
where we now display the dependence of the coeﬃcients on n; in this case, αk
in (3.59) is $φ_{n,n+1−k}$ in (3.61), for k = 1, . . . , n. Using Property 3.3, the coeﬃcients
{φn1, φn2, . . . , φnn } satisfy
![](https://i.imgur.com/nCjhjcP.png)
![](https://i.imgur.com/IZORVHs.png)
For ARMA models, the fact that σw2 > 0 and γ(h) → 0 as h → ∞ is enough to
ensure that Γn is positive deﬁnite
It is sometimes convenient to write the one-step-ahead forecast in vector notation
![](https://i.imgur.com/55v3aK1.png)
![](https://i.imgur.com/4bVXmgK.png)



### Example 3.19 Prediction for an AR(2)
![](https://i.imgur.com/zywWZvH.png)

From Example 3.19, it should be clear that if the time series is a
causal AR(p) process, then, for n ≥ p,
![](https://i.imgur.com/DWV2a1J.png)

## Property 3.4 The Durbin–Levinson Algorithm
Equations (3.64) and (3.66) can be solved iteratively as follows:
![](https://i.imgur.com/Tmp67Y1.png)
![](https://i.imgur.com/AdVOsDp.png)
### Example 3.20 Using the Durbin–Levinson Algorithm
![](https://i.imgur.com/4Ld3J0a.png)

## Property 3.5 Iterative Solution for the PACF

![](https://i.imgur.com/0VtdwAF.png)


### Example 3.21 The PACF of an AR(2)
![](https://i.imgur.com/HErNUzO.png)

![](https://i.imgur.com/9wnQr0F.png)


## Property 3.6 The Innovations Algorithm
![](https://i.imgur.com/GxVP6Xk.png)


![](https://i.imgur.com/1HzHwo8.png)
![](https://i.imgur.com/QlRAuAw.png)
![](https://i.imgur.com/N9y264L.png)
## Example 3.23 Long-Range Forecasts
![](https://i.imgur.com/KS8j60o.png)
It should be clear from (3.89) and (3.90) that ARMA forecasts quickly settle to
the mean with a constant prediction error as the forecast horizon, m, grows.

When n is small, the general prediction equations (3.60) can be used easily.
When n is large, we would use (3.85) by truncating, because we do not observe
![](https://i.imgur.com/ZB3H120.png)
![](https://i.imgur.com/Ct3AzBw.png)

## Property 3.7 Truncated Prediction for ARMA
![](https://i.imgur.com/LOq8jut.png)
