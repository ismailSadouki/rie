
#### Example 1.11 Random Walk with Drift
$$xt = δ + x(t−1) + wt\;\;(1.3)$$
when δ = 0, (1.3) is called simply a random walk.
Note that we may rewrite (1.3) as a cumulative sum of white noise variates. That is
$$x_t=\delta t+\sum_{j=1}^t{w_j}$$
#### Example 1.12 Signal in Noise
Many realistic models for generating time series assume an underlying signal with
some consistent periodic variation, contaminated by adding a random noise.
$$x_t=2cos(2\Pi \frac{t+15}{50})+w_t$$
for t = 1, 2, . . . , 500, where the ﬁrst term is regarded as the signal, shown in the
upper panel of Fig. 1.11. We note that a sinusoidal waveform can be written as
$$Acos(2\Pi ωt + φ)$$
where A is the amplitude, ω is the frequency of oscillation, and φ is a phase shift.
In (1.5), A = 2, ω = 1/50 (one cycle every 50 time points), and φ = 2π15/50 = .6π.
An additive noise term was taken to be white noise with $σ_w$ = 1 (middle panel)
and $σ_w$ =5
![](https://i.imgur.com/QuppHQL.png)
Of course, the degree to which the signal is obscured depends on the amplitude of the signal and the size of σw . The ratio of the amplitude of the signal to σw (or some function of the ratio) is sometimes called the signal-to-noise ratio (SNR); the larger the SNR, the easier it is to detect the signal.


# 1.3 Measures of Dependence
![](https://i.imgur.com/u6tcTql.png)
The autocovariance measures the linear dependence between two points on the
same series observed at diﬀerent times.Very smooth series exhibit 
functions that stay large even when the t and s are far apart, whereas choppy series tend to have autocovariance functions that are nearly zero for large separations
Recall from classical statistics that if γx (s, t) = 0, xs and xt are not linearly related, but there still may be some dependence structure between them.
If, however, xs and xt are bivariate normal, γx (s, t) = 0 ensures their independence.

![](https://i.imgur.com/dqdEgYu.png)

![](https://i.imgur.com/nhtkukG.png)
Note that, the autocovariance function of a random walk depends on the
particular time values s and t, and not on the time separation or lag.

![](https://i.imgur.com/Au29jnv.png)
−1 ≤ ρ(s, t) ≤ 1 The ACF measures the linear predictability of the series at time t, say xt , using only the value xs .


<mark>The Cauchy–Schwarz inequality implies |γ(s, t) | 2 ≤ γ(s, s)γ(t, t).</mark>
Often, we would like to measure the predictability of another series yt from the
series xs . Assuming both series have ﬁnite variances, we have the following deﬁnition.
![](https://i.imgur.com/waC20ze.png)


# 1.4 Stationary Time Series
![](https://i.imgur.com/zuSyI7f.png)
![](https://i.imgur.com/6CEtwkP.png)
One important case where stationarity implies strict stationarity
is if the time series is Gaussian
![](https://i.imgur.com/KclWFw5.png)
because the time diﬀerence between times t + h and t is the same as the time diﬀerence between times h and 0. Thus, the autocovariance function of a stationary time series does not depend on the time argument t. Henceforth, for convenience, we will drop the second argument of γ(h, 0).
![](https://i.imgur.com/xn35yEi.png)
![](https://i.imgur.com/1gr8yAh.png)
#### Example 1.22 Trend Stationarity
![](https://i.imgur.com/ou2xQap.png)




The autocovariance function of a stationary process has several special properties. First, γ(h) is non-negative deﬁnite (see Problem 1.25) ensuring that variances of linear combinations of the variates $x_t$ will never be negative. That is, for any n ≥ 1, and constants a1, . . . , an ,
![](https://i.imgur.com/PH7D1Cm.png)
Also, the value at h = 0, namely $$γ(0) = E[(x_t − μ)^2 ]$$
is the variance of the time series and the Cauchy–Schwarz inequality implies
$$|γ(h)| ≤ γ(0).$$
the autocovariance function of a stationary series is symmetric around the origin; that is,
$$γ(h) = γ(−h)$$
![](https://i.imgur.com/VBFktco.png)
−1 ≤ ρxy (h) ≤ 1, The cross-correlation function is not generally symmetric about zero This is an important concept; it should be clear that cov(x2, y1 )
and cov(x1, y2 ) need not be the same. It is the case, however, that
![](https://i.imgur.com/pX6ZLG4.png)
![](https://i.imgur.com/S5Cm9UC.png)



![](https://i.imgur.com/2baZ4lk.png)
Notice that the linear process (1.31) is dependent on the future ( j < 0), the present ( j = 0), and the past ( j > 0). For the purpose of forecasting, a future dependent model will be useless. Consequently, we will focus on processes that do not depend on the future. Such models are called causal, and a causal linear process has ψ j = 0 for j < 0


![](https://i.imgur.com/D0JoVFQ.png)


# 1.5 Estimation of Correlation
if a time series is stationary, the mean function (1.22) μt = μ is
constant so that we can estimate it by the sample mean,
![](https://i.imgur.com/aYJfRwg.png)
In our case, E( x̄) = μ, and the standard error of the estimate is the square root of
var( x̄), which can be computed using ﬁrst principles (recall Chap. 1.1), and is given by
![](https://i.imgur.com/HJCz1ZW.png)
If the process is white noise, (1.35) reduces to the familiar σx2 /n recalling that
$γ_x (0) = σ_x^2$

The theoretical autocovariance function, (1.23), is estimated by the sample auto-
covariance function deﬁned as follows.
![](https://i.imgur.com/aYEyObP.png)
The sum in (1.36) runs over a restricted range because xt+h is not available for
t + h > n.
The estimator in (1.36) is preferred to the one that would be obtained by
dividing by n − h because (1.36) is a non-negative deﬁnite function.And because a variance is never negative, the estimate of that variance
![](https://i.imgur.com/Nd3Dc7f.png)
should also be non-negative.The estimator in (1.36) guarantees this result, but no
such guarantee exists if we divide by n − h.

![](https://i.imgur.com/BoVa6wl.png)
![](https://i.imgur.com/xqdcfNo.png)
Based on the previous result, we obtain a rough method of assessing whether peaks in ρ̂(h) √ are signiﬁcant by determining whether the observed peak is outside the interval $±2/\sqrt(n)$ ; for a white noise sequence, approximately
95% of the sample ACFs should be within these limits.

![](https://i.imgur.com/Zx51DLD.png)
![](https://i.imgur.com/KbCrPsa.png)
