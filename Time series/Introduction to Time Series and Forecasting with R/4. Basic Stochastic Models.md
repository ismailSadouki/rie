
So far, we have considered two approaches for modelling time series. The
first is based on an assumption that there is a fixed seasonal pattern about a
trend. We can estimate the trend by local averaging of the deseasonalised data,
and this is implemented by the R function decompose. The second approach
allows the seasonal variation and trend, described in terms of a level and slope,
to change over time and estimates these features by exponentially weighted
averages. We used the HoltWinters function to demonstrate this method.

### 4.2 White noise
If we suppose the model is defined for the variable $y_t$ and $ŷ_t$ is the value predicted by the model, the residual error $x_t$ is
$$
x_t=y_t + \hat{y_t}
$$
As the residual errors occur in time, they form a time series: x1 , x2 , . . . , xn .
if a model has accounted for all the serial correlation in the data, the residual series would be serially uncorrelated, so that a correlogram of the residual series would exhibit no obvious patterns. This ideal motivates the following definition
![](https://i.imgur.com/xowZwtI.png)

#### 4.2.4 Second-order properties and the correlogram
![](https://i.imgur.com/KPOG1Dc.png)

NOTE: **Simulated** white noise data will not have autocorrelations that are exactly
zero (when k 6= 0) because of sampling variation, for a simu-
lated white noise series, it is expected that 5% of the autocorrelations will
be significantly different from zero at the 5% significance level,
![](https://i.imgur.com/3wfEoPn.png)

#### 4.2.5 Fitting a white noise model
A white noise series usually arises as a residual series after fitting an appropri-
ate time series model.


# 4.3 Random walks

A random walk often provides a good fit to data with
stochastic trends, although even better fits are usually obtained from more
general model formulations, such as the ARIMA models

![](https://i.imgur.com/cHOo5TS.png)

## 4.3.4 Random walk: Second-order properties
![](https://i.imgur.com/LeGcJts.png)
The covariance is a function of time, so the process is non-stationary.In par-
ticular, the variance is $tσ^2$ and so it increases without limit as t increases. It follows that <mark>a random walk is only suitable for short term predictions</mark>.

The time-varying autocorrelation function for k > 0 follows:
![](https://i.imgur.com/45WqZ0N.png)
so that, for large t with k considerably less than t, ρk is nearly 1. Hence, <mark>the correlogram for a random walk is characterised by positive autocorrelations that decay very slowly down from unity.</mark>

## 4.3.5 Derivation of second-order properties*
![](https://i.imgur.com/ySp44Hh.png)

# 4.4 Fitted models and diagnostic plots
The first-order differences of a random walk are a white noise series, so the
correlogram of the series of differences can be used to assess whether a given
series is reasonably modelled as a random walk.
	`>  acf(diff(x))`

## 4.4.3 Random walk with drift
Company stockholders generally expect their investment to increase in value
despite the volatility of financial markets. The random walk model can be
adapted to allow for this by including a drift parameter δ:
$$
x_t = x_{t−1} + δ + w_t
$$

# 4.5 Autoregressive models
![](https://i.imgur.com/8tR4aoA.png)
![](https://i.imgur.com/1OMPmqk.png)

## 4.5.2 Stationary and non-stationary AR processes
The equation $θ_p (B) = 0$, where B is formally treated as a number (real or
complex), is called the characteristic equation. The roots of the characteristic
equation (i.e., the polynomial $θ_p (B)$ from Equation (4.16)) must all exceed
unity in absolute value for the process to be stationary.

## 4.5.3 Second-order properties of an AR(1) model
![](https://i.imgur.com/ELkAfxW.png)
## 4.5.4 Derivation of second-order properties for an AR(1) process*
![](https://i.imgur.com/70aB1Ka.png)




## 4.5.5 Correlogram of an AR(1) process
From Equation (4.19), the autocorrelation function follows as 
$$
ρ_k = α^ k
\;\;(k ≥ 0)\;\;\;\;(4.21)
$$
where |α| < 1. Thus, the correlogram decays to zero more rapidly for small α.
## 4.5.6 Partial autocorrelation
From Equation (4.21), the autocorrelations are non-zero for all lags even
though in the underlying model xt only depends on the previous value xt−1.
The partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.
**Difference Between Autocorrelation and Partial Autocorrelation**
- **Autocorrelation** at lag k measures the **total correlation** between $x_t$​ and $x_{t−k}$​, **including** any indirect correlations due to intermediate lags.
- **Partial Autocorrelation** at lag k isolates the **direct** correlation between $x_t$​ and $x_{t−k}$​, **removing** the effect of shorter lags $(1,2,...,k−1)$.
**Why is this useful?**
- Helps in identifying the **order of autoregressive (AR) processes** in time series models like **ARMA** or **ARIMA**.
- Shows which lags **directly** influence the current value, rather than through an indirect chain of dependencies.
**How to Interpret PACF in a Plot?**
- If **PACF drops to near zero after lag p**, it suggests an **AR(p)** model.
- If PACF **slowly declines**, it suggests a **moving average (MA) process**, not an AR process.

an AR(p) process has a correlogram of partial autocorrelations that is zero after lag p. Hence, a plot of the estimated partial autocorrelations can be useful when determining the order of a suitable AR process for a time series.

Note: pacf starts at lag 1, whilst the acf starts at lag 0 in corrologram

# 4.6 Fitted models
## 4.6.1 Model fitted to simulated series
> set.seed(1)
> x <- w <- rnorm(100)
> for (t in 2:100) x[t] <- 0.7 * x[t - 1] + w[t]
> plot(x, type = "l")
> acf(x)
> pacf(x)