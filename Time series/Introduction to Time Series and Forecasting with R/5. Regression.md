**Trends in time series can be classified as stochastic or deterministic.**

|Feature|Stochastic Trend|Deterministic Trend|
|---|---|---|
|**Predictability**|Unpredictable|Predictable|
|**Cause**|High serial correlation & randomness|Underlying physical/economic reason|
|**Examples**|Stock prices, exchange rates|Population growth, seasonal sales|
|**Modeling Approach**|Random walk, AR models|Regression (linear, polynomial, seasonal)|
|**Forecasting**|Risky, only short-term|More reliable, especially short-term|
In this chapter various regression models are studied that are suitable for
a time series analysis of data that contain deterministic trends and regular
seasonal changes.
Time series regression usually differs from a standard regression analysis
because the residuals form a time series and therefore tend to be serially cor-
related.When this correlation is positive, the estimated standard errors of
the parameter estimates, read from the computer output of a standard re-
gression analysis, will tend to be less than their true value.This will leadto erroneously high statistical significance being attributed to statistical tests
in standard computer output

# 5.2 Linear models
![](https://i.imgur.com/ivdgJAg.png)

## 5.2.2 Stationarity
Linear models for time series are non-stationary when they include functions
of time. Differencing can often transform a non-stationary series with a de-
terministic trend to a stationary series. For example
let $x_t = α_0 + α_1 t + z_t$ then the first order difference is $$∇x_t=z_t-z_{t-1}+\alpha_1$$
Assuming the error series $\{z_t \}$ is stationary, the series $\{∇x_t\}$ is stationary
as it is not a function of $t$.

differencing can remove both stochastic and deterministic trends from time series. If the underlying trend is a polynomial of order m, then mth-order differencing is required to remove the trend.
Notice that differencing the straight-line function plus white noise leads to
a different stationary time series than subtracting the trend. The latter gives
white noise, whereas differencing gives a series of consecutive white noise terms

## 5.2.3 Simulation
In time series regression, it is common for the error series {zt } in Equation
(5.1) to be autocorrelated. In the code below a time series with an increas-
ing straight-line trend (50 + 3t) with autocorrelated errors is simulated and
plotted:
> set.seed(1)
> z <- w <- rnorm(100, sd = 20)
> for (t in 2:100) z[t] <- 0.8 * z[t - 1] + w[t]
> Time <- 1:100
> x <- 50 + 3 * Time + z
> plot(x, xlab = "time", type = "l")

The model for the code above can be expressed as xt = 50 + 3t + zt , where
{zt } is the AR(1) process zt = 0.8zt−1 + wt and {wt } is Gaussian white noise
with σ = 20.

# 5.3 Fitted models
## 5.3.1 Model fitted to simulated data
![](https://i.imgur.com/shfu0CL.png)


After fitting a regression model, we should consider various diagnostic
plots. In the case of time series regression, an important diagnostic plot is the
correlogram of the residuals:
> acf(resid(x.lm))
> pacf(resid(x.lm))

## 5.3.3 Autocorrelation and the estimation of sample statistics*
![](https://i.imgur.com/E7K8s5T.png)
![](https://i.imgur.com/itxlWwy.png)
![](https://i.imgur.com/oUvzuFz.png)

# 5.4 Generalised least squares
We have seen that in time series regression it is common and expected that the
residual series will be autocorrelated. For a positive serial correlation in the
residual series, this implies that the standard errors of the estimated regres-
sion parameters are likely to be underestimated (Equation (5.5)), and should
therefore be corrected.
A fitting procedure known as generalised least squares (GLS) can be used
to provide better estimates of the standard errors of the regression parameters
to account for the autocorrelation in the residual series.

# 5.5 Linear models with seasonal variables
## 5.5.2 Additive seasonal indicator variables
A seasonal indicator model for a time series {xt : t = 1, . . . , n} containing s seasons and a trend mt is given by
$$x_t=m_t+s_t+z_t\;\;\;(5.6)$$
where $s_t = β_i$ when $t$ falls in the $ith$ season $(t = 1, . . . , n; i = 1, . . . , s)$
and $\{z_t\}$ is the residual error series, which may be autocorrelated.
Equation (5.6) can therefore be written as
$$
x_t = m_t + β_{1+(t−1) mod\, s} + z_t\;\;\;(5.7)
$$
![](https://i.imgur.com/i3HjfEc.png)
### **What Does $β1+(t−1)mod  s$ Mean?**

This is the key term that defines **seasonality** in a compact way.

- s is the number of seasons (e.g., s=12 for monthly data ).
- The term **$(t−1)mod  s$** ensures that the index cycles through the seasons repeatedly.
- The "+1" adjustment shifts the index so that it starts from 1 instead of 0.
# 5.6 Harmonic seasonal models
seasonal effects often vary smoothly over the seasons, so that it may be more
parameter-efficient to use a smooth function instead of separate indices.
Sine and cosine functions can be used to build smooth variation into a
seasonal model. A sine wave with frequency f (cycles per sampling interval),
amplitude A, and phase shift φ can be expressed as
![](https://i.imgur.com/61gkQPB.png)
![](https://i.imgur.com/FBXDpVJ.png)


# ==<mark>NOT Completed</mark>==


# 5.7 Logarithmic transformations
![](https://i.imgur.com/C41QzRv.png)
a log-transformation may be seen as an appropriate model
formulation when a series can only take positive values and has values near
zero because the anti-log forces the predicted and simulated values for {xt }
to be positive.

NOTE: using the natural logarithm of a series could help stabilise the variance.
### **How Does Taking the Log Help?**

The **natural logarithm (ln)** **compresses larger values more than smaller values**, reducing the impact of large fluctuations.
###### **Summary**

| **Concept**            | **Explanation**                                                                |
| ---------------------- | ------------------------------------------------------------------------------ |
| **Heteroscedasticity** | When variance **increases over time**, causing modeling issues.                |
| **Log Transformation** | Takes $Y_t=ln⁡(X_t)$ to reduce large fluctuations.                             |
| **Why It Works**       | Compresses large values more than small ones, making variance **more stable**. |
| **Best for**           | Data with **exponential growth** or increasing variance.                       |
| **Not suitable for**   | Data with **negative values or zeros**.                                        |

# 5.8 Non-linear models
for some time series it may be more appropriate to fit a non-linear model directly rather than take logs or use a linear polynomial approximation.For example, if a series is known to derive from a known non-linear process, perhaps based on an underlying known deterministic law in science, then it would be better to use this information in the model formulation and fit a non-linear model directly to the data.
### **When to Use This Approach?**

✔ **Use this method when:**

- Your time series contains **negative values**.
- The data exhibits **exponential growth** but includes **additive noise**.
- Log transformation is needed, but simple log(xt) is not possible due to negatives.
##### **Summary**

|**Concept**|**Explanation**|
|---|---|
|**Log transformation problem**|Log is **undefined** for negative values.|
|**Adding a constant c0c_0c0​**|Ensures all values are **positive before logging**.|
|**Non-linear model**|Uses an **exponential trend** with residuals.|
|**Issue with arbitrary c0c_0c0​**|Should be **estimated** using **non-linear least squares**.|
|**When to use it?**|When data has **negative values** and follows an **exponential trend**.|

As non-linear models are generally fitted when the underlying non-linear func-
tion is known

# 5.9 Forecasting from regression
In the context of time series regression, a forecast involves extrapolating a fitted model into the future by evaluating the model function for a new series of times.The main problem with this approach is that the trends present in the fitted series may change in the future.Therefore, it is better to think of a forecast from a regression model as an expected value conditional on past trends continuing into the future.







https://libraryfyuybp7oyidyya3ah5xvwgyx6weauoini7zyz555litmmumad.onion/