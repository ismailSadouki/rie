# Categorical Variables
## One-Hot-Encoding (Dummy Variables)
The idea behind dummy variables is to replace a categorical variable with one or more new features that can have the values 0 and 1.
##### Numbers Can Encode Categoricals
Categorical features are often encoded using integers. That they are
numbers doesn’t mean that they should necessarily be treated as
continuous features. It is not always clear whether an integer fea‐
ture should be treated as continuous or discrete (and one-hot-
encoded). If there is no ordering between the semantics that are
encoded (like in the workclass example), the feature must be
treated as discrete. For other cases, like five-star ratings, the better
encoding depends on the particular task and data and which
machine learning algorithm is used.
The get_dummies function in pandas treats all numbers as continuous and will not
create dummy variables for them. To get around this, you can either use scikit-
learn’s OneHotEncoder
If you want dummy variables to be created for the “Integer Feature” column, you can explicitly list the columns you want to encode using the columns parameter. Then, both features will be treated as categorical


## Binning, Discretization, Linear Models, and Trees
linear models can only model linear relationships, which are lines in
the case of a single feature. The decision tree can build a much more complex model of the data. However, this is strongly dependent on the representation of the data.
One way to make linear models more powerful on continuous data is to use binning (also known as discretization) of the feature to split it up into multiple features, as described here.
Binning features generally has no beneficial effect for tree-based models, as these models can learn to split up the data anywhere. 

If there are good reasons to use a linear model for a particular dataset—say, because it is very large and high-dimensional, but some features have nonlinear relations with the output—binning can be a great way to increase modeling power.
###### The OneHotEncoder
The OneHotEncoder does the same encoding as pandas.get_dummies, though it currently only works on categorical variables that are integers


## 🔹 **Why Use Binning in Feature Engineering?**

1. **Handles Outliers** 🛡️
    
    - Reduces the impact of extreme values by **grouping similar values together**.
2. **Captures Non-Linear Relationships** 📈
    
    - Some models (like **linear regression**) assume a linear relationship between features and the target variable.
    - Binning allows us to **capture complex patterns** more effectively.
3. **Improves Interpretability** 🧠
    
    - Instead of raw numbers, we get **meaningful categories** (e.g., “Low”, “Medium”, “High”).
4. **Reduces Overfitting** 📉
    
    - Converts **continuous data** into a limited set of bins → **less variance in data** → better generalization.

## ✅ **When to Use Binning?**

### **1️⃣ When Your Model Struggles with Non-Linearity** 📈

- If your model assumes a **linear relationship** but the data shows a **non-linear trend**, binning can help capture patterns.
- **Example:**
    - If **age** affects salary in a non-linear way (e.g., young workers earn less, middle-aged workers earn the most, and older workers earn slightly less), binning **Age** into groups (e.g., "Young", "Middle-aged", "Senior") can make patterns clearer.

🔹 **Good for:** Linear Regression, Logistic Regression, and other linear models.
### **2️⃣ When Your Data Has Extreme Outliers** 🛡️

- Outliers can **distort** numerical relationships, making models unstable.
- Binning reduces their impact by **grouping values together** instead of treating them as individual numbers.

🔹 **Good for:** Logistic Regression, Decision Trees, Random Forests.
### **4️⃣ When Your Data is Highly Skewed** ⚖️

- If a numerical feature has a **skewed distribution**, binning can help create **balanced categories**.
- **Example:**
    - If 90% of houses are priced below $500,000 but some are above $5 million, binning prevents the model from being dominated by rare high-priced homes.

🔹 **Good for:** Any model where feature distributions impact performance.
### **5️⃣ When You Use Tree-Based Models Like Decision Trees & Random Forests 🌳**

- Decision trees **split** data into groups anyway, so manual binning can sometimes improve interpretability.
- However, some tree-based models (e.g., **Gradient Boosting, XGBoost**) work better **without binning**, as they find optimal splits automatically.

🔹 **Good for:** Simple Decision Trees, Rule-Based Models.

## 📌 **Final Rule of Thumb:**

✅ Use binning when it **improves interpretability, reduces noise, or stabilizes your model**.  
❌ Avoid binning when it **removes valuable information or when the model can learn better from raw data**.

---
## Interactions and Polynomials
Using binning is one way to expand a continuous feature. Another one is to use polynomials of the original features.
#### They work best for linear models but:
###### **✅ Random Forests Capture Non-Linear Relationships**

Even though **Random Forests are powerful**, they **may not always detect subtle interactions** unless explicitly provided.

###### **Why Doesn’t Random Forest Automatically Capture Interactions?**

While **tree-based models** (like Random Forest) can **learn some interactions**, they **do not explicitly construct new features**. Adding interactions as **explicit features** can help the model find patterns faster and use data more efficiently.

## Univariate Nonlinear Transformations
applying mathematical functions like log, exp, or sin. While tree-based models only care about the ordering of the features, linear models and neural networks are very tied to the scale and distribution of each feature, and if there is a nonlinear relation between the feature and the target, that becomes hard to model---- particularly in regression. The functions log and exp can help by adjusting the relative scales in the data so that they can be captured better by a linear model or neural network.
The sin and cos functions can come in handy when dealing with data that encodes periodic patterns.

Most models work best when each feature (and in regression also the target) is loosely Gaussian distributed. Using transformations like log and exp is a hacky but simple and efficient way to achieve this.

usually only a subset of the features should be transformed, or sometimes each feature needs to be transformed in a different way.
these kinds of transformations are irrelevant for tree-based models but might be essential for linear models. Sometimes it is also a good idea to transform the target variable y in regression

---

# Automatic Feature Selection

adding more features makes all models more complex, and so increases the chance of overfitting. When adding new features, or with high-dimensional datasets in general, it can be a good idea to reduce the number of features to only the most useful ones,This can lead to simpler models that generalize better.

## Univariate Statistics
we compute whether there is a statistically significant relationship between each feature and the target. Then the features that are related with the highest confidence are selected. In the case of classification, this is also known as
analysis of variance (ANOVA).
Univariate feature selection can still be very helpful, though, if there is such a large number of features that building a model on them is infeasible, or if you suspect that many features are completely uninformative.


## Model-Based Feature Selection
Model-based feature selection uses a supervised machine learning model to judge the importance of each feature, and keeps only the most important ones.

Decision trees and decision tree–based models provide a feature_importances_.attribute, which directly encodes the importance of each feature. Linear models have coefficients, which can also be used to capture feature importances by considering the absolute values.


linear models with L1 penalty learn sparse coefficients, which only use a small subset of features. This can be viewed as a form of feature selection for the model itself, but can also be used as a preprocessing step to select features for another model.
model-based selection considers all features at once, and so can capture interactions
To use model-based feature selection, we need to use the SelectFromModel transformer

```
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier
select = SelectFromModel(
	RandomForestClassifier(n_estimators=100, random_state=42),
	threshold="median")
```
Only features with an **importance score above a given threshold** are kept.


## Iterative Feature Selection
In iterative feature selection, a series of models are built,
with varying numbers of features.
There are two basic methods: starting with no features and adding features one by one until some stopping criterion is reached, or starting with all features and removing features one by one until some stopping criterion is reached.

###### recursive feature elimination (RFE): 
starts with all features, builds a model, and discards the least important feature according to the model. Then a new model is built using all but the discarded feature, and so on until only a prespecified number of features are left.
For this to work, the model used for selection needs to provide some way to determine feature importance, as was the case for the model-based selection.
```
from sklearn.feature_selection import RFE
select = RFE(RandomForestClassifier(n_estimators=100, random_state=42),
n_features_to_select=40)
select.fit(X_train, y_train)
```

---
