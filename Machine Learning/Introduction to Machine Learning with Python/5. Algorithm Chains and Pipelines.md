# Parameter Selection with Preprocessing

<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250321195422.png]]
the splits in the cross-validation no longer correctly mirror how new data will
look to the modeling process. We already leaked information from these parts of the data into our modeling process. This will lead to overly optimistic results during cross-validation, and possibly the selection of suboptimal parameters.
To get around this problem, the splitting of the dataset during cross-validation should be done before doing any preprocessing. Any process that extracts knowledge from the dataset should only ever be applied to the training portion of the dataset, so any cross-validation should be the “outermost loop” in your processing.

The Pipeline class is a class that allows “gluing” together multiple processing steps into a single scikit-learn estimator.The most common use case of the Pipeline class is in chaining preprocessing steps (like scaling of the data) together with a supervised model like a classifier.

```
from sklearn.pipeline import Pipeline
pipe = Pipeline([("scaler", MinMaxScaler()), ("svm", SVC())])
pipe.fit(X_train, y_train)
```
Calling the score method on the pipeline first transforms the test data using the
scaler, and then calls the score method on the SVM using the scaled test data.
Using the pipeline, we reduced the code needed for our “preprocessing + classification” process. The main benefit of using the pipeline, however, is that we can now use this single estimator in cross_val_score or GridSearchCV.

## Using Pipelines in Grid Searches
When specifying the parameter grid, there is a slight
change, though. We need to specify for each parameter which step of the pipeline it belongs to.
Both parameters that we want to adjust, C and gamma, are parameters of
SVC, the second step. We gave this step the name "svm". The syntax to define a parameter grid for a pipeline is to specify for each parameter the step name, followed by __ (a double underscore), followed by the parameter name. To search over the C parameter of SVC we therefore have to use "svm__C" as the key in the parameter grid dictionary, and similarly for gamma:
```
param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],
            'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}
grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)
grid.fit(X_train, y_train)
print("Best cross-validation accuracy: {:.2f}".format(grid.best_score_))
print("Test set score: {:.2f}".format(grid.score(X_test, y_test)))
print("Best parameters: {}".format(grid.best_params_))
```
In contrast to the grid search we did before, now for each split in the cross-validation, the MinMaxScaler is refit with only the training splits and no information is leaked from the test split into the parameter search
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250321200416.png]]
The impact of leaking information in the cross-validation varies depending on the
nature of the preprocessing step. Estimating the scale of the data using the test fold usually doesn’t have a terrible impact, while using the test fold in feature extraction and feature selection can lead to substantial differences in outcomes.

# The General Pipeline Interface
The Pipeline class is not restricted to preprocessing and classification, but can in
fact join any number of estimators together.
For example, you could build a pipeline containing feature extraction, feature selection, scaling, and classification, for a total of four steps. Similarly, the last step could be regression or clustering instead of classification.
The only requirement for estimators in a pipeline is that all but the last step need to have a transform method, so they can produce a new representation of the data that can be used in the next step.
When predicting using Pipeline, we similarly transform the data using all but the
last step, and then call predict on the last step
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250321201228.png]]
The pipeline is actually even more general than this. There is no requirement for the last step in a pipeline to have a predict function, and we could create a pipeline just containing, for example, a scaler and PCA. Then, because the last step (PCA) has a transform method, we could call transform on the pipeline to get the output of PCA.transform applied to the data that was processed by the previous step. The last step of a pipeline is only required to have a fit method.

## Convenient Pipeline Creation with make_pipeline
```
from sklearn.pipeline import make_pipeline
# standard syntax
pipe_long = Pipeline([("scaler", MinMaxScaler()), ("svm", SVC(C=100))])
# abbreviated syntax
pipe_short = make_pipeline(MinMaxScaler(), SVC(C=100))
```

#### Accessing Step Attributes
```
# fit the pipeline defined before to the cancer dataset
pipe.fit(cancer.data)
# extract the first two principal components from the "pca" step
components = pipe.named_steps["pca"].components_
print("components.shape: {}".format(components.shape))
```
#### Accessing Attributes in a Grid-Searched Pipeline
```
print("Best estimator:\n{}".format(grid.best_estimator_))

print("Logistic regression step:\n{}".format(
grid.best_estimator_.named_steps["logisticregression"]))

print("Logistic regression coefficients:\n{}".format(
grid.best_estimator_.named_steps["logisticregression"].coef_))
```

## Grid-Searching Preprocessing Steps and Model Parameters
Using pipelines, we can encapsulate all the processing steps in our machine learning workflow in a single scikit-learn estimator.
Another benefit of doing this is that we can now adjust the parameters of the preprocessing using the outcome of a supervised task like regression or classification.

you can visualize the outcome of the cross-validation using a heat map
```
plt.matshow(grid.cv_results_['mean_test_score'].reshape(3, -1),
vmin=0, cmap="viridis")
plt.xlabel("ridge__alpha")
plt.ylabel("polynomialfeatures__degree")
plt.xticks(range(len(param_grid['ridge__alpha'])), param_grid['ridge__alpha'])
plt.yticks(range(len(param_grid['polynomialfeatures__degree'])),
param_grid['polynomialfeatures__degree'])
plt.colorbar()
```
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250321203646.png]]Looking at the results produced by the cross-validation, we can see that using polynomials of degree two helps, but that degree-three polynomials are much worse than either degree one or two

```
print("Best parameters: {}".format(grid.best_params_))
OUT:
	Best parameters: {'polynomialfeatures__degree': 2, 'ridge__alpha': 10}
```

Searching over preprocessing parameters together with model parameters is a very powerful strategy. However, keep in mind that GridSearchCV tries all possible combinations of the specified parameters. Therefore, adding more parameters to your grid exponentially increases the number of models that need to be built.

## Grid-Searching Which Model To Use
You can even go further in combining GridSearchCV and Pipeline: it is also possible to search over the actual steps being performed in the pipeline (say whether to use StandardScaler or MinMaxScaler). This leads to an even bigger search space and should be considered carefully.

here is an example comparing a RandomForestClassifier and an SVC on the iris dataset.
```
pipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', SVC())])
param_grid = [
    {'classifier': [SVC()], 'preprocessing': [StandardScaler(), None],
    'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100],
    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]},
    {'classifier': [RandomForestClassifier(n_estimators=100)],
    'preprocessing': [None], 'classifier__max_features': [1, 2, 3]}]
	    X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)
	grid = GridSearchCV(pipe, param_grid, cv=5)
	grid.fit(X_train, y_train)
	print("Best params:\n{}\n".format(grid.best_params_))
	print("Best cross-validation score: {:.2f}".format(grid.best_score_))
	print("Test-set score: {:.2f}".format(grid.score(X_test, y_test)))


```