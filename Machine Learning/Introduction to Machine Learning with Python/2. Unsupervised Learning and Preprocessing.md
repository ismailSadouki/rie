unsupervised algorithms are used often in an exploratory setting,
when a data scientist wants to understand the data better, rather than as part of a larger automatic system. Another common application for unsupervised algorithms is as a preprocessing step for supervised algorithms. Learning a new representation of the data can sometimes improve the accuracy of supervised algorithms, or can lead to reduced memory and time consumption.

# Preprocessing and Scaling

![](https://i.imgur.com/KF86kKB.png)

## Different Kinds of Preprocessing
- The StandardScaler in scikit-learn ensures that for each feature the mean is 0 and the variance is 1. However, this scaling does not ensure any particular minimum and maximum values for the features.
- The RobustScaler works similarly to the StandardScaler in that it ensures statistical properties for each feature that guarantee that they are on the same scale. However, the RobustScaler uses the median and quartiles,1 instead of mean and variance. This makes the RobustScaler ignore data points that are very different from the rest (like measurement errors). These odd data points are also called outliers, and can lead to trouble for other scaling techniques.
- The MinMaxScaler, on the other hand, shifts the data such that all features are exactly between 0 and 1.
- the Normalizer does a very different kind of rescaling. It scales each data point such that the feature vector has a Euclidean length of 1. In other words, it projects a data point on the circle with a radius of 1.This normalization is often used when only the direction (orangle) of the data matters, not the length of the feature vector.

### Scaling Training and Test Data the Same Way
It is important to apply exactly the same transformation to the training set and the
test set for the supervised model to work on the test set.
```
# scale the data using MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
```
Here, we called fit on the training set, and then called transform on the training and test sets.
![](https://i.imgur.com/S4QDPQL.png)

or better computational way: 
```
from sklearn.preprocessing import MinMaxScaler

# Initialize MinMaxScaler
scaler = MinMaxScaler()

# Fit and transform training data in one step
X_train_scaled = scaler.fit_transform(X_train)

# Transform test data using the already fitted scaler
X_test_scaled = scaler.transform(X_test)
```

![](https://i.imgur.com/bTC3k25.png)

---

# Dimensionality Reduction, Feature Extraction, and Manifold Learning 

## Principal Component Analysis (PCA)
Principal component analysis is a method that rotates the dataset in a way such 
the rotated features are statistically uncorrelated.This rotation is often followed by
selecting only a subset of the new features, according to how important they are for explaining the data.
![](https://i.imgur.com/hjulF9F.png)
The directions found using this process are called principal components, as they are the main directions of variance in the data. In general, there are as many principal components as original features

### Applying PCA to the cancer dataset for visualization
One of the most common applications of PCA is visualizing high-dimensional datasets.
if we want to look at the Breast Cancer dataset, even using a pair plot is tricky. This dataset has 30 features, which would result in 30 * 14 = 420 scatter plots! We’d never be able to look at all these plots in detail, let alone try to understand them.
There is an even simpler visualization we can use, though—computing histograms of each of the features for the two classes, benign and malignant cancer
![](https://i.imgur.com/P5J9K8V.png)
Here we create a histogram for each of the features, counting how often a data point appears with a feature in a certain range (called a bin). Each plot overlays two histograms, one for all of the points in the benign class (blue) and one for all the points in the malignant class (red). This gives us some idea of how each feature is distributed across the two classes, and allows us to venture a guess as to which features are better at distinguishing malignant and benign samples. For example, the feature “smoothness error” seems quite uninformative, because the two histograms mostly overlap, while the feature “worst concave points” seems quite informative, because the histograms are quite disjoint.

However, this plot doesn’t show us anything about the interactions between variables and how these relate to the classes. Using PCA, we can capture the main interactions and get a slightly more complete picture.

Before we apply PCA, we scale our data so that each feature has unit variance using StandardScaler.
Learning the PCA transformation and applying it is as simple as applying a prepro‐
cessing transformation. We instantiate the PCA object, find the principal components by calling the fit method, and then apply the rotation and dimensionality reduction by calling transform.
By default, PCA only rotates (and shifts) the data, but keeps all principal components. To reduce the dimensionality of the data, we need to specify
how many components we want to keep when creating the PCA object:
```
# keep the first two principal components of the data
pca = PCA(n_components=2)
```

Note that the PCA only looks at the correlations in the data when finding the rotations.
example: 
![](https://i.imgur.com/1VQFHUe.png)
we plotted the first principal component against the second principal component, and then used the class information to color the points.You can see that the two classes separate quite well in this two-dimensional space. This leads us to believe that even a linear classifier (that would learn a line in this space) could do a reasonably good job at distinguishing the two classes. We can also see that the malignant (red) points are more spread out than the benign (blue) points
We can also visualize the coefficients using a heat map (Figure 3-6), which might be easier to understand:
![](https://i.imgur.com/daVyssZ.png)

### Eigenfaces for feature extraction
the idea behind feature extraction is that it is possible to find a representation of your data that is better suited to analysis than the raw representation you were given.

## Non-Negative Matrix Factorization (NMF)
in PCA we wanted components that were orthogonal and that explained as much variance of the data as possible, in NMF, we want the components and the coefficients to be non-negative; Consequently, this method can only be applied to data where each feature is non-negative, as a non-negative sum of non-negative components cannot become negative.

The process of decomposing data into a non-negative weighted sum is particularly helpful for data that is created as the addition (or overlay) of several independent sources, such as an audio track of multiple people speaking, or music with many instruments. In these situations, NMF can identify the original components that make up the combined data.

![](https://i.imgur.com/4dUkYrc.png)
For NMF with two components, as shown on the left, it is clear that all points in the data can be written as a positive combination of the two components. If there are enough components to perfectly reconstruct the data (as many components as there are features), the algorithm will choose directions that point toward the extremes of the data.
If we only use a single component, NMF creates a component that points toward the mean, as pointing there best explains the data.
You can see that in contrast with PCA, reducing the number of components not only removes some directions, but creates an entirely different set of ! also in NMF all components play an equal part.

### Applying NMF to face images
The main parameter of NMF is how many components we want to extract.

as PCA finds the optimum directions in terms of reconstruction. NMF is usually not used for its ability to reconstruct or encode data, but rather for finding interesting patterns within the data.

## Manifold Learning with t-SNE
Manifold learning algorithms are mainly aimed at visualization, and so are rarely
used to generate more than two new features.Some of them, including t-SNE, compute a new representation of the training data, but don’t allow transformations of new data. This means these algorithms cannot be applied to a test set: rather, they can only transform the data they were trained for.
Manifold learning can be useful for exploratory data analysis, but is rarely used if the final goal is supervised learning.
The idea behind t-SNE is to find a two-dimensional representation of the data that preserves the distances between points as best as possible.

![](https://i.imgur.com/9XUcbgm.png)

The result of t-SNE is quite remarkable. All the classes are quite clearly separated.
The t-SNE algorithm has some tuning parameters, though it often works well with
the default settings. You can try playing with perplexity and early_exaggeration,
but the effects are usually minor.



# Clustering
clustering is the task of partitioning the dataset into groups, called clusters.
## k-Means Clustering
It tries to find cluster centers that are representative of certain regions of the
data. The algorithm alternates between two steps: assigning each data point to the closest cluster center, and then setting each cluster center as the mean of the data points that are assigned to it. The algorithm is finished when the assignment of instances to clusters no longer changes.

![](https://i.imgur.com/IqaFLdZ.png)

### Failure cases of k-mean
Even if you know the “right” number of clusters for a given dataset, k-means might not always be able to recover them.

### Vector quantization, or seeing k-means as decomposition
Even though k-means is a clustering algorithm, there are interesting parallels between k-means and the decomposition methods like PCA and NMF
You can think of that as each point being represented using only a single component, which is given by the cluster center. This view of k-means as a decomposition method, where each point is represented using a single component, is called vector quantization.
An interesting aspect of vector quantization using k-means is that we can use many more clusters than input dimensions to encode our data.

![](https://i.imgur.com/tmnnOBw.png)

## Agglomerative Clustering
Agglomerative clustering refers to a collection of clustering algorithms that all build upon the same principles: the algorithm starts by declaring each point its own cluster, and then merges the two most similar clusters until some stopping criterion is satisfied. The stopping criterion implemented in scikit-learn is the number of clusters, so similar clusters are merged until only the specified number of clusters are left.
![](https://i.imgur.com/ZJOyTw0.png)
ward works on most datasets, and we will use it in our examples. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for example), average or complete might work better.
![](https://i.imgur.com/k8KqJ8Z.png)


### Hierarchical clustering and dendrograms
Agglomerative clustering produces what is known as a hierarchical clustering. The
clustering proceeds iteratively, and every point makes a journey from being a single point cluster to belonging to some final cluster.
![](https://i.imgur.com/BMI4jrW.png)
While this visualization provides a very detailed view of the hierarchical clustering, it relies on the two-dimensional nature of the data and therefore cannot be used on datasets that have more than two features. There is, however, another tool to visualize hierarchical clustering, called a dendrogram, that can handle multidimensional datasets.
![](https://i.imgur.com/jcbtCY3.png)

Unfortunately, agglomerative clustering still fails at separating complex shapes like the two_moons dataset. But the same is not true for the next algorithm we will look at, DBSCAN.

## DBSCAN
The main benefits of DBSCAN are that it does not require the user to set the number of clusters a priori, it can capture clusters of complex shapes, and it can identify points that are not part of any cluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but still scales to relatively large datasets.
DBSCAN works by identifying points that are in “crowded” regions of the feature
space, where many data points are close together. These regions are referred to as dense regions in feature space. The idea behind DBSCAN is that clusters form dense regions of data, separated by regions that are relatively empty
Points that are within a dense region are called core samples (or core points), and they are defined as follows. There are two parameters in DBSCAN: min_samples and eps. If there are at least min_samples many data points within a distance of eps to a given data point, that data point is classified as a core sample. Core samples that are closer to each other than the distance eps are put into the same cluster by DBSCAN.

In the end, there are three kinds of points: core points, points that are within distanceeps of core points (called boundary points), and noise. When the DBSCAN algorithm is run on a particular dataset multiple times, the clustering of the core points is always the same, and the same points will always be labeled as noise. However, a boundary point might be neighbor to core samples of more than one cluster. Therefore, the cluster membership of boundary points depends on the order in which points are vis‐ited. Usually there are only few boundary points, and this slight dependence on the order of points is not important.
![](https://i.imgur.com/66mLAIj.png)
![](https://i.imgur.com/RoeI8QR.png)
![](https://i.imgur.com/Do5IOxe.png)

# Comparing and Evaluating Clustering Algorithms
## Evaluating clustering with ground truth
There are metrics that can be used to assess the outcome of a clustering algorithm relative to a ground truth clustering, the most important ones being the adjusted rand index (ARI) and normalized mutual information (NMI), which both provide a quantitative measure between 0 and 1.
![](https://i.imgur.com/VjLjR8m.png)
A common mistake when evaluating clustering in this way is to use accuracy_score instead of adjusted_rand_score, normalized_mutual_info_score, or some other clustering metric. The problem in using accuracy is that it requires the assigned cluster labels to exactly match the ground truth. However, the cluster labels themselves are meaningless—the only thing that matters is which points are in the same cluster.
## Evaluating clustering without ground truth
in practice, there is a big problem with using measures like ARI. When applying clustering algorithms, there is usually no ground truth to which to compare the results.
using metrics like ARI and NMI usually only helps in developing algorithms, not in assessing success in an application.
![](https://i.imgur.com/VG4176N.png)
## Summary of Clustering Methods
Each of the algorithms has somewhat different strengths. k-means allows for a characterization of the clusters using the cluster means. It can also be viewed as a decomposition method, where each data point is represented by its cluster center. DBSCAN allows for the detection of “noise points” that are not assigned any cluster, and it can help automatically determine the number of clusters. In contrast to the other two methods, it allow for complex cluster shapes, as we saw in the two_moons example. DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. Agglomerative clustering can provide a whole hierarchy of possible partitions of the data, which can be easily inspected via dendrograms.









http://bookszlibb74ugqojhzhg2a63w5i2atv5bqarulgczawnbmsb6s6qead.onion/book/3426073/6c67cb/genetic-algorithms-in-search-optimization-and-machine-learning.html?dsource=recommend
http://bookszlibb74ugqojhzhg2a63w5i2atv5bqarulgczawnbmsb6s6qead.onion/book/2948681/f63122/think-like-a-data-scientist-tackle-the-data-science-process-stepbystep.html?dsource=recommend
http://bookszlibb74ugqojhzhg2a63w5i2atv5bqarulgczawnbmsb6s6qead.onion/book/1173029/73136d/clever-algorithms-natureinspired-programming-recipes.html

