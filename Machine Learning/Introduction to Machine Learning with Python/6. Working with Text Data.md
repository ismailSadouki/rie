# Representing Text Data as a Bag of Words
One of the most simple but effective and commonly used ways to represent text for machine learning is using the bag-of-words representation.
When using this representation, we discard most of the structure of the input text, like chapters, paragraphs, sentences, and formatting, and only count how often each word appears in each text in the corpus. Discarding the structure and counting only word occurrences leads to the mental image of representing text as a “bag.”

Computing the bag-of-words representation for a corpus of documents consists of the following three steps:
1. Tokenization. Split each document into the words that appear in it (called tokens),for example by splitting them on whitespace and punctuation.
2. Vocabulary building. Collect a vocabulary of all words that appear in any of the documents, and number them (say, in alphabetical order).
3. Encoding. For each document, count how often each of the words in the vocabulary appear in this document.
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250323225810.png]]
our numeric representation has one feature for each unique word in the whole dataset.


<mark>NOT COMPLETED</mark>
