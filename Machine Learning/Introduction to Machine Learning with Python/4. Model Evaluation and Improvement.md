## Cross-Validation
statistical method of evaluating generalization performance that is more stable and thorough than using a split into a training and a test set.
The most commonly used version of cross-validation is k-fold cross-validation
![](https://i.imgur.com/QkuR6h3.png)
The parameters of the cross_val_score function are the model we want to evaluate, the training data, and the ground-truth labels.

## Benefits of Cross-Validation
Having multiple splits of the data also provides some information about how sensi‐
tive our model is to the selection of the training dataset. it provides us with an
idea about how the model might perform in the worst case and best case scenarios when applied to new data.


It is important to keep in mind that cross-validation is not a way to
build a model that can be applied to new data. Cross-validation
does not return a model. When calling cross_val_score, multiple
models are built internally, but the purpose of cross-validation is
only to evaluate how well a given algorithm will generalize when
trained on a specific dataset.

## Stratified k-Fold Cross-Validation and Other Strategies
In stratified cross-validation, we split the data such that the proportions between classes are the same in each fold as they are in the whole dataset
![](https://i.imgur.com/YwennfH.png)
It is usually a good idea to use stratified k-fold cross-validation instead of k-fold
cross-validation to evaluate a classifier, because it results in more reliable  of
generalization performance.
For regression, scikit-learn uses the standard k-fold cross-validation by default. It
would be possible to also try to make each fold representative of the different values the regression target has, but this is not a commonly used strategy and would be surprising to most users.

#### More control over cross-validation
We saw earlier that we can adjust the number of folds that are used in
cross_val_score using the cv parameter. However, scikit-learn allows for much
finer control over what happens during the splitting of the data by providing a crossvalidation splitter as the cv parameter.
![](https://i.imgur.com/jnyTmLH.png)
Remember: each fold corresponds to one of the classes in the iris dataset, and so
nothing can be learned. Another way to resolve this problem is to shuffle the data
instead of stratifying the folds, to remove the ordering of the samples by label. We can do that by setting the shuffle parameter of KFold to True.
#### Leave-one-out cross-validation
You can think of leave-one-out cross-validation as k-fold cross-validation where each fold is a single sample. For each split, you pick a single data point to be the test set.

#### Shuffle-split cross-validation
![](https://i.imgur.com/R4GzBI1.png)
Subsampling the data in this way can be useful for experimenting with large datasets.
There is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS
plit, which can provide more reliable results for classification tasks.

#### Cross-validation with groups
Another very common setting for cross-validation is when there are groups in the
data that are highly related.
This example of groups in the data is common in medical applications, where you
might have multiple samples from the same patient, but are interested in generalizing to new patients. Similarly, in speech recognition, you might have multiple recordings of the same speaker in your dataset, but are interested in recognizing speech of new speakers.
![](https://i.imgur.com/6aQi0W9.png)


# Grid Search
Consider the case of a kernel SVM with an RBF (radial basis function) kernel, as
implemented in the SVC class.there are two important parameters: the kernel bandwidth, gamma, and the regularization parameter, C.
## Simple Grid Search
We can implement a simple grid search just as for loops over the two parameters,
training and evaluating a classifier for each combination:

```
best_score = 0
for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:
    for C in [0.001, 0.01, 0.1, 1, 10, 100]:
        # for each combination of parameters, train an SVC
        svm = SVC(gamma=gamma, C=C)
        svm.fit(X_train, y_train)
        # evaluate the SVC on the test set
        score = svm.score(X_test, y_test)
        # if we got a better score, store the score and parameters
        if score > best_score:
            best_score = score
            best_parameters = {'C': C, 'gamma': gamma}

print("Best score: {:.2f}".format(best_score))
print("Best parameters: {}".format(best_parameters))
```
### The Danger of Overfitting the Parameters and the Validation Set
we tried many different parameters and selected the one with best accuracy on the test set, but this accuracy won’t necessarily carry over to new data. Because we used the test data to adjust the parameters, we can no longer use it to assess how good the model is. This is the same reason we needed to split the data into training and test sets in the first place; we need an independent dataset to evaluate, one that was not used to create the model.

One way to resolve this problem is to split the data again, so we have three sets: the training set to build the model, the validation (or development) set to select the parameters of the model, and the test set to evaluate the performance of the selected parameters. Figure 5-5 shows what this looks like:
![](https://i.imgur.com/8YcpUUG.png)
After selecting the best parameters using the validation set, we can rebuild a model using the parameter settings we found, but now training on both the training data and the validation data. This way, we can use as much data as possible to build our model. This leads to the following implementation

The distinction between the training set, validation set, and test set is fundamentally  important to applying machine learning methods in practice.Any choices made based on the test set accuracy “leak” information from the test set into the model.
It is good practice to do all exploratory analysis and model selection using
the combination of a training and a validation set, and reserve the test set for a final evaluation. Strictly speaking, evaluating more than one model on the test set and choosing the better of the two will result in an overly optimistic estimate of how accurate the model is.

## Grid Search with Cross-Validation
instead of using a single split into a training and a validation set, we can use cross-validation to evaluate the performance of each parameter combination. This method can be coded up as follows:
```
for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:
    for C in [0.001, 0.01, 0.1, 1, 10, 100]:
        # for each combination of parameters,
        # train an SVC
        svm = SVC(gamma=gamma, C=C)
        # perform cross-validation
        scores = cross_val_score(svm, X_trainval, y_trainval, cv=5)
        # compute mean cross-validation accuracy
        score = np.mean(scores)
        # if we got a better score, store the score and parameters
        if score > best_score:
            best_score = score
            best_parameters = {'C': C, 'gamma': gamma}
# rebuild a model on the combined training and validation set
svm = SVC(**best_parameters)
svm.fit(X_trainval, y_trainval)
```
<mark>THAT COULD REALLY BOOST THE ACCURACY</mark>

![](https://i.imgur.com/KHlUPrA.png)
For each parameter setting (only a subset is shown), five accuracy values are computed, one for each split in the cross-validation. Then the mean validation accuracy is computed for each parameter setting. The parameters with the highest mean validation accuracy are chosen, marked by the circle.
![](https://i.imgur.com/VoD1XgM.png)

![](https://i.imgur.com/9tAwKQM.png)
Because grid search with cross-validation is such a commonly used method to adjust parameters, scikit-learn provides the GridSearchCV class, which implements it in the form of an estimator.
```
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],
				'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}
print("Parameter grid:\n{}".format(param_grid))
```

## Analyzing the result of cross-validation
It is often helpful to visualize the results of cross-validation, to understand how the model generalization depends on the parameters we are searching.
often it is a good idea to start with a relatively coarse and small grid. We can then inspect the results of the cross-validated grid search, and possibly expand our search.
```
import pandas as pd
# convert to DataFrame
results = pd.DataFrame(grid_search.cv_results_)
# show the first 5 rows
display(results.head())
```
Tuning the parameter grid based on the cross-validation scores is perfectly fine, and a good way to explore the importance of different parameters. However, you should not test different parameter ranges on the final test set---as we discussed earlier, evaluation of the test set should happen only once we know exactly what model we want to use.

## Search over spaces that are not grids
In some cases, trying all possible combinations of all parameters as GridSearchCV
usually does, is not a good idea.For example, SVC has a kernel parameter, and
depending on which kernel is chosen, other parameters will be relevant. If ker
nel='linear', the model is linear, and only the C parameter is used. If kernel='rbf',
both the C and gamma parameters are used (but not other parameters like degree).In this case, searching over all possible combinations of C, gamma, and kernel wouldn’t make sense: if kernel='linear', gamma is not used, and trying different values for gamma would be a waste of time. To deal with these kinds of “conditional” parameters, GridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the list is expanded into an independent grid. A possible grid search involving kernel and parameters could look like this:
```
param_grid = [{'kernel': ['rbf'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},
    {'kernel': ['linear'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100]}]
print("List of grids:\n{}".format(param_grid))
```

### Using different cross-validation strategies with grid search
Similarly to cross_val_score, GridSearchCV uses stratified k-fold cross-validation
by default for classification, and k-fold cross-validation for regression. However, you can also pass any cross-validation splitter.In particular, to get only a single split into a training and a validation set, you can use ShuffleSplit or  with n_iter=1. This might be helpful for very large datasets, or very slow models.

#### Nested cross-validation
We can go a step further, and instead of splitting the original data
into training and test sets once, use multiple splits of cross-validation.This will result in what is called nested cross-validation.
In nested cross-validation, there is an outer loop over splits of the data into training and test sets. For each of them, a grid search is run (which might result in different best parameters for each split in the outer loop). Then, for each outer split, the test set score using the best settings is reported.
The result of this procedure is a list of scores—not a model, and not a parameter setting. The scores tell us how well a model generalizes, given the best parameters found by the grid.
As it doesn’t provide a model that can be used on new data, nested cross-
validation is rarely used when looking for a predictive model to apply to future data.
However, it can be useful for evaluating how well a given model works on a particular dataset.
```
scores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5),iris.data, iris.target, cv=5)
print("Cross-validation scores: ", scores)
print("Mean cross-validation score: ", scores.mean())
```
The result of our nested cross-validation can be summarized as “SVC can  98%
mean cross-validation accuracy on the iris dataset”—nothing more and nothing
less.


|**Cross-Validation Strategy**|**Use Case**|
|---|---|
|**k-Fold (`cv=5`)**|Default for balanced datasets|
|**Stratified k-Fold**|Best for imbalanced classification problems|
|**Leave-One-Out (LOO)**|Best for small datasets but very slow|
|**TimeSeriesSplit**|Best for time-series or sequential data|
|**Group k-Fold**|Best when samples belong to predefined groups|


#### Parallelizing cross-validation and grid search


---
---

# Evaluation Metrics and Scoring
## Metrics for Binary Classification
##### Kinds of errors
Often, accuracy is not a good measure of predictive performance, as the number of mistakes we make does not contain all the information we are interested in.
##### Imbalanced datasets
Types of errors play an important role when one of two classes is much more frequent than the other one. This is very common in practice; In reality,  data is the norm,and it is rare that the events of interest have equal or even similar frequency in the data.

## Confusion matrices
![](https://i.imgur.com/wPY0fmD.png)
Relation to accuracy. We already saw one way to summarize the result in the confusion matrix—by computing accuracy, which can be expressed as:
![](https://i.imgur.com/Y3y9Lox.png)
Precision measures how many of the samples predicted as positive are actually positive:
![](https://i.imgur.com/OjdgXRY.png)
Precision is used as a performance metric when the goal is to limit the number of
false positives.
Recall, on the other hand, measures how many of the positive samples are captured by the positive predictions:
![](https://i.imgur.com/onk3Pew.png)
Recall is used as performance metric when we need to identify all positive samples;that is, when it is important to avoid false negatives.

There is a trade-off between optimizing recall and optimizing precision.
f-score or f-measure, which is with the harmonic mean of precision and recall:
![](https://i.imgur.com/k5hWgFh.png)
, it can be a better measure than accuracy on imbalanced binary classifi‐
cation datasets.

##### Taking uncertainty into account
The confusion matrix and the classification report provide a very detailed analysis of a particular set of predictions. However, the predictions themselves already threw away a lot of information that is contained in the model. Making predictions can be seen as thresholding the output of decision_function or predict_proba at a certain fixed point—in binary classification we use 0 for the decision function and 0.5 for predict_proba.

![](https://i.imgur.com/AW9PkSK.png)
If you value precision over recall or the other way around, or your data is
heavily imbalanced, changing the decision threshold is the easiest way to obtain better results.

### Precision-recall curves and ROC curves
As we just discussed, changing the threshold that is used to make a classification decision in a model is a way to adjust the trade-off of precision and recall for a given classifier.

Setting a requirement on a classifier like 90% recall is often called setting the operating point.

Often, when developing a new model, it is not entirely clear what the operating point will be. For this reason, and to understand a modeling problem better, it is instructive to look at all possible thresholds, or all possible trade-offs of precision and recalls at once. This is possible using a tool called the precision-recall curve.
![](https://i.imgur.com/oosBuRI.png)
The closer a curve stays to the upper-right corner, the better the classifier.
![](https://i.imgur.com/5dyg6GN.png)

verage precision is the area under a curve that goes from 0 to 1, average precision always returns a value between 0 (worst) and 1 (best). The average precision of a classifier that assigns decision_function at random is the fraction of positive samples in the dataset.

#### Receiver operating characteristics (ROC) and AUC
the ROC curve considers all possible thresholds for a given classifier, but instead of reporting precision and recall, it shows the false positive rate (FPR) against the true positive rate (TPR).
Recall that the true positive rate is simply another name for recall, while the false positive rate is the fraction of false positives out of all negative samples:<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250321185350.png]]
The AUC can be interpreted as evaluating the ranking of positive samples. It’s equivalent to the probability that a randomly picked point of the positive class will have a higher score according to the classifier than a randomly picked point from the negative class. So, a perfect AUC of 1 means that all positive points have a higher score than all negative points. For classification problems with imbalanced classes, using AUC for model selection is often much more meaningful than using accuracy.

Let’s go back to the problem we studied earlier of classifying all nines in the digits
dataset versus all other digits. We will classify the dataset with an SVM with three different settings of the kernel bandwidth, gamma (see Figure 5-17):
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250321185954.png]]
with the right threshold, this model can classify the data perfectly! Knowing this, we can adjust the threshold on this model and obtain great predictions. If we had only used accuracy, we would never have discovered this.

## Metrics for Multiclass Classification
all metrics for multiclass classification are derived from binary classification metrics, but averaged over all classes.Accuracy for multiclass classification is again defined as the fraction of correctly classified examples. And again, when classes are imbalanced, accuracy is not a great evaluation measure.
With the classification_report function, we can compute the precision, recall,
and f-score for each class.


## Using Evaluation Metrics in Model Selection
we often want to use metrics like AUC in model selection using GridSearchCV or cross_val_score. Luckily scikit-learn provides a very simple way to achieve this, via the scoring argument that can be used in both GridSearchCV and . You can simply provide a string describing the evaluation metric you want to use.

for example, we want to evaluate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC score. Changing the score from the default (accuracy) to AUC can be done by providing "roc_auc" as the scoring parameter:

```
# default scoring for classification is accuracy
print("Default scoring: {}".format(cross_val_score(SVC(), digits.data, digits.target == 9)))
# providing scoring="accuracy" doesn't change the results
explicit_accuracy = cross_val_score(SVC(), digits.data, digits.target == 9,scoring="accuracy")
print("Explicit accuracy scoring: {}".format(explicit_accuracy))
roc_auc = cross_val_score(SVC(), digits.data, digits.target == 9,scoring="roc_auc")
print("AUC scoring: {}".format(roc_auc))
```

```
Out[67]:
Default scoring: [ 0.9 0.9 0.9]
Explicit accuracy scoring: [ 0.9 0.9 0.9]
AUC scoring: [ 0.994 0.99 0.996]
```
Similarly, we can change the metric used to pick the best parameters in Grid
SearchCV:

```
X_train, X_test, y_train, y_test = train_test_split(
digits.data, digits.target == 9, random_state=0)
# we provide a somewhat bad grid to illustrate the point:
param_grid = {'gamma': [0.0001, 0.01, 0.1, 1, 10]}
# using the default scoring of accuracy:
grid = GridSearchCV(SVC(), param_grid=param_grid)
grid.fit(X_train, y_train)
print("Grid-Search with accuracy")
print("Best parameters:", grid.best_params_)
print("Best cross-validation score (accuracy)): {:.3f}".format(grid.best_score_))
print("Test set AUC: {:.3f}".format(
roc_auc_score(y_test, grid.decision_function(X_test))))
print("Test set accuracy: {:.3f}".format(grid.score(X_test, y_test)))
```

```
OUT:
	Grid-Search with accuracy
	Best parameters: {'gamma': 0.0001}
	Best cross-validation score (accuracy)): 0.970
	Test set AUC: 0.992
	Test set accuracy: 0.973
```

```
# using AUC scoring instead:
grid = GridSearchCV(SVC(), param_grid=param_grid, scoring="roc_auc")
grid.fit(X_train, y_train)
print("\nGrid-Search with AUC")
print("Best parameters:", grid.best_params_)
print("Best cross-validation score (AUC): {:.3f}".format(grid.best_score_))
print("Test set AUC: {:.3f}".format(
roc_auc_score(y_test, grid.decision_function(X_test))))
print("Test set accuracy: {:.3f}".format(grid.score(X_test, y_test)))
```

```
OUT
	Grid-Search with AUC
	Best parameters: {'gamma': 0.01}
	Best cross-validation score (AUC): 0.997
	Test set AUC: 1.000
	Test set accuracy: 1.000 
```
When using accuracy, the parameter gamma=0.0001 is selected, while gamma=0.01 is selected when using AUC. The cross-validation accuracy is consistent with the test set accuracy in both cases. However, using AUC found a better parameter setting in terms of AUC and even in terms of accuracy. <mark>WHY</mark>


The most important values for the scoring parameter for classification are accuracy (the default); roc_auc for the area under the ROC curve; average_precision for the area under the precision-recall curve; f1, f1_macro, f1_micro, and f1_weighted for the binary f1-score and the different weighted variants. For regression, the most commonly used values are r2 for the R2 score, mean_squared_error for mean squared error, and mean_absolute_error for mean absolute error