### Generalization, Overfitting, and Underfitting
The more complex we allow our model to be, the better we will be able to predict on the training data. However, if our model becomes too complex, we start focusing too much on each individual data point in our training set, and the model will not generalize well to new data.

### Linear Models
with higher-dimensional datasets (meaning datasets with a large number of features), linear models become more powerful, and there is a higher chance of overfitting.
###### Ridge regression
The optimum setting of alpha depends on the particular dataset we are using.
Increasing alpha forces coefficients to move more toward zero, which decreases
training set performance but might help generalization.
A higher alpha means a more restricted model, so we expect the entries of coef_ to have smaller magnitude for a high value of alpha than for a low value of alpha.

Another way to understand the influence of regularization is to fix a value of alpha
but vary the amount of training data available.
![](https://i.imgur.com/7i2UJy8.png)
As more and more data becomes available to the model, both models improve, and linear regression catches up with ridge in the end. The lesson here is that with enough training data, regularization becomes less important, linear regression will have the same performance

#### Lasso
when using the lasso, some coefficients are exactly zero. This means some fea‐
tures are entirely ignored by the model. This can be seen as a form of automatic feature selection. Having some coefficients be exactly zero often makes a model easier to interpret, and can reveal the most important features of your model.

In practice, ridge regression is usually the first choice between these two models.
However, if you have a large amount of features and expect only a few of them to be important, Lasso might be a better choice. Similarly, if you would like to have a
model that is easy to interpret, Lasso will provide a model that is easier to under‐
stand, as it will select only a subset of the input features.
scikit-learn also provides the **ElasticNet** class, which combines the penalties of Lasso and Ridge. In practice, this combination works best, though at the price of having two parameters to adjust: one for the L1 regularization, and one for the L2 regularization.


There are many algorithms for learning linear models. These algorithms all differ in
the following two ways:
• The way in which they measure how well a particular combination of coefficients
and intercept fits the training data
• If and what kind of regularization they use



# Linear Models for classification
For LogisticRegression and LinearSVC the trade-off parameter that determines the strength of the regularization is called C, when you use a high value for the parameter C, LogisticRegression and LinearSVC try to fit the training set as best as possible, while with low values of the parameter C, the models put more emphasis on finding a coefficient vector (w) that is close to zero.
![](https://i.imgur.com/VLJYGWq.png)


linear models for classification might seem very restrictive in low-dimensional spaces, but in high dimensions, linear models for classification become very powerful, and guarding against overfitting becomes increasingly impor‐
tant when considering more features.
![](https://i.imgur.com/NQeEzKu.png)


### Linear models for multiclass classification
A common technique to extend a binary classification algorithm to a multiclass classification algorithm is the one-vs.-rest approach.
In the one-vs.-rest approach, a binary model is learned for each class that tries to separate that class from all of the other classes, resulting in as many binary models as there are classes.
To make a prediction, all binary classifiers are run on a test point. The classifier that has the highest score on its single class “wins,” and this class label is returned as the prediction.

![](https://i.imgur.com/wFnOETL.png)


But what about the triangle in the middle of the plot? The answer is the one with the highest value for the classification formula: the class of the closest line.

### Strengths, weaknesses, and parameters
- The main parameter of linear models is the regularization parameter, called alpha in the regression models and C in LinearSVC and LogisticRegression. If you assume that only a few of your features are actually important, you should use L1. Otherwise, you should default to L2.
- If your data consists of hundreds of thousands or millions of samples, you might want to investigate using the solver='sag' option in LogisticRegression and Ridge, which can be faster than the default on large datasets. Other options are the SGDClassifier class and the SGDRegressor class, which implement even more scalable versions of the linear models described here.
- Linear models often perform well when the number of features is large compared to the number of samples. However, in lower-dimensional spaces, other models might yield better generalization performance.
---
---
# Naive Bayes Classifiers
they tend to be even faster in training. The price paid for this efficiency is that naive Bayes models often provide generalization performance that is slightly worse than that of linear classifiers like LogisticRegression and LinearSVC.
The reason that naive Bayes models are so efficient is that they learn parameters by looking at each feature individually and collect simple per-class statistics from each feature.
There are three kinds of naive Bayes classifiers
1. **BernoulliNB classifier**: assumes binary data,counts how often every feature of each class is not zero.
2. **MultinomialNB**: assumes count data (that is, that each feature represents an integer count of something, like how often a word appears in a sentence).
3. **GaussianNB** can be applied to any continuous data
To make a prediction, a data point is compared to the statistics for each of the classes, and the best matching class is predicted.

### Strengths, weaknesses, and parameters
MultinomialNB and BernoulliNB have a single parameter, alpha, which controls
model complexity.The way alpha works is that the algorithm adds to the data alpha many virtual data points that have positive values for all the features. This results in a “smoothing” of the statistics. A large alpha means more smoothing, resulting in less complex models.
<mark>setting alpha is not critical for good performance. However, tuning it usually improves accuracy somewhat.</mark>
GaussianNB is mostly used on very high-dimensional data, while the other two variants of naive Bayes are widely used for sparse count data such as text.
he models work very well with high-dimensional sparse data and are
relatively robust to the parameters. Naive Bayes models are great baseline models and are often used on very large datasets, where training even a linear model might take too long.

---
---
# Decision Trees
### Controlling complexity of decision trees
There are two common strategies to prevent overfitting: stopping the creation of the tree early (also called pre-pruning), or building the tree but then removing or collapsing nodes that contain little information (also called post-pruning or just pruning).
Possible criteria for pre-pruning include limiting the maximum depth of the tree,
limiting the maximum number of leaves, or requiring a minimum number of points
in a node to keep splitting it.
scikit-learn only implements pre-pruning, not post-pruning.
### Feature importance in trees
It is a number between 0 and 1 for each feature, where 0
means “not used at all” and 1 means “perfectly predicts the target.” The feature
importances always sum to 1.

However, if a feature has a low feature_importance, it doesn’t mean that this feature is uninformative. It only means that the feature was not picked by the tree, likely because another feature encodes the same information.


#### DecisionTreeRegressor
though. The DecisionTreeRegressor (and all other tree-based regression models) is not able to extrapolate, or make predictions outside of the range of the training data.
The tree has no ability to generate “new” responses, outside of what was seen in the training data. This shortcoming applies to all models based on trees.

### Strengths, weaknesses, and parameters
Usually, picking one of the pre-pruning strategies—setting either max_depth, max_leaf_nodes, or min_samples_leaf—is sufficient to prevent overfit‐
ting.
no preprocessing like normalization or standardization of features
is needed for decision tree algorithms.
The main downside of decision trees is that even with the use of pre-pruning, they tend to overfit and provide poor generalization performance. Therefore, in most applications, the ensemble methods we discuss next are usually used in place of a single decision tree.

---
---

# Ensembles of Decision Trees
Ensembles are methods that combine multiple machine learning models to create
more powerful models.
## Random forests
a high max_features means that the trees in the random forest will be quite similar, and they will be able to fit the data easily, using the most distinctive features. A low max_features means that the trees in the random forest will be quite different, and that each tree might need to be very deep in order to fit the data well.
The random forest overfits less than any of the trees individually, and provides a
much more intuitive decision boundary. In any real application, we would use many more trees (often hundreds or thousands), leading to even smoother boundaries.
##### Strengths, weaknesses, and parameters.
Random forests don’t tend to perform well on very high dimensional, sparse data,
such as text data.Random forests usually work well even on very large datasets,
The important parameters to adjust are n_estimators, max_features, and possibly
pre-pruning options like max_depth.For n_estimators, larger is always better. Aver‐
aging more trees will yield a more robust ensemble by reducing overfitting. However, there are diminishing returns, and more trees need more memory and more time to train. A common rule of thumb is to build “as many as you have time/memory for.”
max_features determines how random each tree is, and a smaller max_features reduces overfitting. its better to use the default but Adding max_features or max_leaf_nodes might sometimes improve performance. It can also drastically reduce space and timerequirements for training and prediction

## Gradient boosted regression trees (gradient boosting machines)
#SEARCH_MORE_ON_THIS

another ensemble method that combines multiple decision trees to create a more powerful model. these models can be used for regression and classification. radient boosting works by building trees in a serial manner, where each tree tries to correct the mistakes of the previous one.By default, there is no randomization in gradient boosted regression trees; instead, strong pre-pruning is used.
The main idea behind gradient boosting is to combine many simple models (in this
context known as weak learners)
<mark>Gradient boosted trees are frequently the winning entries in machine learning competitions</mark>
They are generally a bit more sensitive to parameter settings than random forests, but can provide better accuracy if the parameters are set correctly.

another important parameter of gradient boosting is the **learning_rate** which controls how strongly each tree tries to correct the mistakes of the previous trees.
A higher learning
rate means each tree can make stronger corrections, allowing for more complex models. Adding more trees to the ensemble, which can be accomplished by increasing n_estimators, also increases the model complexity, as the model has more chances to correct mistakes on the training set.

To reduce <mark>overfitting</mark>, we could either apply stronger pre-pruning by limiting the maximum depth or lower the learning rate

As both gradient boosting and random forests perform well on similar kinds of data,a common approach is to first try random forests, which work quite robustly. If random forests work well but prediction time is at a premium, or it is important <mark>to squeeze out the last percentage of accuracy from the machine learning model, moving to gradient boosting often helps.</mark>


#### Strengths, weaknesses, and parameters.
The main parameters of gradient boosted tree models are the number of trees, n_estimators, and the learning_rate, which controls the degree to which each tree is allowed to correct the mistakes of the previous trees.
lower learning_rate means that more trees are needed to build
a model of similar complexity.
In contrast to random forests, where a higher n_estimators value is always better, increasing n_estimators in gradient boosting leads to a more complex model, which may lead to overfitting.
A common practice is to fit
n_estimators depending on the time and memory budget, and then search over different learning_rates.
Another important parameter is max_depth (or alternatively max_leaf_nodes), to
reduce the complexity of each tree. Usually max_depth is set very low for gradient
boosted models, often not deeper than five splits.

# Kernelized Support Vector Machines
Kernelized support vector machines (often just referred to as SVMs) are an extension that allows for more complex models that are not defined simply by hyperplanes in the input space.
## Linear models and nonlinear features
linear models can be quite limiting in low-dimensional spaces, as lines and hyperplanes have limited flexibility. One way to make a linear model more flexible is by adding more features
## The kernel trick
adding nonlinear features to the representation of our data can make linear models much more powerful. often we don’t know which features to add, and adding many features might make computation very expensive. here where kernel trick comes.
There are two ways to map your data into a higher-dimensional space that are commonly used with support vector machines:
- the polynomial kernel: hich computes all possible polynomials up to a certain degree of the original features
- the radial basis function kernel (The Gaussian kernel):it considers all possible polynomials of all degrees, but the importance of the features decreases for higher degrees. 
## Tuning SVM parameters
The gamma parameter controls the width of the Gaussian kernel. It determines the scale of what it means for points to be close together. high value of gamma yields a more complex model
The C parameter is a regularization parameter, It limits the importance of each point (or more precisely, their dual_coef_).a small C means a very restricted model where each data point can only have very limited influence.

While SVMs often perform quite well, they are very sensitive to the settings of the parameters and to the scaling of the data. they require all the features to vary on a similar scale.

## Preprocessing data for SVMs

One way to resolve this problem is by rescaling each feature so that they are all
approximately on the same scale. A common rescaling method for kernel SVMs is to scale the data such that all features are between 0 and 1
## Strengths, weaknesses, and parameters
SVMs allow for complex decision boundaries, even if the data has only a few features. They work well on low-dimensional and high-dimensional data
downside of SVMs is that they require careful preprocessing of the data and
tuning of the parameters. Still, it might be worth trying SVMs, particularly if all of your features represent measurements in similar units (e.g., all are pixel intensities) and they are on similar scales.

# Neural Networks (Deep Learning)
Multilayer perceptrons (MLPs) are also known as (vanilla) feed-forward neural networks, or sometimes just neural networks.

MLPs can be viewed as generalizations of linear models that perform multiple stages of processing to come to a decision.


We can reduce the number of hidden layers (which reduces the complexity of the model)
we can also control the complexity of a neural network by using an l2 penalty
to shrink the weights toward zero, as we did in ridge regression and the linear classifiers, The parameter for this in the MLPClassifier is alpha and it’s set to a very low value (little regularization) by default.
![](https://i.imgur.com/nm5QFL5.png)
there are many ways to control the complexity of a neural network: the number of hidden layers, the number of units in each hidden layer, and the regularization (alpha). There are actually even more, which we won’t go into here.

An important property of neural networks is that their weights are set randomly
before learning is started, and this random initialization affects the model that is
learned. That means that even when using exactly the same parameters, we can
obtain very different models when using different random seeds.If the networks are large, and their complexity is chosen properly, this should not affect accuracy too much, but it is worth keeping in mind (particularly for smaller networks).

Neural networks also expect all input features to vary in a similar way, and ideally to have a mean of 0, and variance 1.

One way to introspect what was learned is to look at the weights in the model using  Heat map of the first layer weights in a neural network learned on the data, and One possible inference we can make is that features that have very small weights for all of the hidden units are “less important” to the model.

##### Strengths, weaknesses, and parameters
One of their main advantages is that they are able to capture information contained in large amounts of data and build incredibly complex models.
neural networks often beat other machine learning algorithms (for classification and regression tasks).
Similarly to SVMs, they work best with “homogeneous” data, where all the features have similar meanings. For data that has very different kinds of features, tree-based models might work better.

##### Estimating complexity in neural networks.
The most important parameters are the number of layers and the number of hidden units per layer. The number of nodes per hidden layer is often similar to the number of input features, but rarely higher than in the low to mid-thousands.

<mark> A common way to adjust parameters in a neural network is to first create a network that is large enough to overfit, making sure that the task can actually be learned by the network. Then, once you know the training data can be learned, either shrink the network or increase alpha to add regularization, which will improve generalization performance.</mark>


# Uncertainty Estimates from Classifiers
There are two different functions in scikit-learn that can be used to obtain uncer‐
tainty estimates from classifiers: **decision_function** and **predict_proba**.
## The Decision Function
This value encodes how strongly the model believes a data point to belong to the
“positive” class, Positive values indicate a preference for the posi‐
tive class, and negative values indicate a preference for the “negative” (other) class.

We can recover the prediction by looking only at the sign of the decision function

For binary classification, the “negative” class is always the first entry of the classes_ attribute, and the “positive” class is the second entry of classes_.


## Predicting Probabilities
The output of predict_proba is a probability for each class, and is often more easily understood than the output of decision_function. It is always of shape (n_samples, 2) for binary classification.
The first entry in each row is the estimated probability of the first class, and the second entry is the estimated probability of the second class.

A model that is more overfitted tends to make more certain predictions, even if they might be wrong. A model with less complexity usu ally has more uncertainty in its predictions.



# quick summary of when to use each model:

![](https://i.imgur.com/I04XOkF.png)
When working with a new dataset, it is in general a good idea to start with a simple model, such as a linear model or a naive Bayes or nearest neighbors classifier, and see how far you can get. After understanding more about the data, you can consider moving to an algorithm that can build more complex models, such as random forests, gradient boosted decision trees, SVMs, or neural networks.