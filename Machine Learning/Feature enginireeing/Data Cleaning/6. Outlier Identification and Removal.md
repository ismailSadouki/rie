# 6.4 Standard Deviation Method
If we know that the distribution of values in the sample is Gaussian or Gaussian-like, we can use the standard deviation of the sample as a cut-off for identifying outliers.
 1 Standard Deviation from the Mean: 68 percent.
 2 Standard Deviations from the Mean: 95 percent.
 3 Standard Deviations from the Mean: 99.7 percent.
Three standard deviations from the
mean is a common cut-off in practice for identifying outliers in a Gaussian or Gaussian-like distribution.
For smaller samples of data, perhaps a value of 2 standard deviations (95 percent) can be used, and for larger samples, perhaps a value of 4 standard deviations (99.9 percent) can be used.
```
# calculate summary statistics
data_mean, data_std = mean(data), std(data)
# define outliers
cut_off = data_std * 3
lower, upper = data_mean - cut_off, data_mean + cut_off
outliers = [x for x in data if x < lower or x > upper]
```
# 6.5 Interquartile Range Method
A good statistic for summarizing a non-Gaussian distribution sample of data is the Interquartile Range
```
q25, q75 = percentile(data, 25), percentile(data, 75)
iqr = q75 - q25
cut_off = iqr * 1.5
lower, upper = q25 - cut_off, q75 + cut_off
```

# 6.6 Automatic Outlier Detection
A simple approach to identifying outliers is to locate those examples that are far from the other examples in the multi-dimensional feature space. This can work well for feature spaces with low dimensionality (few features), although it can become less reliable as the number of features is increased, referred to as the curse of dimensionality. The local outlier factor, or LOF for short, is a technique that attempts to harness the idea of nearest neighbors for outlier detection. Each example is assigned a scoring of how isolated or how likely it is to be outliers based on the size of its local neighborhood. Those examples with the largest score are more likely to be outliers. The scikit-learn library provides an implementation of this approach in the LocalOutlierFactor class.