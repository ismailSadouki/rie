The seminal papers on gradient boosting are quite readable, containing lots of useful tips for
configuring and applying the algorithm to your problem. Below are a few select papers you may
consider reading.
 Arcing the edge, 1998.
http://goo.gl/Kedl7V
 Boosting Algorithms as Gradient Descent in Function Space, 1999.
http://goo.gl/Tz9hNg
 Greedy Function Approximation: A Gradient Boosting Machine, 1999.
https://goo.gl/5dbi4V
 Stochastic Gradient Boosting, 1999.
https://goo.gl/LHKp4T
 XGBoost: A Scalable Tree Boosting System, 2016 (XGBoost paper).
http://goo.gl/aFfSef



https://www.youtube.com/watch?v=LgLcfZjNF44

https://www.slideshare.net/slideshow/winning-data-science-competitions-presented-by-owen-zhang/44086154

https://www.linkedin.com/pulse/approaching-almost-any-machine-learning-problem-abhishek-thakur/
https://www.youtube.com/watch?v=7YnVZrabTA8

https://github.com/dmlc/xgboost/tree/master/demo


https://www.slideshare.net/slideshow/owen-zhangopen-sourcetoolsanddscompetitions1/49003320


<mark>IMPORTANT</mark>
https://www.youtube.com/watch?v=LgLcfZjNF44
https://www.slideshare.net/slideshow/winning-data-science-competitions-presented-by-owen-zhang/44086154
https://www.youtube.com/watch?v=7YnVZrabTA8
https://www.linkedin.com/pulse/approaching-almost-any-machine-learning-problem-abhishek-thakur/

https://github.com/dmlc/xgboost/tree/master/demo
https://www.slideshare.net/slideshow/owen-zhangopen-sourcetoolsanddscompetitions1/49003320