

# Why consider nonlinear dimensionality reduction?
Many machine learning algorithms make assumptions about the linear separability of the input data.
perceptron even requires perfectly linearly separable training data to con-
verge.
Other algorithms that we have covered so far assume that the lack of perfect linear separability is due to noise: Adaline, logistic regression, and the (standard) SVM to just name a few.
However, if we are dealing with nonlinear problems, which we may encounter rather frequently in real-world applications, linear transformation techniques for dimensionality reduction, such as PCA and LDA, may not be the best choice:
![](https://i.imgur.com/xyrX2BO.png)
The scikit-learn library implements a selection of advanced techniques for nonlinear dimensionality reduction that are beyond the scope of this book.
The development and application of nonlinear dimensionality reduction techniques is also often referred to as manifold learning, where a manifold refers to a lower dimensional topological space embedded in a high-dimensional space. Algorithms for manifold learning have to capture the complicated structure of the data in order to project it onto a lower-dimensional space where the relationship between data points is preserved.
A classic example of manifold learning is the 3-dimensional **Swiss roll** illustrated in Figure 5.14:
![](https://i.imgur.com/lOa2XZO.png)
While nonlinear dimensionality reduction and manifold learning algorithms are very powerful, we should note that these techniques are notoriously hard to use, and with non-ideal hyperparameter choices, they may cause more harm than good.The reason behind this difficulty is that we are often
working with high-dimensional datasets that we cannot readily visualize and where the structure is not obvious.

Moreover, unless we project the dataset into two or three dimensions (which is often not sufficient for capturing more complicated relationships), it is hard or even impossible to assess the quality of the results. Hence, many people still rely on simpler techniques such as PCA and LDA for dimensionality reduction.


# Visualizing data via t-distributed stochastic neighbor embedding
which is often used for visualizing complex datasets in two or three dimensions.
In a nutshell, t-SNE is modeling data points based on their pair-wise distances in the high-dimensional (original) feature space. Then, it finds a probability distribution of pair-wise distances in the new, lower-dimensional space that is close to the probability distribution of pair-wise distances in the original space. Or, in other words, t-SNE learns to embed data points into a lower-dimensional space such that the pairwise distances in the original space are preserved.
<mark>The original paper: https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
</mark>

t-SNE is a technique intended for visualization purposes as it requires the whole dataset for the projection. Since it projects the points directly (unlike PCA, it does not involve a projection matrix), we cannot apply t-SNE to new data points

a quick demonstration of how t-SNE can be applied to a 64-dimensional
dataset.
using low-resolution handwritten digits.
```
from sklearn.datasets import load_digits
digits = load_digits()
```
The digits are 8×8 grayscale images. The following code plots the first four images in the dataset, which consists of 1,797 images in total:
```
fig, ax = plt.subplots(1, 4)
for i in range(4):
    ax[i].imshow(digits.images[i], cmap='Greys')
plt.show()
```
![](https://i.imgur.com/eljWICL.png)
we import the t-SNE class from scikit-learn and fit a new tsne object. Using fit_transform, we perform the t-SNE fitting and data transformation in one step:
```
y_digits = digits.target
X_digits = digits.data

from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, init='pca', random_state=123)
X_digits_tsne = tsne.fit_transform(X_digits)
```
Using this code, we projected the 64-dimensional dataset onto a 2-dimensional space. We specified init='pca', which initializes the t-SNE embedding using PCA as it is recommended in the research article Initialization is critical for preserving global data structure in both t-SNE and UMAP by Kobak and Linderman, https://www.nature.com/articles/s41587-020-00809-z

More information about other parameters and their effects on the results can be found in the excellent article How to Use t-SNE Effectively by Watten-berg, Viegas, and Johnson, Distill, 2016 (https://distill.pub/2016/misread-tsne/).


visualize the 2D t-SNE embeddings using the following code:
```
import matplotlib.patheffects as PathEffects

def plot_projection(x, colors):
    f = plt.figure(figsize=(8, 8))
    ax = plt.subplot(1, 1, 1)
    ax.set_aspect('equal')  # Correct way to set aspect ratio

    for i in range(10):
        ax.scatter(x[colors == i, 0], x[colors == i, 1], label=str(i), alpha=0.6)

    for i in range(10):
        xtext, ytext = np.median(x[colors == i, :], axis=0)
        txt = ax.text(xtext, ytext, str(i), fontsize=24)
        txt.set_path_effects([
            PathEffects.Stroke(linewidth=5, foreground="w"),
            PathEffects.Normal()
        ])
```
Like PCA, t-SNE is an unsupervised method, and in the preceding code, we use the class labels y_digits (0-9) only for visualization purposes via the functions color argument. Matplotlib’s PathEffects are used for visual purposes, such that the class label is displayed in the center (via np.median) of data points belonging to each respective digit. The resulting plot is as follows:
![](https://i.imgur.com/k0adHha.png)
It might be possible to achieve better separation by tuning the hyperparameters. However, a certain degree of class mixing might be unavoidable due to illegible handwriting. For instance, by inspecting individual images, we might find that certain instances of the number 3 indeed look like the number 9, and so forth.

![](https://i.imgur.com/DcfjcEs.png)
