In practice, feature extraction is not only used to improve storage space or the computational efficiency of the learning algorithm but can also improve the predictive performance by reducing the curse of dimensionalityâ€”<mark>especially if we are working with non-regularized models</mark>.
# The main steps in principal component analysis
PCA helps us to identify patterns in data based on the correlation between features.
In a nutshell, PCA aims to find the directions of maximum variance in high-dimensional data and projects the data onto a new subspace with equal or fewer dimensions than the original one.
The orthogonal axes (principal components) of the new subspace can be interpreted as the directions of maximum variance given the constraint that the new feature axes are orthogonal to each other, as illustrated in Figure 5.1:
![](https://i.imgur.com/Wn7ArOR.png)
In Figure 5.1, x1 and x2 are the original feature axes, and PC 1 and PC 2 are the principal components.
If we use PCA for dimensionality reduction, we construct a dÃ—k-dimensional transformation matrix, W, that allows us to map a vector of the features of the training example, x, onto a new k-dimensional feature subspace that has fewer dimensions than the original d-dimensional feature space. For instance, the process is as follows. Suppose we have a feature vector, x:
![](https://i.imgur.com/X8D0F0X.png)
![](https://i.imgur.com/p4ZlQtz.png)
As a result of transforming the original d-dimensional data onto this new k-dimensional subspace (typically k << d), the first principal component will have the largest possible variance. All consequent principal components will have the largest variance given the constraint that these components are uncorrelated (orthogonal) to the other principal componentsâ€”even if the input features are correlated, the resulting principal components will be mutually orthogonal (uncorrelated). <mark>Note that the PCA directions are highly sensitive to data scaling, and we need to standardize the features prior to PCA if the features were measured on different scales and we want to assign equal importance to all features</mark>.
letâ€™s summarize the approach in a few simple steps:
![](https://i.imgur.com/ZQZjr1C.png)

![](https://i.imgur.com/FhAZAwX.png)
# Extracting the principal components step by step
1.Standardizing the data
2.Constructing the covariance matrix
3.Obtaining the eigenvalues and eigenvectors of the covariance matrix
4.Sorting the eigenvalues by decreasing order to rank the eigenvectors

First, we will start by loading and preprocessing the Wine dataset
```
import pandas as pd
df_wine = pd.read_csv(
    'https://archive.ics.uci.edu/ml/'
    'machine-learning-databases/wine/wine.data',
    header=None
 )
 from sklearn.model_selection import train_test_split
X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train_std = sc.fit_transform(X_train)
X_test_std = sc.transform(X_test)
```

The symmetric dÃ—d-dimensional covariance matrix, where d is the number of dimensions in the dataset, stores the pairwise covariances between the different features.
the covariance between two features, xj and xk, on the population level
![](https://i.imgur.com/BRZxc5Z.png)
Note that the sample mean is zero if we standardized the dataset.
For example, the covariance matrix of three features
![](https://i.imgur.com/4UnvcxP.png)
The eigenvectors of the covariance matrix represent the principal components (the directions of maximum variance), whereas the corresponding eigenvalues will define their magnitude. In the case of the Wine dataset, we would obtain 13 eigenvectors and eigenvalues from the 13Ã—13-dimensional covariance matrix.
for our third step, letâ€™s obtain the eigenpairs of the covariance matrix.
![](https://i.imgur.com/Ad6VmCn.png)
Here, ðœ†ðœ† is a scalar: the eigenvalue. Since the manual computation of eigenvectors and eigenvalues is a somewhat tedious and elaborate task, we will use the linalg.eig function from NumPy to obtain the eigenpairs of the Wine covariance matrix:
```
import numpy as np
cov_mat = np.cov(X_train_std.T)
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
print(eigen_vals)
```
Using the numpy.cov function, we computed the covariance matrix of the standardized training dataset.
Using the linalg.eig function, we performed the eigendecomposition.
![](https://i.imgur.com/qwEs53m.png)
# Total and explained variance
Since we want to reduce the dimensionality of our dataset by compressing it onto a new feature sub-space, we only select the subset of the eigenvectors (principal components) that contains most of the information (variance). The eigenvalues define the magnitude of the eigenvectors, so we have to sort the eigenvalues by decreasing magnitude; we are interested in the top k eigenvectors based on the values of their corresponding eigenvalues. But before we collect those k most informative eigenvectors. letâ€™s plot the variance explained ratios of the eigenvalues. The variance explained ratio of an eigenvalue, ðœ†ð‘—, is simply the fraction of an eigenvalue, ðœ†ð‘—, and the total sum of the eigenvalues:
![](https://i.imgur.com/N4xTNxf.png)

![](https://i.imgur.com/cv5VM0k.png)
The resulting plot indicates that the first principal component alone accounts for approximately 40 percent of the variance.
Also, we can see that the first two principal components combined explain almost 60 percent of the variance in the dataset.
we should remind ourselves that PCA is an unsupervised method, which means that information about the class labels is ignored. Whereas a random forest uses the class membership information to compute the node impurities, variance measures the spread of values along a feature axis.

# Feature transformation
letâ€™s proceed with the last three steps to transform the Wine dataset onto the new principal component axes.
1.Select k eigenvectors, which correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace (ð‘˜).
2.Construct a projection matrix, W, from the â€œtopâ€ k eigenvectors.
3.Transform the d-dimensional input dataset, X, using the projection matrix, W, to obtain the new k-dimensional feature subspace.
sorting the eigenpairs by decreasing order of the eigenvalues:
```
# Make a list of (eigenvalue, eigenvector) tuples
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])for i in range(len(eigen_vals))]
# Sort the (eigenvalue, eigenvector) tuples from high to low
eigen_pairs.sort(key=lambda k: k[0], reverse=True)
```
Next, we collect the two eigenvectors that correspond to the two largest eigenvalues, to capture about 60 percent of the variance in this dataset.
In practice, the number of principal components has to be determined by a tradeoff between computational efficiency and the performance of the classifier:
```
w = np.hstack((eigen_pairs[0][1][:, np.newaxis],eigen_pairs[1][1][:, np.newaxis]))
print('Matrix W:\n', w)
```
By executing the preceding code, we have created a 13Ã—2-dimensional projection matrix, W, from the top two eigenvectors.
![](https://i.imgur.com/orKjb9L.png)
Using the projection matrix, we can now transform an example, x (represented as a 13-dimensional row vector), onto the PCA subspace (the principal components one and two) obtaining xâ€², now a two-dimensional example vector consisting of two new features:
![](https://i.imgur.com/np7vdpB.png)
```
X_train_std[0].dot(w)
```
Similarly, we can transform the entire 124Ã—13-dimensional training dataset onto the two principal components by calculating the matrix dot product:
![](https://i.imgur.com/2ldUjg5.png)
```
X_train_pca = X_train_std.dot(w)
```
![](https://i.imgur.com/0j6YRkd.png)
As we can see in Figure 5.3, the data is more spread along the first principal component (x axis) than the second principal component (y axis), which is consistent with the explained variance ratio plot that we created in the previous subsection. However, we can tell that a linear classifier will likely be able to separate the classes well.
Although we encoded the class label information for the purpose of illustration in the preceding scatterplot, we have to keep in mind that PCA is an unsupervised technique that doesnâ€™t use any class label information.
# Principal component analysis in scikit-learn
The PCA class is another one of scikit-learnâ€™s transformer classes.
```
>>> pca = PCA(n_components=2)
>>> lr = LogisticRegression(multi_class='ovr',
...random_state=1,
...solver='lbfgs')
>>> # dimensionality reduction:
>>> X_train_pca = pca.fit_transform(X_train_std)
>>> X_test_pca = pca.transform(X_test_std)
>>> # fitting the logistic regression model on the reduced dataset:
>>> lr.fit(X_train_pca, y_train)
```
When we compare the PCA projections via scikit-learn with our own PCA implementation, we might see that the resulting plots are mirror images of each other. Note that this is not due to an error in either of those two implementations; the reason for this difference is that, depending on the eigensolver, eigenvectors can have either negative or positive signs.
If we are interested in the explained variance ratios of the different principal components, we can simply initialize the PCA class with the n_components parameter set to None, so all principal components are kept and the explained variance ratio can then be accessed via the explained_variance_ratio_ attribute:
```
>>> pca = PCA(n_components=None)
>>> X_train_pca = pca.fit_transform(X_train_std)
>>> pca.explained_variance_ratio_
array([ 0.36951469, 0.18434927, 0.11815159, 0.07334252,
```
Note that we set n_components=None when we initialized the PCA class so that it will return all principal components in a sorted order, instead of performing a dimensionality reduction.
# Assessing feature contributions
In this section, we will take a brief look at how we can assess the contributions of the original features to the principal components. As we learned, via PCA, we create principal components that represent linear combinations of the features. Sometimes, we are interested to know about how much each orig-inal feature contributes to a given principal component. These contributions are often called loadings.
The factor loadings can be computed by scaling the eigenvectors by the square root of the eigenvalues. The resulting values can then be interpreted as the correlation between the original features and the principal component. To illustrate this, let us plot the loadings for the first principal component.
```
loadings = eigen_vecs * np.sqrt(eigen_vals)
>>> fig, ax = plt.subplots()
>>> ax.bar(range(13), loadings[:, 0], align='center')
>>> ax.set_ylabel('Loadings for PC 1')
>>> ax.set_xticks(range(13))
>>> ax.set_xticklabels(df_wine.columns[1:], rotation=90)
>>> plt.ylim([-1, 1])
>>> plt.tight_layout()
>>> plt.show()
```

![](https://i.imgur.com/Ft1vATC.png)
In Figure 5.6, we can see that, for example, Alcohol has a negative correlation with the first principal component (approximately â€“0.3), whereas Malic acid has a positive correlation (approximately 0.54). Note that a value of 1 describes a perfect positive correlation whereas a value of â€“1 corresponds to a perfect negative correlation.

We can obtain the loadings from a fitted scikit-learn PCA object in a similar manner, where pca. components_ represents the eigenvectors and pca.explained_variance_ represents the eigenvalues:
```
>>> sklearn_loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
```