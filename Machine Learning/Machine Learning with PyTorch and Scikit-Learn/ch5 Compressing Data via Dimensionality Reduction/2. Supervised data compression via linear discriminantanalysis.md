LDA can be used as a technique for feature extraction to increase computational efficiency and reduce the degree of overfitting due to the curse of dimensionality in non-regularized models.
whereas PCA attempts to find the orthogonal component axes of maximum variance in a dataset, the goal in LDA is to find the feature subspace that optimizes class separability.
# PCA Vs LDA
Both PCA and LDA are linear transformation techniques that can be used to reduce the number of dimensions in a dataset; the former is an unsupervised algorithm, whereas the latter is supervised.
A.M. Martinez reported that preprocessing via PCA tends to result in better clas-
sification results in an image recognition task in certain cases, for instance, if each class consists of only a small number of examples
![](https://i.imgur.com/4LhLzwD.png)
A linear discriminant, as shown on the x axis (LD 1), would separate the two normal distributed classes well. Although the exemplary linear discriminant shown on the y axis (LD 2) captures a lot of the variance in the dataset, it would fail as a good linear discriminant since it does not capture any of the class-discriminatory information.
One assumption in LDA is that the data is normally distributed. Also, we assume that the classes have identical covariance matrices and that the training examples are statistically independent of each other. However, even if one, or more, of those assumptions is (slightly) violated, LDA for dimensionality reduction can still work reasonably well.
# The inner workings of linear discriminant analysis
the main steps that are required to perform LDA:
![](https://i.imgur.com/ozo3WbF.png)
LDA takes class label information into account, which is represented in the form
of the mean vectors computed in step 2.
# Computing the scatter matrices
Each mean vector, mi, stores the mean feature value, ùúáùëö , with respect to the examples of class i:
![](https://i.imgur.com/rEedyyQ.png)
This results in three mean vectors:
![](https://i.imgur.com/sFeQe2u.png)
Using the mean vectors, we can now compute the within-class scatter matrix, $S_W$:
![](https://i.imgur.com/eVjvYVI.png)
![](https://i.imgur.com/PKHbyk3.png)
```
d = 13 # number of features
S_W = np.zeros((d, d))
for label, mv in zip(range(1, 4), mean_vecs):
    class_scatter = np.zeros((d, d))
    for row in X_train_std[y_train == label]:
        row, mv = row.reshape(d, 1), mv.reshape(d, 1)
        class_scatter += (row - mv).dot((row - mv).T)
    S_W += class_scatter
print('Within-class scatter matrix: 'f'{S_W.shape[0]}x{S_W.shape[1]}')
#Within-class scatter matrix: 13x13
```
The assumption that we are making when we are computing the scatter matrices is that the class labels in the training dataset are uniformly distributed. However, if we print the number of class labels, we see that this assumption is violated:
```
>>> print('Class label distribution:',
...
np.bincount(y_train)[1:])
Class label distribution: [41 50 33]
```
Thus, we want to scale the individual scatter matrices, Si, before we sum them up as the scatter matrix, SW. When we divide the scatter matrices by the number of class-examples, ni, we can see that computing the scatter matrix is in fact the same as computing the covariance matrix, Œ£ùëñ ‚Äîthe covariance matrix is a normalized version of the scatter matrix:
![](https://i.imgur.com/20dCn5D.png)
```
d = 13 # number of features
S_W = np.zeros((d, d))
for label,mv in zip(range(1, 4), mean_vecs):
  class_scatter = np.cov(X_train_std[y_train==label].T)
  S_W += class_scatter
print('Scaled within-class scatter matrix: 'f'{S_W.shape[0]}x{S_W.shape[1]}')
# Scaled within-class scatter matrix: 13x13
```
After we compute the scaled within-class scatter matrix (or covariance matrix), we can move on to the next step and compute the between-class scatter matrix $S_B$:
![](https://i.imgur.com/SoWMvwE.png)
Here, m is the overall mean that is computed, including examples from all c classes:
```
mean_overall = np.mean(X_train_std, axis=0)
mean_overall = mean_overall.reshape(d, 1)
d = 13 # number of features
S_B = np.zeros((d, d))
for i, mean_vec in enumerate(mean_vecs):
    n = X_train_std[y_train == i + 1, :].shape[0]
    mean_vec = mean_vec.reshape(d, 1) # make column vector
    S_B += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)
print('Between-class scatter matrix: 'f'{S_B.shape[0]}x{S_B.shape[1]}')
```
# Selecting linear discriminants for the new feature subspace
The remaining steps of the LDA are similar to the steps of the PCA. However, instead of performing the eigendecomposition on the covariance matrix, we solve the generalized eigenvalue problem of the matrix
![](https://i.imgur.com/IRP54u6.png)
```
eigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i])for i in range(len(eigen_vals))]
eigen_pairs = sorted(eigen_pairs,key=lambda k: k[0], reverse=True)
print('Eigenvalues in descending order:\n')
for eigen_val in eigen_pairs:
    print(eigen_val[0])
```
In LDA, the number of linear discriminants is at most c ‚Äì 1, where c is the number of class labels, since the in-between scatter matrix, SB, is the sum of c matrices with rank one or less. We can indeed see that we only have two nonzero eigenvalues (the eigenvalues 3-13 are not exactly zero, but this is due to the floating-point arithmetic in NumPy.)
![Uploading file...xi0il]()
![[Pasted image 20250612131825.png]]To measure how much of the class-discriminatory information is captured by the linear discriminants (eigenvectors), let‚Äôs plot the linear discriminants by decreasing eigenvalues, similar to the explained variance plot that we created in the PCA section.
```
tot = sum(eigen_vals.real)
discr = [(i / tot) for i in sorted(eigen_vals.real, reverse=True)]

cum_discr = np.cumsum(discr)
plt.bar(range(1, 14), discr, align='center',

label='Individual discriminability')
plt.step(range(1, 14), cum_discr, where='mid',label='Cumulative discriminability')

plt.ylabel('"Discriminability" ratio')
plt.xlabel('Linear Discriminants')
plt.ylim([-0.1, 1.1])
plt.legend(loc='best')
plt.tight_layout()
plt.show()
```
![](https://i.imgur.com/fRMiQwP.png)
Let‚Äôs now stack the two most discriminative eigenvector columns to create the transformation matrix, W:
```
w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real,eigen_pairs[1][1][:, np.newaxis].real))
```
# Projecting examples onto the new feature space
![](https://i.imgur.com/hIEdDHN.png)
```
X_train_lda = X_train_std.dot(w)
colors = ['r', 'b', 'g']
markers = ['o', 's', '^']
for l, c, m in zip(np.unique(y_train), colors, markers):
    plt.scatter(X_train_lda[y_train==l, 0],X_train_lda[y_train==l, 1] * (-1),c=c, label= f'Class {l}', marker=m)
    plt.xlabel('LD 1')
    plt.ylabel('LD 2')
    plt.legend(loc='lower right')
    plt.tight_layout()
    plt.show()
```
![](https://i.imgur.com/AfwcPh1.png)
# LDA via scikit-learn
```
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=2)
X_train_lda = lda.fit_transform(X_train_std, y_train)
```
Next, let‚Äôs see how the logistic regression classifier handles the lower-dimensional training dataset after the LDA transformation:
![](https://i.imgur.com/O7IHg9O.png)
logistic regression model misclassifies one of the examples from class 2.
By lowering the regularization strength, we could probably shift the decision boundaries so that the logistic regression model classifies all examples in the training dataset correctly.