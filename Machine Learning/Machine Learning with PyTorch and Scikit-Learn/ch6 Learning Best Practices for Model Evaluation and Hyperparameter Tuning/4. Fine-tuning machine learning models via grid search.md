In machine learning, we have two types of parameters: those that are learned from the training data, for example, the weights in logistic regression, and the parameters of a learning algorithm that are optimized separately. The latter are the tuning parameters (or hyperparameters) of a model.

`grid search` finds the optimal combination of hyperparameter values.

# Tuning hyperparameters via grid search
it’s a brute-force exhaustive search paradigm where we specify a list of values for different hyperparameters, and the computer evaluates the model performance for each combination to obtain the optimal combination of values from this set:
```
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

pipe_svc = make_pipeline(StandardScaler(), SVC(random_state=1))
param_range = [0.0001, 0.001, 0.01, 0.1,1.0, 10.0, 100.0, 1000.0]
param_grid = [{'svc__C': param_range, 'svc__kernel': ['linear']}, {'svc__C': param_range,'svc__gamma': param_range,'svc__kernel': ['rbf']}]

gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=10, refit=True, n_jobs=-1)
gs = gs.fit(X_train, y_train)
print(gs.best_score_)
print(gs.best_params_)
```
We set the param_grid parameter of GridSearchCV to a list of dictionaries to specify the parameters that we’d want to tune. For the linear SVM, we only evaluated the inverse regularization parameter, C; for the radial basis function (RBF) kernel SVM, we tuned both the svc__C and svc__gamma parameters. Note that the svc__gamma parameter is specific to kernel SVMs.
GridSearchCV uses k-fold cross-validation for comparing models trained with different hyperparameter settings. Via the cv=10 setting, it will carry out 10-fold cross-validation and compute the average accuracy (via scoring='accuracy') across these 10-folds to assess the model performance.

Finally, we use the independent test dataset to estimate the performance of the best-selected model, which is available via the best_estimator_ attribute of the GridSearchCV object:
```
clf = gs.best_estimator_
clf.fit(X_train, y_train)
print(f'Test accuracy: {clf.score(X_test, y_test):.3f}')
```
Please note that fitting a model with the best settings (gs.best_estimator_) on the training set manually via clf.fit(X_train, y_train) after completing the grid search is not necessary. The GridSearchCV class has a refit parameter, which will refit the gs.best_estimator_ to the whole training set automatically if we set refit=True (default).
# Exploring hyperparameter configurations more widely with randomized search
Figure 6.7, which shows a fixed grid of nine hyperparameter settings being searched via grid search and randomized search:
![](https://i.imgur.com/mBDSlAN.png)
The main takeaway is that while grid search only explores discrete, user-specified choices, it may miss good hyperparameter configurations if the search space is too scarce. Interested readers can find additional details about randomized search, along with empirical studies, in the following article:
Random Search for Hyper-Parameter Optimization by J. Bergstra, Y. Bengio, Journal of Machine Learning Research, pp. 281-305, 2012, https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.


Scikit-learn implements a RandomizedSearchCV class, which is analogous to the GridSearchCV we used in the previous subsection. The main difference is that we can specify distributions as part of our parameter grid and specify the total number of hyperparameter configurations to be evaluated.
example in the previous section with gridsearch:
```
param_range = [0.0001, 0.001, 0.01, 0.1,1.0, 10.0, 100.0, 1000.0]
```
Note that while RandomizedSearchCV can accept similar discrete lists of values as inputs for the parameter grid, which is useful when considering categorical hyperparameters, its main power lies in the fact that we can replace these lists with distributions to sample from. Thus, for example, we may substitute the preceding list with the following distribution from SciPy:
```
param_range = scipy.stats.loguniform(0.0001, 1000.0)
```
For instance, using a loguniform distribution instead of a regular uniform distribution will ensure that in a sufficiently large number of trials, the same number of samples will be drawn from the [0.0001, 0.001] range as, for example, the [10.0, 100.0] range. To check its behavior, we can draw 10
random samples from this distribution via the rvs(10) method, as shown here:
```
>>> np.random.seed(1)
>>> param_range.rvs(10)
array([8.30145146e-02, 1.10222804e+01, 1.00184520e-04, 1.30715777e-02,
1.06485687e-03, 4.42965766e-04, 2.01289666e-03, 2.62376594e-02,
5.98924832e-02, 5.91176467e-01])
```
![](https://i.imgur.com/W4KgMZc.png)
une an SVM as we did with RandomizedSearchCV :
```
from sklearn.model_selection import RandomizedSearchCV
pipe_svc = make_pipeline(StandardScaler(),SVC(random_state=1))
param_grid = [{'svc__C': param_range,'svc__kernel': ['linear']},{'svc__C': param_range,'svc__gamma': param_range,'svc__kernel': ['rbf']}]
rs = RandomizedSearchCV(estimator=pipe_svc,param_distributions=param_grid,scoring='accuracy',refit=True,n_iter=20,cv=10,random_state=1,n_jobs=-1)
rs = rs.fit(X_train, y_train)
print(rs.best_score_)
print(rs.best_params_)
```
we can see that the usage is very similar to GridSearchCV, except that we
could use distributions for specifying parameter ranges and specified the number of iterations—20 iterations—by setting n_iter=20.


# More resource-efficient hyperparameter search with successive halving
HalvingRandomSearchCV, that makes finding suitable hyperparameter configurations more efficient. Successive halving, given a large set of candidate configurations, successively throws out unpromising hyperparameter configurations until only one configuration remains. 
1.Draw a large set of candidate configurations via random sampling
2.Train the models with limited resources, for example, a small subset of the training data (as opposed to using the entire training set)
3.Discard the bottom 50 percent based on predictive performance
4.Go back to step 2 with an increased amount of available resources

The steps are repeated until only one hyperparameter configuration remains. Note that there is also a successive halving implementation for the grid search variant called HalvingGridSearchCV, where all specified hyperparameter configurations are used in step 1 instead of random samples.
```
from sklearn.model_selection import HalvingRandomSearchCV  # The actual import
hs = HalvingRandomSearchCV(pipe_svc, param_distributions=param_grid,n_candidates='exhaust',resource='n_samples',factor=1.5,random_state=1,n_jobs=-1)
```
The resource='n_samples' (default) setting specifies that we consider the training set size as the resource we vary between the rounds. Via the factor parameter, we can determine how many candidates are eliminated in each round. For example, setting factor=2 eliminates half of the candidates, and set-ting factor=1.5 means that only 100%/1.5 ≈ 66% of the candidates make it into the next round. Instead of choosing a fixed number of iterations as in RandomizedSearchCV, we set n_candidates='exhaust' (default), which will sample the number of hyperparameter configurations such that the maximum number of resources (here: training examples) are used in the last round
We can then carry out the search similar to RandomizedSearchCV:
```
hs = hs.fit(X_train, y_train)
print(hs.best_score_)
print(hs.best_params_)
clf = hs.best_estimator_
print(f'Test accuracy: {hs.score(X_test, y_test):.3f}')
```
In this data, If we compare the results from GridSearchCV and RandomizedSearchCV from the previous two subsections with the model from HalvingRandomSearchCV, we can see that the latter yields a model that performs slightly better on the test set (98.2 percent accuracy as opposed to 97.4 percent).
![](https://i.imgur.com/banZdol.png)

# Algorithm selection with nested cross-validation
Using k-fold cross-validation in combination with grid search or randomized search is a useful approach for fine-tuning the performance of a machine learning model by varying its hyperparameter values, as we saw in the previous subsections. If we want to select among different machine learning algorithms, though, another recommended approach is nested cross-validation. In a nice study on the bias in error estimation, Sudhir Varma and Richard Simon concluded that the true error of the estimate is almost unbiased relative to the test dataset when nested cross-validation is used (Bias in Error Estimation When Using Cross-Validation for Model Selection by S. Varma and R. Si-mon, BMC Bioinformatics, 7(1): 91, 2006, https://bmcbioinformatics.biomedcentral.com/articl
es/10.1186/1471-2105-7-91).

In nested cross-validation, we have an outer k-fold cross-validation loop to split the data into training and test folds, and an inner loop is used to select the model using k-fold cross-validation on the training fold. After model selection, the test fold is then used to evaluate the model performance. Figure 6.8 explains the concept of nested cross-validation with only five outer and two inner folds, which can be useful for large datasets where computational performance is important; this particular type of nested cross-validation is also known as 5×2 cross-validation:
![](https://i.imgur.com/F1HFpq1.png)
we can perform nested cross-validation with grid search as follows:
```
param_range = [0.0001, 0.001, 0.01, 0.1,1.0, 10.0, 100.0, 1000.0]
param_grid = [{'svc__C': param_range,'svc__kernel': ['linear']},{'svc__C': param_range,'svc__gamma': param_range,'svc__kernel': ['rbf']}]
gs = GridSearchCV(estimator=pipe_svc,param_grid=param_grid,scoring='accuracy',cv=2)
scores = cross_val_score(gs, X_train, y_train,scoring='accuracy', cv=5)
print(f'CV accuracy: {np.mean(scores):.3f} 'f'+/- {np.std(scores):.3f}')
# CV accuracy: 0.974 +/- 0.015
```
The returned average cross-validation accuracy gives us a good estimate of what to expect if we tune the hyperparameters of a model and use it on unseen data.
For example, we can use the nested cross-validation approach to compare an SVM model to a simple decision tree classifier; for simplicity, we will only tune its depth parameter:
```
from sklearn.tree import DecisionTreeClassifier
gs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0),param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None]}],scoring='accuracy',cv=2)
scores = cross_val_score(gs, X_train, y_train,scoring='accuracy', cv=5)
print(f'CV accuracy: {np.mean(scores):.3f} 'f'+/- {np.std(scores):.3f}')
# CV accuracy: 0.934 +/- 0.016
```
As we can see, the nested cross-validation performance of the SVM model (97.4 percent) is notably better than the performance of the decision tree (93.4 percent), and thus, we’d expect that it might be the better choice to classify new data that comes from the same population as this particular dataset.
