In machine learning, we have two types of parameters: those that are learned from the training data, for example, the weights in logistic regression, and the parameters of a learning algorithm that are optimized separately. The latter are the tuning parameters (or hyperparameters) of a model.

`grid search` finds the optimal combination of hyperparameter values.

# Tuning hyperparameters via grid search
it’s a brute-force exhaustive search paradigm where we specify a list of values for different hyperparameters, and the computer evaluates the model performance for each combination to obtain the optimal combination of values from this set:
```
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

pipe_svc = make_pipeline(StandardScaler(), SVC(random_state=1))
param_range = [0.0001, 0.001, 0.01, 0.1,1.0, 10.0, 100.0, 1000.0]
param_grid = [{'svc__C': param_range, 'svc__kernel': ['linear']}, {'svc__C': param_range,'svc__gamma': param_range,'svc__kernel': ['rbf']}]

gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=10, refit=True, n_jobs=-1)
gs = gs.fit(X_train, y_train)
print(gs.best_score_)
print(gs.best_params_)
```
We set the param_grid parameter of GridSearchCV to a list of dictionaries to specify the parameters that we’d want to tune. For the linear SVM, we only evaluated the inverse regularization parameter, C; for the radial basis function (RBF) kernel SVM, we tuned both the svc__C and svc__gamma parameters. Note that the svc__gamma parameter is specific to kernel SVMs.
GridSearchCV uses k-fold cross-validation for comparing models trained with different hyperparameter settings. Via the cv=10 setting, it will carry out 10-fold cross-validation and compute the average accuracy (via scoring='accuracy') across these 10-folds to assess the model performance.

Finally, we use the independent test dataset to estimate the performance of the best-selected model, which is available via the best_estimator_ attribute of the GridSearchCV object:
```
clf = gs.best_estimator_
clf.fit(X_train, y_train)
print(f'Test accuracy: {clf.score(X_test, y_test):.3f}')
```
Please note that fitting a model with the best settings (gs.best_estimator_) on the training set manually via clf.fit(X_train, y_train) after completing the grid search is not necessary. The GridSearchCV class has a refit parameter, which will refit the gs.best_estimator_ to the whole training set automatically if we set refit=True (default).
# Exploring hyperparameter configurations more widely with randomized search
Figure 6.7, which shows a fixed grid of nine hyperparameter settings being searched via grid search and randomized search:
![](https://i.imgur.com/mBDSlAN.png)
The main takeaway is that while grid search only explores discrete, user-specified choices, it may miss good hyperparameter configurations if the search space is too scarce. Interested readers can find additional details about randomized search, along with empirical studies, in the following article:
Random Search for Hyper-Parameter Optimization by J. Bergstra, Y. Bengio, Journal of Machine Learning Research, pp. 281-305, 2012, https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.


Scikit-learn implements a RandomizedSearchCV class, which is analogous to the GridSearchCV we used in the previous subsection. The main difference is that we can specify distributions as part of our parameter grid and specify the total number of hyperparameter configurations to be evaluated.
example in the previous section with gridsearch:
```
param_range = [0.0001, 0.001, 0.01, 0.1,1.0, 10.0, 100.0, 1000.0]
```
Note that while RandomizedSearchCV can accept similar discrete lists of values as inputs for the parameter grid, which is useful when considering categorical hyperparameters, its main power lies in the fact that we can replace these lists with distributions to sample from. Thus, for example, we may substitute the preceding list with the following distribution from SciPy:
```
param_range = scipy.stats.loguniform(0.0001, 1000.0)
```
For instance, using a loguniform distribution instead of a regular uniform distribution will ensure that in a sufficiently large number of trials, the same number of samples will be drawn from the [0.0001, 0.001] range as, for example, the [10.0, 100.0] range. To check its behavior, we can draw 10
random samples from this distribution via the rvs(10) method, as shown here:
```
>>> np.random.seed(1)
>>> param_range.rvs(10)
array([8.30145146e-02, 1.10222804e+01, 1.00184520e-04, 1.30715777e-02,
1.06485687e-03, 4.42965766e-04, 2.01289666e-03, 2.62376594e-02,
5.98924832e-02, 5.91176467e-01])
```
![](https://i.imgur.com/W4KgMZc.png)
une an SVM as we did with RandomizedSearchCV :
```
from sklearn.model_selection import RandomizedSearchCV
pipe_svc = make_pipeline(StandardScaler(),SVC(random_state=1))
param_grid = [{'svc__C': param_range,'svc__kernel': ['linear']},{'svc__C': param_range,'svc__gamma': param_range,'svc__kernel': ['rbf']}]
rs = RandomizedSearchCV(estimator=pipe_svc,param_distributions=param_grid,scoring='accuracy',refit=True,n_iter=20,cv=10,random_state=1,n_jobs=-1)
rs = rs.fit(X_train, y_train)
print(rs.best_score_)
print(rs.best_params_)
```
we can see that the usage is very similar to GridSearchCV, except that we
could use distributions for specifying parameter ranges and specified the number of iterations—20 iterations—by setting n_iter=20.


# More resource-efficient hyperparameter search with successive halving