![](https://i.imgur.com/rsFxEyi.png)
Using the decision algorithm, we start at the tree root and split the data on the feature that results in the largest **information gain (IG)**, In an iterative process, we can then repeat this splitting procedure at each child node until the leaves are
pure. This means that the training examples at each node all belong to the same class. In practice, this can result in a very deep tree with many nodes, which can easily lead to overfitting. Thus, we typically want to prune the tree by setting a limit for the maximum depth of the tree.

# Maximizing IG ‚Äì getting the most bang for your buck
To split the nodes at the most informative features, we need to define an objective function to optimize via the tree learning algorithm. Here, our objective function is to maximize the IG at each split, which we define as follows:
![](https://i.imgur.com/KrzSca8.png)
Here, $f$ is the feature to perform the split; $D_p$ and $D_j$ are the dataset of the parent and jth child node; $I$ is our **impurity** measure; $N_p$ is the total number of training examples at the parent node; and $N_j$ is the number of examples in the jth child node. As we can see, the **information gain** is simply the difference between the impurity of the parent node and the sum of the child node impurities‚Äîthe lower the impurities of the child nodes, the larger the information gain. However, for simplicity and to reduce the combinatorial search space, most libraries (including scikit-learn) implement binary decision trees. This means that each parent node is split into two child nodes, **$D_{left}$** and **$D_{right}$**:
![](https://i.imgur.com/qtkSzz3.png)
The three impurity measures or splitting criteria that are commonly used in binary decision trees are Gini impurity ($I_G$), entropy ($I_H$), and the classification error ($I_E$). Let‚Äôs start with the definition of entropy for all non-empty classes ($ùëù(ùëñ|ùë°) ‚â† 0$):
![](https://i.imgur.com/Lai8jLs.png)
Here, $p(i|t)$ is the proportion of the examples that belong to class i for a particular node, t. The entropy is therefore 0 if all examples at a node belong to the same class, and the entropy is maximal if we have a uniform class distribution.
For example, in a binary class setting, the entropy is 0 if p(i=1|t) = 1 or
p(i=0|t) = 0. If the classes are distributed uniformly with p(i=1|t) = 0.5 and p(i=0|t) = 0.5, the entropy is 1. Therefore, we can say that the entropy criterion attempts to maximize the mutual information in the tree.
To provide a visual intuition, let us visualize the entropy values for different class distributions via the following code:
```
>>> def entropy(p):
...
return - p * np.log2(p) - (1 - p) * np.log2((1 - p))
>>> x = np.arange(0.0, 1.0, 0.01)
>>> ent = [entropy(p) if p != 0 else None for p in x]
>>> plt.ylabel('Entropy')
>>> plt.xlabel('Class-membership probability p(i=1)')
>>> plt.plot(x, ent)
>>> plt.show()
```
![](https://i.imgur.com/kWhK6OH.png)
The Gini impurity can be understood as a criterion to minimize the probability of misclassification:
![](https://i.imgur.com/kje0if7.png)
Similar to entropy, the Gini impurity is maximal if the classes are perfectly mixed.
However, in practice, both the Gini impurity and entropy typically yield very similar results, and it is often not worth spending much time on evaluating trees using different impurity criteria rather than experimenting with different pruning cut-offs.
both the Gini impurity and entropy have a similar shape.
Another impurity measure is the classification error:
![](https://i.imgur.com/TiyYRzH.png)
This is a useful criterion for pruning, but not recommended for growing a decision tree, since it is less sensitive to changes in the class probabilities of the nodes.

We can illustrate this by looking at the two possible splitting scenarios shown in Figure 3.20:
![](https://i.imgur.com/Vj7Wv1h.png)
For a visual comparison of the three different impurity criteria that we discussed previously, let‚Äôs plot the impurity indices for the probability range [0, 1] for class 1. Note that we will also add ascaled version of the entropy (entropy / 2) to observe that the Gini impurity is an intermediate measure between entropy and the classification error:
![](https://i.imgur.com/AmEpb5B.png)

# Building a decision tree
Decision trees can build complex decision boundaries by dividing the feature space into rectangles.
the deeper the decision tree, the more complex the decision boundary becomes, which can easily result in overfitting.

Using scikit-learn, we will now train a decision tree with a maximum depth of 4, using the Gini impurity as a criterion for impurity.
```
from sklearn.tree import DecisionTreeClassifier
tree_model = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)
tree_model.fit(X_train, y_train)
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined,
      y_combined,
      classifier=tree_model,
      test_idx=range(105, 150))
plt.xlabel('Petal length [cm]')
plt.ylabel('Petal width [cm]')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()
```
![](https://i.imgur.com/gvgsmm4.png)
![](https://i.imgur.com/kuwu4yT.png)
# Combining multiple decision trees via random forests
The idea behind a random forest is to average multiple (deep) deci-
sion trees that individually suffer from high variance to build a more robust model that has a better generalization performance and is less susceptible to overfitting. The random forest algorithm can be summarized in four simple steps:
![](https://i.imgur.com/vLMHRuz.png)
We should note one slight modification in step 2 when we are training the individual decision trees:
instead of evaluating all features to determine the best split at each node, we only consider a random subset of those.

a big advantage of random forests is that we don‚Äôt have to worry so much about choosing good hyperparameter values like in decision trees. We typically don‚Äôt need to prune the random forest since the ensemble model is quite robust to noise from averaging the predictions among the individual decision trees.
The only parameter that we need to care about in practice is the number of trees, k, (step 3) that we choose for the random forest. Typically, the larger the number of trees, the better the performance of the random forest classifier at the expense of an increased computational cost.
other hyperparameters of the random forest classifier that
can be optimized are the size, n, of the bootstrap sample (step 1) and the number
of features, d, that are randomly chosen for each split (step 2a), respectively. Via the sample size, n, of the bootstrap sample, we control the bias-variance tradeoff of the random forest.

Decreasing the size of the bootstrap sample increases the diversity among the individual trees since the probability that a particular training example is included in the bootstrap sample is lower. Thus, <mark>shrinking the size of the bootstrap samples may increase the randomness of the random forest, and it can help to reduce the effect of overfitting</mark>.
In most implementations, including the RandomForestClassifier implementation in scikit-learn, the size of the bootstrap sample is chosen to be equal to the number of training examples in the original training dataset, which usually provides a good bias-variance tradeoff. For the number of features, d, at each split, we want to choose a value that is smaller than the total number of features in the training dataset. A reasonable default that is used in scikit-learn and other implementations is ùëë=‚àöùëö , where m is the number of features in the training dataset. Conveniently, we don‚Äôt have to construct the random forest