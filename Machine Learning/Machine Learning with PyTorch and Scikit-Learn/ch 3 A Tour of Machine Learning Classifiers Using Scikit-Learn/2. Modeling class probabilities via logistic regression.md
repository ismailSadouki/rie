# Logistic regression and conditional probabilities
performs very well on linearly separable classes.
**odds**: the odds in favor of a particular event. The odds can be written as
$\frac{ğ‘}{1âˆ’ğ‘}$ , where p stands for the probability of the positive event.The term â€œpositive eventâ€ does not necessarily mean â€œgood,â€ but refers to the event that we want to predict.
We can then further define the logit function, which is simply the logarithm of the odds (log-odds):
![](https://i.imgur.com/rJAodaD.png)
The logit function takes input values in the range 0 to 1 and transforms them into values over the entire real-number range.

Under the logistic model, we assume that there is a linear relationship between the weighted inputs (referred to as net inputs in Chapter 2) and the log-odds:
![](https://i.imgur.com/2zav2c1.png)
what we are actually interested in is the probability p, the class-member-
ship probability of an example given its features. While the logit function maps the probability to a real-number range, we can consider the inverse of this function to map the real-number range back to a [0, 1] range for the probability p.

This inverse of the logit function is typically called the <mark>logistic sigmoid function</mark>, which is sometimes simply abbreviated to **sigmoid function** due to its characteristic **S-shape**:
![](https://i.imgur.com/SEWkb94.png)
Here, z is the net input, the linear combination of weights, and the inputs
![](https://i.imgur.com/u9azDjh.png)

To build some understanding of the logistic regression model, we can relate it to Chapter 2. In Adaline, we used the identity function, ğœ(ğ‘§) = ğ‘§, as the activation function. In logistic regression, this activation function simply becomes the sigmoid function that we define earlier.
![](https://i.imgur.com/QlIuSIO.png)
The output of the sigmoid function is then interpreted as the probability of a particular ex-ample belonging to class 1, ğœ(ğ‘§) = ğ‘(ğ‘¦=1|ğ’™;w,b) , given its features, x, and parameterized by w and b.
The predicted probability can then simply be converted into a binary outcome via a threshold function:
![](https://i.imgur.com/h5x1lsN.png)
If we look at the preceding plot of the sigmoid function, this is equivalent to the following:
![](https://i.imgur.com/pc5Ktto.png)
In fact, there are many applications where we are not only interested in the predicted class labels, but where the estimation of the class-membership probability is particularly useful (the output of the sigmoid function prior to applying the threshold function).



# Learning the model weights via the logistic loss function
