	# Logistic regression and conditional probabilities
performs very well on linearly separable classes.
**odds**: the odds in favor of a particular event. The odds can be written as
$\frac{ùëù}{1‚àíùëù}$ , where p stands for the probability of the positive event.The term ‚Äúpositive event‚Äù does not necessarily mean ‚Äúgood,‚Äù but refers to the event that we want to predict.
We can then further define the logit function, which is simply the logarithm of the odds (log-odds):
![](https://i.imgur.com/rJAodaD.png)
The logit function takes input values in the range 0 to 1 and transforms them into values over the entire real-number range.

Under the logistic model, we assume that there is a linear relationship between the weighted inputs (referred to as net inputs in Chapter 2) and the log-odds:
![](https://i.imgur.com/2zav2c1.png)
what we are actually interested in is the probability p, the class-member-
ship probability of an example given its features. While the logit function maps the probability to a real-number range, we can consider the inverse of this function to map the real-number range back to a [0, 1] range for the probability p.

This inverse of the logit function is typically called the <mark>logistic sigmoid function</mark>, which is sometimes simply abbreviated to **sigmoid function** due to its characteristic **S-shape**:
![](https://i.imgur.com/SEWkb94.png)
Here, z is the net input, the linear combination of weights, and the inputs
![](https://i.imgur.com/u9azDjh.png)

To build some understanding of the logistic regression model, we can relate it to Chapter 2. In Adaline, we used the identity function, ùúé(ùëß) = ùëß, as the activation function. In logistic regression, this activation function simply becomes the sigmoid function that we define earlier.
![](https://i.imgur.com/QlIuSIO.png)
The output of the sigmoid function is then interpreted as the probability of a particular ex-ample belonging to class 1, ùúé(ùëß) = ùëù(ùë¶=1|ùíô;w,b) , given its features, x, and parameterized by w and b.
The predicted probability can then simply be converted into a binary outcome via a threshold function:
![](https://i.imgur.com/h5x1lsN.png)
If we look at the preceding plot of the sigmoid function, this is equivalent to the following:
![](https://i.imgur.com/pc5Ktto.png)
In fact, there are many applications where we are not only interested in the predicted class labels, but where the estimation of the class-membership probability is particularly useful (the output of the sigmoid function prior to applying the threshold function).



# Learning the model weights via the logistic loss function

In the previous chapter, we defined the mean squared error loss function as follows:
![](https://i.imgur.com/KPNTiGY.png)
We minimized this function in order to learn the parameters for our Adaline classification model.
To explain how we can derive the loss function for logistic regression, let‚Äôs first define the likelihood, ‚Ñí , that we want to maximize when we build a logistic regression model, assuming that the individual examples in our dataset are independent of one another. The formula is as follows:
![](https://i.imgur.com/z1FybQ0.png)
log-likelihood:
![](https://i.imgur.com/IBVZcgj.png)

![](https://i.imgur.com/VmrmIto.png)
Now, we could use an optimization algorithm such as gradient ascent to maximize this log-likelihood function.
let‚Äôs rewrite the log-likelihood as a loss function, L, that can be minimized using gradient descent as in Chapter 2:
![](https://i.imgur.com/drPfhFG.png)
![](https://i.imgur.com/9cV9l7X.png)
![](https://i.imgur.com/I0AdPV1.png)
the loss of classifying a single training example for different values of ùúé(ùëß): 
![](https://i.imgur.com/wAL07dH.png)
We can see that the loss approaches 0 (continuous line) if we correctly predict that an example belongs to class 1. Similarly, we can see on the y axis that the loss also approaches 0 if we correctly predict y = 0 (dashed line). However, if the prediction is wrong, the loss goes toward infinity. The main point is that we penalize wrong predictions with an increasingly larger loss.

# Converting an Adaline implementation into an algorithm for logistic regression
If we were to implement logistic regression ourselves, we could simply substitute the loss function, L, in our Adaline implementation from Chapter 2, with the new loss function:
![](https://i.imgur.com/4wfQpxr.png)
We use this to compute the loss of classifying all training examples per epoch. Also, we need to swap the linear activation function with the sigmoid. If we make those changes to the Adaline code, we will end up with a working logistic regression implementation. The following is an implementation for full-batch gradient descent
```
class LogisticRegressionGD:
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
    def fit(self, X, y):
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])
        self.b_ = np.float64(0.)
        self.losses_ = []

        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_ += self.eta*2.0*X.T.dot(errors)/X.shape[0]
            self.b_ += self.eta*2.0*errors.mean()
            loss = (-y.dot(np.log(output)) - (1 - y).dot(np.log(1 - output))) / X.shape[0]
            self.losses_.append(loss)
        return self
    def net_input(self,X):
        return np.dot(X, self.w_)+self.b_
    def activation(self, z):
        return 1./(1.+np.exp(-np.clip(z, -250, 250)))
    def predict(self, X):
        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
```

apply and plot the algorithm
```
>>> plot_decision_regions(X=X_train_01_subset,
...
y=y_train_01_subset,
...
classifier=lrgd)
>>> plt.xlabel('Petal length [standardized]')
>>> plt.ylabel('Petal width [standardized]')
>>> plt.legend(loc='upper left')
>>> plt.tight_layout()
>>> plt.show()
```
![](https://i.imgur.com/A7i1NJm.png)
![](https://i.imgur.com/xj7y40H.png)


# Training a logistic regression model with scikit-learn
Note that in recent versions of scikit-learn, the technique used for multiclass
classification, multinomial, or OvR, is chosen automatically.
multi_class='multinomial'. Note that the multinomial setting is now the default choice in scikit learn‚Äôs LogisticRegression class and recommended in practice for mutually exclusive classes, such as those found in the Iris dataset. Here, ‚Äúmutually exclusive‚Äù means that each training example can only belong to a single class (in contrast to multilabel classification, where a training example can be
a member of multiple classes).

```
lr = LogisticRegression(C=100.0, solver='lbfgs', multi_class='ovr')
lr.fit(X_train_std, y_train)
plot_decision_regions(X_train_std,y_train,classifier=lr,test_idx=range(105, 150))
plt.xlabel('Petal length [standardized]')
plt.ylabel('Petal width [standardized]')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()
```
![](https://i.imgur.com/yD7lcWG.png)
The probability that training examples belong to a certain class can be computed using the predict_ proba method.
```
lr.predict_proba(X_test_std[:3, :])
```
we can get the predicted class labels by identifying the largest column in each row, for example, using NumPy‚Äôs argmax function:
```
lr.predict_proba(X_test_std[:3, :]).argmax(axis=1)
```
In practice, the more convenient way of obtaining class labels when using scikit-learn is to call the predict method directly:
```
lr.predict(X_test_std[:3, :])
```
# Tackling overfitting via regularization
If a model suffers from overfitting, we also say that the model has a high variance, which can be caused by having too many parameters, leading to a model that is too complex given the underlying data.
. Similarly, our model can also suffer fromunderfitting (high bias), which means that our model is not complex enough to capture the pattern in the training data well and therefore also suffers from low performance on unseen data.

One way of finding a good bias-variance tradeoff is to tune the complexity of the model via regularization. Regularization is a very useful method for handling collinearity (high correlation among features), filtering out noise from data, and eventually preventing overfitting.

The concept behind regularization is to introduce additional information to penalize extreme parameter(weight) values.
###### L2 regularization
![](https://i.imgur.com/3sd7TKe.png)
ùúÜ is the so-called regularization parameter.2 in the denominator is merely a
scaling factor, such that it cancels when computing the loss gradient. The sample size n is added to scale the regularization term similar to the loss.

![](https://i.imgur.com/o4acRdG.png)
The loss function for logistic regression can be regularized by adding a simple regularization term, which will shrink the weights during model training:
![](https://i.imgur.com/HreCWMH.png)
Adding the regularization term to the loss changes the partial derivative to the following form:
![](https://i.imgur.com/JxIGJJd.png)

Via the regularization parameter, ùúÜùúÜ, we can then control how closely we fit the training data, while keeping the weights small. By increasing the value of ùúÜùúÜ, we increase the regularization strength.
note that the bias unit is usually not regularized.

The term C is inversely proportional to the regularization parameter, ùúÜùúÜ. Consequently, decreasing the value of the inverse regularization parameter, C, means that we are increasing the regularization strength, which we can visualize by plotting the L2 regularization path for the two weight coefficients:
![](https://i.imgur.com/JTKt5We.png)

Increasing the regularization strength can reduce overfitting, so we might ask why we don‚Äôt strongly regularize all models by default. The reason is that we have to be careful when adjusting the regularization strength. For instance, if the regularization strength is too high and the weights coefficients approach zero, the model can perform very poorly due to underfitting

# Other advanced resources 
Logistic Regression: From Introductory to Advanced Concepts and Applications, Dr.Scott Menard, Sage Publications, 2009
![](https://i.imgur.com/8N9LLpm.png)










