	# Logistic regression and conditional probabilities
performs very well on linearly separable classes.
**odds**: the odds in favor of a particular event. The odds can be written as
$\frac{ğ‘}{1âˆ’ğ‘}$ , where p stands for the probability of the positive event.The term â€œpositive eventâ€ does not necessarily mean â€œgood,â€ but refers to the event that we want to predict.
We can then further define the logit function, which is simply the logarithm of the odds (log-odds):
![](https://i.imgur.com/rJAodaD.png)
The logit function takes input values in the range 0 to 1 and transforms them into values over the entire real-number range.

Under the logistic model, we assume that there is a linear relationship between the weighted inputs (referred to as net inputs in Chapter 2) and the log-odds:
![](https://i.imgur.com/2zav2c1.png)
what we are actually interested in is the probability p, the class-member-
ship probability of an example given its features. While the logit function maps the probability to a real-number range, we can consider the inverse of this function to map the real-number range back to a [0, 1] range for the probability p.

This inverse of the logit function is typically called the <mark>logistic sigmoid function</mark>, which is sometimes simply abbreviated to **sigmoid function** due to its characteristic **S-shape**:
![](https://i.imgur.com/SEWkb94.png)
Here, z is the net input, the linear combination of weights, and the inputs
![](https://i.imgur.com/u9azDjh.png)

To build some understanding of the logistic regression model, we can relate it to Chapter 2. In Adaline, we used the identity function, ğœ(ğ‘§) = ğ‘§, as the activation function. In logistic regression, this activation function simply becomes the sigmoid function that we define earlier.
![](https://i.imgur.com/QlIuSIO.png)
The output of the sigmoid function is then interpreted as the probability of a particular ex-ample belonging to class 1, ğœ(ğ‘§) = ğ‘(ğ‘¦=1|ğ’™;w,b) , given its features, x, and parameterized by w and b.
The predicted probability can then simply be converted into a binary outcome via a threshold function:
![](https://i.imgur.com/h5x1lsN.png)
If we look at the preceding plot of the sigmoid function, this is equivalent to the following:
![](https://i.imgur.com/pc5Ktto.png)
In fact, there are many applications where we are not only interested in the predicted class labels, but where the estimation of the class-membership probability is particularly useful (the output of the sigmoid function prior to applying the threshold function).



# Learning the model weights via the logistic loss function

In the previous chapter, we defined the mean squared error loss function as follows:
![](https://i.imgur.com/KPNTiGY.png)
We minimized this function in order to learn the parameters for our Adaline classification model.
To explain how we can derive the loss function for logistic regression, letâ€™s first define the likelihood, â„’ , that we want to maximize when we build a logistic regression model, assuming that the individual examples in our dataset are independent of one another. The formula is as follows:
![](https://i.imgur.com/z1FybQ0.png)
log-likelihood:
![](https://i.imgur.com/IBVZcgj.png)

![](https://i.imgur.com/VmrmIto.png)
Now, we could use an optimization algorithm such as gradient ascent to maximize this log-likelihood function.
letâ€™s rewrite the log-likelihood as a loss function, L, that can be minimized using gradient descent as in Chapter 2:
![](https://i.imgur.com/drPfhFG.png)
![](https://i.imgur.com/9cV9l7X.png)
![](https://i.imgur.com/I0AdPV1.png)
the loss of classifying a single training example for different values of ğœ(ğ‘§): 
![](https://i.imgur.com/wAL07dH.png)
We can see that the loss approaches 0 (continuous line) if we correctly predict that an example belongs to class 1. Similarly, we can see on the y axis that the loss also approaches 0 if we correctly predict y = 0 (dashed line). However, if the prediction is wrong, the loss goes toward infinity. The main point is that we penalize wrong predictions with an increasingly larger loss.

# Converting an Adaline implementation into an algorithm for logistic regression
If we were to implement logistic regression ourselves, we could simply substitute the loss function, L, in our Adaline implementation from Chapter 2, with the new loss function:
![](https://i.imgur.com/4wfQpxr.png)
We use this to compute the loss of classifying all training examples per epoch. Also, we need to swap the linear activation function with the sigmoid. If we make those changes to the Adaline code, we will end up with a working logistic regression implementation. The following is an implementation for full-batch gradient descent
```
class LogisticRegressionGD:
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
    def fit(self, X, y):
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])
        self.b_ = np.float64(0.)
        self.losses_ = []

        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_ += self.eta*2.0*X.T.dot(errors)/X.shape[0]
            self.b_ += self.eta*2.0*errors.mean()
            loss = (-y.dot(np.log(output)) - (1 - y).dot(np.log(1 - output))) / X.shape[0]
            self.losses_.append(loss)
        return self
    def net_input(self,X):
        return np.dot(X, self.w_)+self.b_
    def activation(self, z):
        return 1./(1.+np.exp(-np.clip(z, -250, 250)))
    def predict(self, X):
        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
```

apply and plot the algorithm
```
>>> plot_decision_regions(X=X_train_01_subset,
...
y=y_train_01_subset,
...
classifier=lrgd)
>>> plt.xlabel('Petal length [standardized]')
>>> plt.ylabel('Petal width [standardized]')
>>> plt.legend(loc='upper left')
>>> plt.tight_layout()
>>> plt.show()
```
![](https://i.imgur.com/A7i1NJm.png)
![](https://i.imgur.com/xj7y40H.png)
