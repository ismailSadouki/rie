# Logistic regression and conditional probabilities
performs very well on linearly separable classes.
**odds**: the odds in favor of a particular event. The odds can be written as
$\frac{𝑝}{1−𝑝}$ , where p stands for the probability of the positive event.The term “positive event” does not necessarily mean “good,” but refers to the event that we want to predict.
We can then further define the logit function, which is simply the logarithm of the odds (log-odds):
![](https://i.imgur.com/rJAodaD.png)
The logit function takes input values in the range 0 to 1 and transforms them into values over the entire real-number range.

Under the logistic model, we assume that there is a linear relationship between the weighted inputs (referred to as net inputs in Chapter 2) and the log-odds:
![](https://i.imgur.com/2zav2c1.png)
what we are actually interested in is the probability p, the class-member-
ship probability of an example given its features. While the logit function maps the probability to a real-number range, we can consider the inverse of this function to map the real-number range back to a [0, 1] range for the probability p.

This inverse of the logit function is typically called the <mark>logistic sigmoid function</mark>, which is sometimes simply abbreviated to **sigmoid function** due to its characteristic **S-shape**:
![](https://i.imgur.com/SEWkb94.png)
Here, z is the net input, the linear combination of weights, and the inputs
![](https://i.imgur.com/u9azDjh.png)

To build some understanding of the logistic regression model, we can relate it to Chapter 2. In Adaline, we used the identity function, 𝜎(𝑧) = 𝑧, as the activation function. In logistic regression, this activation function simply becomes the sigmoid function that we define earlier.
![](https://i.imgur.com/QlIuSIO.png)
The output of the sigmoid function is then interpreted as the probability of a particular ex-ample belonging to class 1, 𝜎(𝑧) = 𝑝(𝑦=1|𝒙;w,b) , given its features, x, and parameterized by w and b.
The predicted probability can then simply be converted into a binary outcome via a threshold function:
![](https://i.imgur.com/h5x1lsN.png)
If we look at the preceding plot of the sigmoid function, this is equivalent to the following:
![](https://i.imgur.com/pc5Ktto.png)
In fact, there are many applications where we are not only interested in the predicted class labels, but where the estimation of the class-membership probability is particularly useful (the output of the sigmoid function prior to applying the threshold function).



# Learning the model weights via the logistic loss function
