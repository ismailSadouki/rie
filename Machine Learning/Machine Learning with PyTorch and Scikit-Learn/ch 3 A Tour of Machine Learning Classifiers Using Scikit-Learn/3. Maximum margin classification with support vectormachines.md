sing the perceptron algorithm, we minimized misclassification errors. However, in SVMs, our optimization objective is to maximize the margin.
The margin is defined as the distance between the separating hyperplane (decision boundary) and the training examples that are closest to this hyperplane, which are the so-called support vectors.
![](https://i.imgur.com/zOtTM3X.png)
# Maximum margin intuition
The rationale behind having decision boundaries with large margins is that they tend to have a lower generalization error, whereas models with small margins are more prone to overfitting.
he mathematics behind SVM is quite advanced and would require sound knowledge of constrained optimization.
we recommend the following resources if you are interested in learning more:
•Chris J.C. Burges’ excellent explanation in A Tutorial on Support Vector Machines for Pattern Recognition (Data Mining and Knowledge Discovery, 2(2): 121-167, 1998)
•Vladimir Vapnik’s book The Nature of Statistical Learning Theory, Springer Science+Business Media, Vladimir Vapnik, 2000
•Andrew Ng’s very detailed lecture notes available at https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf

# Dealing with a nonlinearly separable case using slack variables
let’s briefly mention the so-called slack variable. The motivation
for introducing the slack variable was that the linear constraints in the SVM optimization objective need to be relaxed for nonlinearly separable data to allow the convergence of the optimization in the presence of misclassifications, under appropriate loss penalization.
The use of the slack variable, in turn, introduces the variable, which is commonly referred to as C in SVM contexts. We can consider C as a hyperparameter for controlling the penalty for misclassification.
Large values of C correspond to large error penalties, whereas we are less strict about misclassification errors if we choose smaller values for C. We can then use the C parameter to control the width of the margin and therefore tune the bias-variance tradeoff, as illustrated in Figure 3.11:
![](https://i.imgur.com/SgBqxWF.png)


```
from sklearn.svm import SVC
svm = SVC(kernel='linear', C=1.0, random_state=1)
svm.fit(X_train_std, y_train)
```
![](https://i.imgur.com/um4WPIN.png)
![](https://i.imgur.com/T8IePhi.png)

# Alternative implementations in scikit-learn
The scikit-learn library’s LogisticRegression class, which we used in the previous sections, can make use of the LIBLINEAR library by setting solver='liblinear'. LIBLINEAR is a highly optimized C/C++ library developed at the National Taiwan University (http://www.csie.ntu.edu.tw/~cjlin/liblinear/).
Similarly, the SVC class that we used to train an SVM makes use of LIBSVM, which is an equivalent C/C++ library specialized for SVMs (http://www.csie.ntu.edu.tw/~cjlin/libsvm/).

The advantage of using LIBLINEAR and LIBSVM over, for example, native Python implementations is that they allow the extremely quick training of large amounts of linear classifiers. However, sometimes our datasets are too large to fit into computer memory. Thus, scikit-learn also offers alternative implementations via the SGDClassifier class, which also supports online learning via the partial_fit
method. The concept behind the SGDClassifier class is similar to the stochastic gradient algorithm that we implemented in Chapter 2 for Adaline.

We could initialize the SGD version of the perceptron (loss='perceptron'), logistic regression (loss='log'), and an SVM with default parameters (loss='hinge'), as follows:
```
from sklearn.linear_model import SGDClassifier
ppn = SGDClassifier(loss='perceptron')
lr = SGDClassifier(loss='log')
svm = SGDClassifier(loss='hinge')
```