Another reason why SVMs enjoy high popularity among machine learning practitioners is that they can be easily kernelized to solve nonlinear classification problems.
# Kernel methods for linearly inseparable data
Using the following code, we will create a simple dataset that has the form of an XOR gate using the logical_or function from NumPy, where 100 examples will be assigned the class label 1, and 100 examples will be assigned the class label -1:
```
np.random.seed(1)
X_xor = np.random.randn(200,2)
y_xor = np.logical_xor(X_xor[:,0]>0,X_xor[:,1]>0)
y_xor = np.where(y_xor, 1, 0)
```
an XOR dataset with random noise, as shown in Figure 3.13:
![](https://i.imgur.com/bvcKGmJ.png)
Obviously, we would not be able to separate the examples from the positive and negative class very well using a linear hyperplane as a decision boundary via the linear logistic regression or linear SVM model that we discussed in earlier sections.
The basic idea behind kernel methods for dealing with such linearly inseparable data is to create nonlinear combinations of the original features to project them onto a higher-dimensional space via a mapping function, ğœ™ , where the data becomes linearly separable. As shown in Figure 3.14, we can transform a two-dimensional dataset into a new three-dimensional feature space, where the classes become separable via the following projection:
![](https://i.imgur.com/4Fmr6ld.png)
This allows us to separate the two classes shown in the plot via a linear hyperplane that becomes a nonlinear decision boundary if we project it back onto the original feature space, as illustrated with the following concentric circle dataset:
![](https://i.imgur.com/MX2bhaj.png)
# Using the kernel trick to find separating hyperplanes in a high-dimensional space
To solve a nonlinear problem using an SVM, we would transform the training data into a higher-dimensional feature space via a mapping function, ğœ™ , and train a linear SVM model to classify the data in this new feature space. Then, we could use the same function to transform new, unseen data to classify it using the lnear SVM model.

However, one problem with this mapping approach is that the construction of the new features is computationally very expensive, especially if we are dealing with high-dimensional data. This is where the so-called kernel trick comes into play.
![](https://i.imgur.com/LFoMVii.png)
One of the most widely used kernels is the **radial basis function (RBF) kernel**, which can simply be called the **Gaussian kernel**:
![](https://i.imgur.com/2gkEYkw.png)
![](https://i.imgur.com/dKeTQz6.png)
Roughly speaking, the term â€œkernelâ€ can be interpreted as a similarity function between a pair of examples. The minus sign inverts the distance measure into a similarity score, and, due to the exponential term, the resulting similarity score will fall into a range between 1 (for exactly similar examples) and 0 (for very dissimilar examples). 
Now that we have covered the big picture behind the kernel trick, letâ€™s see if we can train a kernel SVM that is able to draw a nonlinear decision boundary that separates the XOR data well. Here, we simply use the SVC class from scikit-learn that we imported earlier and replace the kernel='linear' parameter with kernel='rbf':
```
svm = SVC(kernel='rbf', random_state=1, gamma=0.10, C=10.0)
svm.fit(X_xor, y_xor)
plot_decision_regions(X_xor, y_xor, classifier=svm)
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()
```
![](https://i.imgur.com/ou0UEVg.png)
The ğ›¾ parameter, which we set to gamma=0.1, can be understood as a cut-off parameter for the Gaussian sphere. If we increase the value for ğ›¾, we increase the influence or reach of the training examples, which leads to a tighter and bumpier decision boundary. 
To get a better understanding of gamma, lets apply smv on iris data:
withe <mark>gamma=0.2</mark>
![](https://i.imgur.com/fSOcMz5.png)
and <mark>gamma=100.0</mark>
![](https://i.imgur.com/Bx7yl79.png)
Although the model fits the training dataset very well, such a classifier will likely have a high generalization error on unseen data. This illustrates that the ğ›¾ğ›¾ parameter also plays an important role in controlling overfitting or variance when the algorithm is too sensitive to fluctuations in the training dataset.

