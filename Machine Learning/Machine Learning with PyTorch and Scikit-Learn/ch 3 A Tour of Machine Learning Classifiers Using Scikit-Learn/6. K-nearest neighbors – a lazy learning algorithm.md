It is called “lazy” not because of its apparent simplicity, but
because it doesn’t learn a discriminative function from the training data but memorizes the training dataset instead.
![](https://i.imgur.com/WBCHoi0.png)
The KNN algorithm itself is fairly straightforward and can be summarized by the following steps:
1.Choose the number of k and a distance metric
2.Find the k-nearest neighbors of the data record that we want to classify
3.Assign the class label by majority vote
![](https://i.imgur.com/6k5SdCp.png)
Based on the chosen distance metric, the KNN algorithm finds the k examples in the training dataset that are closest (most similar) to the point that we want to classify. The class label of the data point is then determined by a majority vote among its k nearest neighbors.
![](https://i.imgur.com/CUDb50u.png)
implement a KNN model in scikit-learn using a Euclidean distance metric:
```
knn = KNeighborsClassifier(n_neighbors=5, p=2,metric='minkowski')
```
![](https://i.imgur.com/Z2RG6cr.png)
![](https://i.imgur.com/Zwe63zu.png)
The right choice of k is crucial to finding a good balance between overfitting and underfitting. We also have to make sure that we choose a distance metric that is appropriate for the features in the dataset. Often, a simple Euclidean distance measure is used for real-value examples.
if we are using a Euclidean distance measure, it is also important to standardize the data so that each feature contributes equally to the distance. The minkowski distance that we used in the previous code is just a generalization of the Euclidean and Manhattan distance, which can be written as follows:
![](https://i.imgur.com/AfqWEnT.png)
It becomes the Euclidean distance if we set the parameter p=2 or the Manhattan distance at p=1.
it is important to mention that KNN is very susceptible to overfitting due to the curse of dimensionality.

<mark>in models where regularization is not applicable, such as decision trees and KNN, we can use feature selection and dimensionality reduction techniques to help us to avoid the curse of dimensionality.</mark>
![](https://i.imgur.com/XNGuFcr.png)
