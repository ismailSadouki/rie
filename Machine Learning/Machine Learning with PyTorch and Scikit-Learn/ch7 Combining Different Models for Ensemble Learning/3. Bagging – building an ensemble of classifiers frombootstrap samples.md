instead of using the same training dataset to fit the individual classifiers in the ensemble, we draw bootstrap samples (random samples with replacement) from the initial training dataset, which is why bagging is also known as bootstrap aggregating.

![](https://i.imgur.com/iZgmovu.png)
# Bagging in a nutshell
how the bootstrap aggregating of a bagging classifier works,  in figure 7.7. Here, we have seven different training instances (denoted as indices 1-7) that are sampled randomly with replacement in each round of bagging. Each bootstrap sample is then used to fit a classifier, Cj, which is most typically an unpruned decision tree:
![](https://i.imgur.com/iU1Mv0Y.png)
Once the individual classifiers are fit to the bootstrap samples, the predictions are combined using majority voting.

random forests are a special case of bagging where we also use random feature subsets when fitting the individual decision trees.
![](https://i.imgur.com/6U3Rc39.png)

# Applying bagging to classify examples in the Wine dataset
Here, we will only consider the Wine classes 2 and 3, and we will select two features – Alcohol and OD280/OD315 of diluted wines:
```
import pandas as pd
df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/''machine-learning-databases/''wine/wine.data',header=None)
df_wine.columns = ['Class label', 'Alcohol','Malic acid', 'Ash','Alcalinity of ash','Magnesium', 'Total phenols','Flavanoids', 'Nonflavanoid phenols','Proanthocyanins','Color intensity', 'Hue','OD280/OD315 of diluted wines','Proline']
df_wine = df_wine[df_wine['Class label'] != 1]
y = df_wine['Class label'].values
X = df_wine[['Alcohol','OD280/OD315 of diluted wines']].values

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
le = LabelEncoder()
y = le.fit_transform(y)
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=1,stratify=y)

```
we will use an unpruned decision tree as the base classifier and create an
ensemble of 500 decision trees fit on different bootstrap samples of the training dataset:
```
from sklearn.ensemble import BaggingClassifier
tree = DecisionTreeClassifier(criterion="entropy", random_state=1, max_depth=None)
bag = BaggingClassifier(estimator=tree, n_estimators=500, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, n_jobs=-1, random_state=1)

```
we will calculate the accuracy score of the prediction on the training and test datasets to compare the performance of the bagging classifier to the performance of a single unpruned decision tree:
```
from sklearn.metrics import accuracy_score
tree = tree.fit(X_train, y_train)
y_train_pred = tree.predict(X_train)
y_test_pred = tree.predict(X_test)
tree_train = accuracy_score(y_train, y_train_pred)
tree_test = accuracy_score(y_test, y_test_pred)
print(f'Decision tree train/test accuracies 'f'{tree_train:.3f}/{tree_test:.3f}')

#OUTPUT
Decision tree train/test accuracies 1.000/0.833
```
the substantially lower test accuracy indicates high variance (overfitting) of the model.
```
bag = bag.fit(X_train, y_train)
y_train_pred = bag.predict(X_train)
y_test_pred = bag.predict(X_test)
bag_train = accuracy_score(y_train, y_train_pred)
bag_test = accuracy_score(y_test, y_test_pred)
print(f'Bagging train/test accuracies 'f'{bag_train:.3f}/{bag_test:.3f}')

# OUTPUT
Bagging train/test accuracies 1.000/0.917
```
we can see that the bagging classifier has a slightly better generalization
performance, as estimated on the test dataset.
let’s compare the decision regions between the decision tree and the bagging classifier:
![](https://i.imgur.com/1w2NVUd.png)
the three-node deep decision tree looks smoother in the bagging ensemble.

In practice, more complex classification tasks and a dataset’s high dimensionality can easily lead to overfitting in single decision trees, and this is where the bagging algorithm can really play to its strengths. Finally, we must note that the bagging algorithm can be an effective approach to reducing the variance of a model. However, bagging is ineffective in reducing model bias, that is, models that are too simple to capture the trends in the data well. This is why we want to perform bagging on an ensemble of classifiers with low bias, for example, unpruned decision trees.