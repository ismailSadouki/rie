# Comparing AdaBoost with gradient boosting
AdaBoost trains decision tree stumps based on errors of the previous decision tree stump. In particular, the errors are used to compute sample weights in each round as well as for computing a classifier weight for each decision tree stump when combining the individual stumps into an ensemble. We stop training once a maximum number of iterations (decision tree stumps) is reached.

Like AdaBoost, gradient boosting fits decision trees in an iterative fashion using prediction errors. However, gradient boosting trees are usually deeper than decision tree stumps and have typically a maximum depth of 3 to 6 (or a maximum number of 8 to 64 leaf nodes). Also, in contrast to AdaBoost, gradient boosting does not use the prediction errors for assigning sample weights; they are used directly to form the target variable for fitting the next tree. Moreover, instead of having an individual weighting term for each tree, like in AdaBoost, gradient boosting uses a global learning rate that is the same for each tree.

# Outlining the general gradient boosting algorithm
<mark> https://www.youtube.com/watch?v=zblsrxc7XpM</mark>
In essence, gradient boosting builds a series of trees, where each tree is fit on the error—the difference between the label and the predicted value—of the previous tree. In each round, the tree ensemble improves as we are nudging each tree more in the right direction via small updates. These updates are based on a loss gradient, which is how gradient boosting got its name.

The following steps will introduce the general algorithm behind gradient boosting.<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250617092524.png]]
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250617092738.png]]

# Explaining the gradient boosting algorithm for classification
In this subsection, we will go over the details for implementing the gradient boosting algorithm for binary classification. In this context, we will be using the logistic loss function that we introduced for logistic regression in Chapter 3

we can specify the logistic loss as follows:
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250617110155.png]]
we will use these log(odds) to rewrite the logistic function as
follows (omitting intermediate steps here):
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250617110332.png]]
we can define the partial derivative of the loss function with respect to these log(odds), $\hat{y}$. The derivative of this loss function with respect to the log(odds) is:
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250617110607.png]]
Now let us now revisit the general gradient boosting steps
1 to 2d from the previous section and reformulate them for this binary classification scenario.
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250617110742.png]]
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250617111114.png]]