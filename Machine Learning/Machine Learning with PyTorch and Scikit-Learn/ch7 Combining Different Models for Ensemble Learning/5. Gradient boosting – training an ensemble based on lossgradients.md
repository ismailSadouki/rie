# Comparing AdaBoost with gradient boosting
AdaBoost trains decision tree stumps based on errors of the previous decision tree stump. In particular, the errors are used to compute sample weights in each round as well as for computing a classifier weight for each decision tree stump when combining the individual stumps into an ensemble. We stop training once a maximum number of iterations (decision tree stumps) is reached.

Like AdaBoost, gradient boosting fits decision trees in an iterative fashion using prediction errors. However, gradient boosting trees are usually deeper than decision tree stumps and have typically a maximum depth of 3 to 6 (or a maximum number of 8 to 64 leaf nodes). Also, in contrast to AdaBoost, gradient boosting does not use the prediction errors for assigning sample weights; they are used directly to form the target variable for fitting the next tree. Moreover, instead of having an individual weighting term for each tree, like in AdaBoost, gradient boosting uses a global learning rate that is the same for each tree.

# Outlining the general gradient boosting algorithm
<mark> https://www.youtube.com/watch?v=zblsrxc7XpM</mark>
In essence, gradient boosting builds a series of trees, where each tree is fit on the error‚Äîthe difference between the label and the predicted value‚Äîof the previous tree. In each round, the tree ensemble improves as we are nudging each tree more in the right direction via small updates. These updates are based on a loss gradient, which is how gradient boosting got its name.

The following steps will introduce the general algorithm behind gradient boosting.<!--‚ö†Ô∏èImgur upload failed, check dev console-->
![[Pasted image 20250617092524.png]]
<!--‚ö†Ô∏èImgur upload failed, check dev console-->
![[Pasted image 20250617092738.png]]

# Explaining the gradient boosting algorithm for classification
In this subsection, we will go over the details for implementing the gradient boosting algorithm for binary classification. In this context, we will be using the logistic loss function that we introduced for logistic regression in Chapter 3

we can specify the logistic loss as follows:
<!--‚ö†Ô∏èImgur upload failed, check dev console-->
![[Pasted image 20250617110155.png]]
we will use these log(odds) to rewrite the logistic function as
follows (omitting intermediate steps here):
<!--‚ö†Ô∏èImgur upload failed, check dev console-->
![[Pasted image 20250617110332.png]]
we can define the partial derivative of the loss function with respect to these log(odds), $\hat{y}$. The derivative of this loss function with respect to the log(odds) is:
<!--‚ö†Ô∏èImgur upload failed, check dev console-->
![[Pasted image 20250617110607.png]]
Now let us now revisit the general gradient boosting steps
1 to 2d from the previous section and reformulate them for this binary classification scenario.
<!--‚ö†Ô∏èImgur upload failed, check dev console-->
![[Pasted image 20250617110742.png]]
<!--‚ö†Ô∏èImgur upload failed, check dev console-->
![[Pasted image 20250617111114.png]]


# Illustrating gradient boosting for classification
<!--‚ö†Ô∏èImgur upload failed, check dev console-->
![[Pasted image 20250617151426.png]]
the odds can be computed as the number of successes divided
by the number of failures. Here, we regard label 1 as success and label 0 as failure, so the odds are computed as: odds = 2/1.
<!--‚ö†Ô∏èImgur upload failed, check dev console-->
![[Pasted image 20250617151452.png]]
<!--‚ö†Ô∏èImgur upload failed, check dev console-->
![[Pasted image 20250617151525.png]]
(Note that we artificially limit the tree to have only two leaf nodes, which helps illustrate what happens if a leaf node contains more than one example.) 

Then, in the final step 2d, we update the previous model and the current model. Assuming a learning rate of ùúÇùúÇ ùúÇ ùúÇùúÇùúÇ, the resulting prediction for the first training example is shown in Figure 7.15:
<!--‚ö†Ô∏èImgur upload failed, check dev console-->
![[Pasted image 20250617151708.png]]
<!--‚ö†Ô∏èImgur upload failed, check dev console-->
![[Pasted image 20250617151726.png]]
We can already see that the predicted probabilities are higher for the positive class and lower for the
negative class. Consequently, the residuals are getting smaller, too. Note that the process of steps 2a
to 2d is repeated until we have fit M trees or the residuals are smaller than a user-specified threshold
value. Then, once the gradient boosting algorithm has completed, we can use it to predict the class
labels by thresholding the probability values of the final model, FM(x) at 0.5, like logistic regression in
Chapter 3. However, in contrast to logistic regression, gradient boosting consists of multiple trees and
produces nonlinear decision boundaries. In the next section, we will look at how gradient boosting
looks in action.


# Using XGBoost
It is important to note that gradient boosting is a sequential process that can be slow to train. However, in recent years a more popular implementation of gradient boosting has emerged, namely, XGBoost.

XGBoost proposed several tricks and approximations that speed up the training process substantially. Hence, the name XGBoost, which stands for extreme gradient boosting. Moreover, these approximations and tricks result in very good predictive performances. In fact, XGBoost gained popularity as it has been the winning solution for many Kaggle competitions.

Next to XGBoost, there are also other popular implementations of gradient boosting, for example, LightGBM and CatBoost. Inspired by LightGBM, scikit-learn now also implements a HistGradientBoostingClassifier, which is more performant than the original gradient boosting classifier (GradientBoostingClassifier).

You can find more details about these methods via the resources below:
‚Ä¢XGBoost: https://xgboost.readthedocs.io/en/stable/
‚Ä¢LightGBM: https://lightgbm.readthedocs.io/en/latest/
‚Ä¢CatBoost: https://catboost.ai
‚Ä¢HistGradientBoostingClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html

if we fit the gradient boosting classifier with 1,000 trees (rounds) and a learning rate of 0.01. Typically, a learning rate between 0.01 and 0.1 is recommended. However, remember that the learning rate is used for scaling the predictions from the individual rounds. So, intuitively, the lower the learning rate, the more estimators are required to achieve accurate predictions.
Next, we have the max_depth for the individual decision trees, which we set to 4. Since we are still boosting weak learners, a value between 2 and 6 is reasonable, but larger values may also work well depending on the dataset.

Finally, use_label_encoder=False disables a warning message which informs users that XGBoost is not converting labels by default anymore, and it expects users to provide labels in an integer format starting with label 0. (There is nothing to worry about here, since we have been following this format throughout this book.) There are many more settings available, and a detailed discussion is out of the scope of this book. However, interested readers can find more details in the original documentation at https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier.
