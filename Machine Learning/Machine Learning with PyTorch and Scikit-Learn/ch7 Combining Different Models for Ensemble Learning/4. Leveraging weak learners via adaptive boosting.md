In boosting, the ensemble consists of very simple base classifiers, also often referred to as **weak learners**, which often only have a slight performance advantage over random guessing.

The key concept behind boosting is to focus on training examples that are hard to classify, that is, to let the weak learners subsequently learn from misclassified training examples to improve the performance of the ensemble.




# How adaptive boosting works
the initial formulation of the boosting algorithm uses random subsets of train-
ing examples drawn from the training dataset without replacement; the original boosting procedure can be summarized in the following four key steps:
1.Draw a random subset (sample) of training examples, d1, without replacement from the training dataset, D, to train a weak learner, C1.
2.Draw a second random training subset, d2, without replacement from the training dataset and add 50 percent of the examples that were previously misclassified to train a weak learner, C2.
3.Find the training examples, d3, in the training dataset, D, which C1 and C2 disagree upon, to train a third weak learner, C3.
4.Combine the weak learners C1, C2, and C3 via majority voting.

boosting can lead to a decrease in bias as well as variance compared to bagging models.
In practice, however, boosting algorithms such as AdaBoost are also known for their high variance, that is, the tendency to overfit the training data.

AdaBoost uses the complete training dataset to train the weak learners, where the training examples are reweighted in each iteration to build a strong classifier that learns from the mistakes of the previous weak learners in the ensemble.

<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250617084719.png]]

subfigure 1, which represents a training dataset for binary classification where all training examples are assigned equal weights. Based on this training dataset, we train a decision stump (shown as a dashed line) that tries to classify the examples of the two classes (triangles and circles), as well as possibly minimizing the loss function (or the impurity score in the special case of decision tree ensembles).

For the next round (subfigure 2), we assign a larger weight to the two previously misclassified examples (circles). Furthermore, we lower the weight of the correctly classified examples. The next decision stump will now be more focused on the training examples that have the largest weights—the training examples that are supposedly hard to classify.
The weak learner shown in subfigure 2 misclassifies three different examples from the circle class, which are then assigned a larger weight, as shown in subfigure 3.
Assuming that our AdaBoost ensemble only consists of three rounds of boosting, we then combine the three weak learners trained on different reweighted training subsets by a weighted majority vote, as shown in subfigure 4.

et’s take a more detailed look at the algorithm using pseudo code.
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250617084954.png]]
<mark>Revisit code</mark>

# Applying AdaBoost using scikit-learn
```
from sklearn.ensemble import AdaBoostClassifier
tree = DecisionTreeClassifier(criterion='entropy', random_state=1, max_depth=1)
ada = AdaBoostClassifier(estimator=tree, n_estimators=500, learning_rate=0.1, random_state=1)

tree = tree.fit(X_train, y_train)
y_train_pred = tree.predict(X_train)
y_test_pred = tree.predict(X_test)
tree_train = accuracy_score(y_train, y_train_pred)
tree_test = accuracy_score(y_test, y_test_pred)
print(f'Decision tree train/test accuracies 'f'{tree_train:.3f}/{tree_test:.3f}')
ada = ada.fit(X_train, y_train)
y_train_pred = ada.predict(X_train)
y_test_pred = ada.predict(X_test)
ada_train = accuracy_score(y_train, y_train_pred)
ada_test = accuracy_score(y_test, y_test_pred)
print(f'AdaBoost train/test accuracies 'f'{ada_train:.3f}/{ada_test:.3f}')


#OUTPUT
Decision tree train/test accuracies 0.916/0.875
AdaBoost train/test accuracies 0.968/0.917
```
the AdaBoost model predicts all class labels of the training dataset correctly
and also shows a slightly improved test dataset performance compared to the decision tree stump. However, you can also see that we introduced additional variance with our attempt to reduce the model bias—a greater gap between training and test performance.
and achieved very similar accuracy scores as the bagging classifier that we trained in the previous section.
However, we must note that it is considered bad practice to select a model based on the repeated usage of the test dataset. The estimate of the generalization performance may be overoptimistic
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250617090143.png]]


The BigChaos Solution to the Netflix Grand Prize