Our goal is to build a stronger meta-classifier that balances out the individual classifiers’ weaknesses on a particular dataset. In more precise mathematical terms, we can write the weighted majority vote as follows:
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250616100303.png]]
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250616100405.png]]
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250616100517.png]]
via the predict_proba method, he modified version of the majority vote for
predicting class labels from probabilities can be written as follows:
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250616100917.png]]
Here, pij is the predicted probability of the jth classifier for class label i.
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250616101250.png]]

let’s now implement MajorityVoteClassifier in Python:
```
from sklearn.base import BaseEstimator
from sklearn.base import ClassifierMixin
from sklearn.preprocessing import LabelEncoder
from sklearn.base import clone
from sklearn.pipeline import _name_estimators
import operator
class MajorityVoteClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, classifiers, vote='classlabel', weights=None):
        self.classifiers = classifiers
        self.named_classifiers = {
            key: value for key,
            value in _name_estimators(classifiers)
        }
        self.vote = vote
        self.weights = weights
    def fit(self, X, y):
        if self.vote not in ('probability', 'classlabel'):
            raise ValueError(f"vote must be 'probability' "
                             f"or 'classlabel'"
                             f"; got(vote={self.vote})")
        if self.weights and len(self.weights) != len(self.classifiers):
            raise ValueError(f'Number of classifiers and'f' weights must be equal'f'; got {len(self.weights)} weights,'f' {len(self.classifiers)} classifiers')
        # Use LabelEncoder to ensure class labels start
        # with 0, which is important for np.argmax
        # call in self.predict
        self.lablenc_ = LabelEncoder()
        self.lablenc_.fit(y)
        self.classes_ = self.lablenc_.classes_
        self.classifiers_ = []
        for clf in self.classifiers:
            fitted_clf = clone(clf).fit(X,
                                        self.lablenc_.transform(y))
            self.classifiers_.append(fitted_clf)
        return self
```
We used the BaseEstimator and ClassifierMixin parent classes to get some base
functionality for free, including the get_params and set_params methods to set and return the classifier’s parameters, as well as the score method to calculate the prediction accuracy.

Next, we will add the predict method to predict the class label via a majority vote based on the class labels if we initialize a new MajorityVoteClassifier object with vote='classlabel'. Alternatively, we will be able to initialize the ensemble classifier with vote='probability' to predict the class label based on the class membership probabilities. Furthermore, we will also add a predict_proba method to return the averaged probabilities, which is useful when computing the receiver operating characteristic area under the curve (ROC AUC):
```
# this should be written in the prev class
 def predict(self, X):
        if self.vote == 'probability':
            maj_vote = np.argmax(self.predict_proba(X), axis=1)
        else: # 'classlabel' vote
            # collect results from clf.predict class
            predictions = np.asarray([
                clf.predict(X) for clf in self.classifiers_
            ]).T
            maj_vote = np.apply_along_axis(
                lambda x: np.argmax(
                    np.bincount(x, weights=self.weights)
                ),
                axis=1, arr=predictions
            )
        maj_vote = self.lablenc_.inverse_transform(maj_vote)
        return maj_vote
    def predict_proba(self, X):
        probas = np.asarray([clf.predict_proba(X) for clf in self.classifiers_])
        avg_proba = np.average(probas, axis=0, weights=self.weights)
        return avg_proba
    
    def get_params(self, deep=True):
        if not deep:
            return super().get_params(deep=False)
        else:
            out = self.named_classifiers.copy()
            for name, step in self.named_classifiers.items():
                for key, value in step.get_params(deep=True).items():
                    out[f'{name}__{key}'] = value
            return out
    
            
```
Also, note that we defined our own modified version of the get_params method to use the `_name_` estimators function to access the parameters of individual classifiers in the ensemble; this may look a little bit complicated at first, but it will make perfect sense when we use grid search for hyperpa-rameter tuning in later sections.
![](https://i.imgur.com/enPHtkn.png)
# Using the majority voting principle to make predictions
we will use Iris dataset with two features, sepal width and petal length.
Although our MajorityVoteClassifier generalizes to multiclass problems, we will only classify flower examples from the Iris-versicolor and Iris-virginica classes, with which we will compute the ROC AUC later. The code is as follows:
```
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
iris = datasets.load_iris()
X, y = iris.data[50:, [1, 2]], iris.target[50:]
le = LabelEncoder()
y = le.fit_transform(y)

```
![](https://i.imgur.com/Xl0Z4Pe.png)
we will split the Iris examples into 50 percent training and 50 percent test data:
```
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.5,random_state=1,stratify=y)
```
we now will train three different classifiers:
•Logistic regression classifier
•Decision tree classifier
•k-nearest neighbors classifier
We will then evaluate the model performance of each classifier via 10-fold cross-validation on the training dataset before we combine them into an ensemble classifier:
```
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
clf1 = LogisticRegression(penalty='l2', C=0.001, solver='lbfgs',random_state=1)
clf2 = DecisionTreeClassifier(max_depth=1, criterion='entropy', random_state=0)
clf3 = KNeighborsClassifier(n_neighbors=1, p=2, metric='minkowski')
pipe1 = Pipeline([
    ['sc', StandardScaler()], ['clf', clf1]
])
pipe3 = Pipeline([
    ['sc', StandardScaler()],
    ['clf', clf3]
])
clf_labels = ['Logistic regression', 'Decision tree', 'KNN']
print('10-fold cross validation:\n')
for clf, label in zip([pipe1, clf2, pipe3], clf_labels):
    scores = cross_val_score(estimator=clf, X=X_train, y=y_train,cv=10, scoring='roc_auc')
    print(f'ROC AUC: {scores.mean():.2f} 'f'(+/- {scores.std():.2f}) [{label}]')

```
```
10-fold cross validation:
ROC AUC: 0.92 (+/- 0.15) [Logistic regression]
ROC AUC: 0.87 (+/- 0.18) [Decision tree]
ROC AUC: 0.85 (+/- 0.13) [KNN]
```
he predictive performances of the individual classifiers are almost equal.

combine the individual classifiers for majority rule voting in our MajorityVoteClassifier:
```
mv_clf = MajorityVoteClassifier(classifiers=[pipe1, clf2, pipe3])
mv_clf.fit(X_train, y_train)  # ← REQUIRED!
clf_labels += ['Majority voting']
all_clf = [pipe1, clf2, pipe3, mv_clf]
for clf, label in zip(all_clf, clf_labels):
    scores = cross_val_score(estimator=clf,X=X_train,y=y_train,cv=10,scoring='accuracy')
    print(f'Accuracy: {scores.mean():.2f} 'f'(+/- {scores.std():.2f}) [{label}]')


# OUTPUT
Accuracy: 0.44 (+/- 0.08) [Logistic regression]
Accuracy: 0.86 (+/- 0.20) [Decision tree]
Accuracy: 0.84 (+/- 0.15) [KNN]
Accuracy: 0.92 (+/- 0.13) [Majority voting]
```
the performance of MajorityVotingClassifier has improved over the individual
classifiers in the 10-fold cross-validation evaluation.

# Evaluating and tuning the ensemble classifier
we are going to compute the ROC curves from the test dataset to check that
MajorityVoteClassifier generalizes well with unseen data.
```
from sklearn.metrics import roc_curve
from sklearn.metrics import auc
colors = ['black', 'orange', 'blue', 'green']
linestyles = [':', '--', '-.', '-']
for clf, label, clr, ls in zip(all_clf, clf_labels, colors, linestyles):
    # assuming the label of the positive class is 1
    y_pred = clf.fit(X_train, y_train).predict_proba(X_test)[:, 1]
    fpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_pred)
    roc_auc = auc(x=fpr, y=tpr)
    plt.plot(fpr, tpr, color=clr, linestyle=ls, label=f'{label} (auc = {roc_auc:.2f})')
plt.legend(loc='lower right')
plt.plot([0, 1], [0, 1],linestyle='--',color='gray',linewidth=2)
plt.xlim([-0.1, 1.1])
plt.ylim([-0.1, 1.1])
plt.grid(alpha=0.5)
plt.xlabel('False positive rate (FPR)')
plt.ylabel('True positive rate (TPR)')
plt.show()
```

![[Pasted image 20250616145706.png]]
As you can see in the resulting ROC, the ensemble classifier also performs well on the test dataset (ROC AUC = 0.95). However, you can see that the logistic regression classifier performs similarly well on the same dataset, which is probably due to the high variance (in this case, the sensitivity of how we split the dataset) given the small size of the dataset.

Since we only selected two features for the classification examples, it would be interesting to see what the decision region of the ensemble classifier actually looks like.
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250616152222.png]]
Before we tune the individual classifier’s parameters for ensemble classification, let’s call the get_params method to get a basic idea of how we can access the individual parameters inside a GridSearchCV
```
mv_clf.get_params()
```
we now know how to access the individual
classifier’s attributes. Let’s now tune the inverse regularization parameter, C, of the logistic regression
classifier and the decision tree depth via a grid search for demonstration purposes:
```
from sklearn.model_selection import GridSearchCV
params = {'decisiontreeclassifier__max_depth': [1, 2],'pipeline-1__clf__C': [0.001, 0.1, 100.0]}
grid = GridSearchCV(estimator=mv_clf,param_grid=params,cv=10,scoring='roc_auc')
grid.fit(X_train, y_train)
```
After the grid search has completed, we can print the different hyperparameter value combinations
and the average ROC AUC scores computed via 10-fold cross-validation as follows:
```
>>> for r, _ in enumerate(grid.cv_results_['mean_test_score']):
...mean_score = grid.cv_results_['mean_test_score'][r]
...std_dev = grid.cv_results_['std_test_score'][r]
...params = grid.cv_results_['params'][r]
...print(f'{mean_score:.3f} +/- {std_dev:.2f} {params}')
0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 1,
'pipeline-1__clf__C': 0.001}
0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 1,
'pipeline-1__clf__C': 0.1}
0.967 +/- 0.10 {'decisiontreeclassifier__max_depth': 1,
'pipeline-1__clf__C': 100.0}
0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 2,
'pipeline-1__clf__C': 0.001}
0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 2,
'pipeline-1__clf__C': 0.1}
0.967 +/- 0.10 {'decisiontreeclassifier__max_depth': 2,
'pipeline-1__clf__C': 100.0}
>>> print(f'Best parameters: {grid.best_params_}')
Best parameters: {'decisiontreeclassifier__max_depth': 1,
'pipeline-1__clf__C': 0.001}
>>> print(f'ROC AUC : {grid.best_score_:.2f}')
ROC AUC: 0.98
```
As you can see, we get the best cross-validation results when we choose a lower regularization strength (C=0.001), whereas the tree depth does not seem to affect the performance at all, suggesting that a decision stump is sufficient to separate the data. To remind ourselves that it is a bad practice to use the test dataset more than once for model evaluation, we are not going to estimate the generalization performance of the tuned hyperparameters in this section. We will move on swiftly to an alternative approach for ensemble learning: bagging.
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250616152656.png]]




