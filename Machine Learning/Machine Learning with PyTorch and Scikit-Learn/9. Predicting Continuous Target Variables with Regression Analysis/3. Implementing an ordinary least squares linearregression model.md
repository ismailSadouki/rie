
### Solving regression for regression parameters with gradient descent

Consider our implementation of the Adaptive Linear Neuron (Adaline) from Chapter 2, Training Simple Machine Learning Algorithms for Classification. You will remember that the artificial neuron uses a linear  activation function. Also, we defined a loss function, L(w), which we minimized to learn the weights via optimization algorithms, such as gradient descent (GD) and stochastic gradient descent (SGD).
This loss function in Adaline is the mean squared error (MSE), which is identical to the loss function that we use for OLS:
![](https://i.imgur.com/cCVTggZ.png)

Here, $\hat{y}$ is the predicted value $\hat{y}=w^Tx+b$ (note that the term is just used for convenience to derive 2 the update rule of GD). Essentially, OLS regression can be understood as Adaline without the threshold function so that we obtain continuous target values instead of the class labels 0 and 1. To demonstrate this, let’s take the GD implementation of Adaline from Chapter 2 and remove the threshold function to implement our first linear regression model:
```
class LinearRegressionGD:
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self, X, y):
        rgen = np.random.RandomStates(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])
        self.b_ = np.array([0.])
        self.losses_ = []

        for i in range(self.n_iter):
            output = self.net_input(X)
            errors = (y-output)
            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]
            self.b_ += self.eta * 2.0 * errors.mean()
            loss = (errors**2).mean()
        return self

    def net_input(self, X):
        return np.dot(X, self.w_) + self.b_

    def predict(self, X):
        return self.net_input(X)
```

To see our LinearRegressionGD regressor in action, let’s use the Gr Living Area (size of the living area above ground in square feet) feature from the Ames Housing dataset as the explanatory variable and train a model that can predict SalePrice. Furthermore, we will standardize the variables for better convergence of the GD algorithm. The code is as follows:
```
X = df[['Gr Liv Area']].values
y = df['SalePrice'].values
from sklearn.preprocessing import StandardScaler
sc_x = StandardScaler()
sc_y = StandardScaler()
X_std = sc_x.fit_transform(X)
y_std = sc_y.fit_transform(y[:, np.newaxis]).flatten()
lr = LinearRegressionGD(eta=0.1)
lr.fit(X_std, y_std)
```
Notice the workaround regarding y_std, using np.newaxis and flatten. Most data preprocessing classes in scikit-learn expect data to be stored in two-dimensional arrays. In the previous code ex-ample, the use of np.newaxis in y[:, np.newaxis] added a new dimension to the array. Then, after StandardScaler returned the scaled variable, we converted it back to the original one-dimensional array representation using the flatten() method for our convenience.

We discussed in Chapter 2 that it is always a good idea to plot the loss as a function of the number of epochs (complete iterations) over the training dataset when we are using optimization algorithms, such as GD, to check that the algorithm converged to a loss minimum (here, a global loss minimum):
```
plt.plot(range(1, lr.n_iter+1), lr.losses_)
plt.ylabel('MSE')
plt.xlabel('Epoch')
plt.show()
```
![](https://i.imgur.com/QKKEg6w.png)


Next, let’s visualize how well the linear regression line fits the training data. To do so, we will define a simple helper function that will plot a scatterplot of the training examples and add the regression line:
```
def lin_regplot(X, y, model):
    plt.scatter(X, y, c='steelblue', edgecolor='white', s=70)
    plt.plot(X, model.predict(X), color='black', lw=2)
lin_regplot(X_std, y_std, lr)
plt.xlabel(' Living area above ground (standardized)')
plt.ylabel('Sale price (standardized)')
plt.show()
```
![](https://i.imgur.com/bJ3Yb17.png)
Although this observation makes sense, the data also tells us that the living area size does not explain house prices very well in many cases. Later in this chapter, we will discuss how to quantify the performance of a regression model. Interestingly, we can also observe several outliers, for example, the three data points corresponding to a standardized living area greater than 6. We will discuss how we can deal with outliers later in this chapter.


To scale the predicted price back onto the original price in U.S. dollars scale, we can simply apply the inverse_transform method of StandardScaler:

```
feature_std = sc_x.transform(np.array([[2500]]))
target_std = lr.predict(feature_std)
target_reverted = sc_y.inverse_transform(target_std.reshape(-1, 1))
print(f'Sales price: ${target_reverted.flatten()[0]:.2f}')
```
it is also worth mentioning that we technically don’t have to update the intercept parameter (for instance, the bias unit, b) if we are working with standardized variables, since the y axis intercept is always 0 in those cases. We can quickly confirm this by printing the model parameters:
```
print(f'Slope: {lr.w_[0]:.3f}')
print(f'Intercept: {lr.b_[0]:.3f}')
```
# Estimating the coefficient of a regression model via scikit-learn
many of scikit-learn’s estimators for regression make use of the least squares implementation in SciPy (scipy.linalg.lstsq), which, in turn, uses highly optimized code optimizations based on the Linear Algebra Package (LAPACK). The linear regression implementation in scikit-learn also works (better) with unstandardized variables, since it does not use (S)GD-based optimization, so we can skip the standardization step:
### ⚠️ When is Standardization Needed?

You **do need to standardize** your data **if**:

- You’re using **SGD**, **Ridge**, **Lasso**, or **ElasticNet** (especially with solvers that depend on gradient steps),
    
- Or you're using **regularization** — because scale affects how much each coefficient is penalized.

### ✅ Summary

- `LinearRegression` in scikit-learn **does not require standardization** and may even perform **better without it**, especially in terms of interpretability.
    
- However, **for regularized models or SGD-based models**, standardization is generally **essential**.
```
from sklearn.linear_model import LinearRegression
slr = LinearRegression()
slr.fit(X, y)
y_pred = slr.predict(X)
print(f'Slope: {slr.coef_[0]:.3f}')
print(f'Intercept: {slr.intercept_:.3f}')
```
As you can see from executing this code, scikit-learn’s LinearRegression model, fitted with the unstandardized Gr Liv Area and SalePrice variables, yielded different model coefficients, since the features have not been standardized. However, when we compare it to our GD implementation by plotting SalePrice against Gr Liv Area, we can qualitatively see that it fits the data similarly well:
![](https://i.imgur.com/MRHB3mU.png)
![Uploading file...bnq5e]()
