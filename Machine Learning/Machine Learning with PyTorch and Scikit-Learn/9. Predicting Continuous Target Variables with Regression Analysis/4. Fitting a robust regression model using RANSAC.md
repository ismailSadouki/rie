Linear regression models can be heavily impacted by the presence of outliers. In certain situations, a very small subset of our data can have a big effect on the estimated model coefficients.
removing outliers always requires our own judgment as data scientists as well as our domain knowledge.
As an alternative to throwing out outliers, we will look at a robust method of regression using the **RANdom SAmple Consensus (RANSAC)** algorithm, which fits a regression model to a subset of the data, the so-called **inliers**.

RANSAC algorithm:
1.Select a random number of examples to be inliers and fit the model.
2.Test all other data points against the fitted model and add those points that fall within a user-given tolerance to the inliers.
3.Refit the model using all inliers.
4.Estimate the error of the fitted model versus the inliers.
5.Terminate the algorithm if the performance meets a certain user-defined threshold or if a fixed number of iterations was reached; go back to step 1 otherwise.

Letâ€™s now use a linear model in combination with the RANSAC algorithm as implemented in scikit-learnâ€™s RANSACRegressor class:
```
from sklearn.linear_model import RANSACRegressor
ransec = RANSACRegressor(
    LinearRegression(),
    max_trials=100, # default
    min_samples=0.95,
    residual_threshold=None, # default
    random_state=123
)
ransec.fit(X, y)
```
We set the maximum number of iterations of the RANSACRegressor to 100, and using min_samples=0.95, we set the minimum number of the randomly chosen training examples to be at least 95 percent of the dataset.

By default (via residual_threshold=None), scikit-learn uses the MAD estimate to select the inlier threshold, where MAD stands for the median absolute deviation of the target values, y. However, the choice of an appropriate value for the inlier threshold is problem-specific, which is one disadvantage of RANSAC.

Many different approaches have been developed in recent years to select a good inlier threshold automatically. You can find a detailed discussion in Automatic Estimation of the Inlier Threshold in Robust Multiple Structures Fitting by R. Toldo and A. Fusiello, Springer, 2009 (in Image Analysis and Processingâ€“ICIAP 2009, pages: 123-131).

Once we have fitted the RANSAC model, letâ€™s obtain the inliers and outliers from the fitted RANSAC linear regression model and plot them together with the linear fit:
```
inlier_mask = ransac.inlier_mask_
outlier_mask = np.logical_not(inlier_mask)
line_X = np.arange(3, 10, 1)
line_y_ransac = ransac.predict(line_X[:, np.newaxis])
plt.scatter(X[inlier_mask], y[inlier_mask],
            c='steelblue', edgecolor='white',
            marker='o', label='Inliers')
plt.scatter(X[outlier_mask], y[outlier_mask],c='limegreen', edgecolor='white',marker='s', label='Outliers')
plt.plot(line_X, line_y_ransac, color='black', lw=2)
plt.xlabel('Living area above ground in square feet')
plt.ylabel('Sale price in U.S. dollars')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()
```
![](https://i.imgur.com/EIAxNLV.png)

When we print the slope and intercept of the model by executing the following code, the linear re-
gression line will be slightly different from the fit that we obtained in the previous section without
using RANSAC:
```
>>> print(f'Slope: {ransac.estimator_.coef_[0]:.3f}')
Slope: 106.348
>>> print(f'Intercept: {ransac.estimator_.intercept_:.3f}')
Intercept: 20190.093
```
Remember that we set the residual_threshold parameter to None, so RANSAC was using the MAD to compute the threshold for flagging inliers and outliers. The MAD, for this dataset, can be computed as follows:
```
>>> def mean_absolute_deviation(data):
...
return np.mean(np.abs(data - np.mean(data)))
>>> mean_absolute_deviation(y)
58269.561754979375
```
So, if we want to identify fewer data points as outliers, we can choose a residual_threshold value greater than the preceding MAD. For example, Figure 9.10 shows the inliers and outliers of a RANSAC linear regression model with a residual threshold of 65,000:
![](https://i.imgur.com/XkDJ3wH.png)
Using RANSAC, we reduced the potential effect of the outliers in this dataset, but we donâ€™t know whether this approach will have a positive effect on the predictive performance for unseen data or not. Thus, in the next section, we will look at different approaches for evaluating a regression model, which is a crucial part of building systems for predictive modeling.


# Evaluating the performance of linear regression models
Instead of proceeding with the simple regression model, we will now
use all five features in the dataset and train a multiple regression model:
```
from sklearn.model_selection import train_test_split
target = 'SalePrice'
features = df.columns[df.columns != target]
X = df[features].values
y = df[target].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)
slr = LinearRegression()
slr.fit(X_train, y_train)
y_train_pred = slr.predict(X_train)
y_test_pred = slr.predict(X_test)
```
Residual plots are a commonly used graphical tool for diagnosing regression
models. They can help to detect nonlinearity and outliers and check whether the errors are randomly distributed.
we simply subtract the true target variables from our predicted responses:
```
x_max = np.max([np.max(y_train_pred), np.max(y_test_pred)])
x_min = np.min([np.min(y_train_pred), np.min(y_test_pred)])
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3), sharey=True)
ax1.scatter(y_test_pred, y_test_pred - y_test,c='limegreen', marker='s',edgecolor='white',label='Test data')
ax2.scatter(y_train_pred, y_train_pred - y_train,c='steelblue', marker='o', edgecolor='white',label='Training data')
ax1.set_ylabel('Residuals')
for ax in (ax1, ax2):
    ax.set_xlabel('Predicted values')
    ax.legend(loc='upper left')
    ax.hlines(y=0, xmin=x_min-100, xmax=x_max+100,color='black', lw=2)
plt.tight_layout()
plt.show()
```
![](https://i.imgur.com/lWImkVF.png)
In the case of a perfect prediction, the residuals would be exactly zero, which we will probably never encounter in realistic and practical applications. However, for a good regression model, we would expect the errors to be randomly distributed and the residuals to be randomly scattered around the centerline. If we see patterns in a residual plot, it means that our model is unable to capture some ex-planatory information, which has leaked into the residuals, as you can see to a degree in our previous residual plot. Furthermore, we can also use residual plots to detect outliers, which are represented by the points with a large deviation from the centerline.


![](https://i.imgur.com/DLJZH7d.png)

Similar to prediction accuracy in classification contexts, we can use the MSE for cross-validation and model selection.
```
from sklearn.metrics import mean_squared_error
mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
print(f'MSE train: {mse_train:.2f}')
print(f'MSE test: {mse_test:.2f}')
# OUTPUT
MSE train: 1497216245.85
MSE test: 1516565821.00
```
We can see that the MSE on the training dataset is larger than on the test set, which is an indicator that our model is slightly overfitting the training data in this case. Note that it can be more intuitive to show the error on the original unit scale (here, dollar instead of dollar-squared), which is why we may choose to compute the square root of the MSE, called root mean squared error, or the mean absolute error (MAE), which emphasizes incorrect prediction slightly less:
![](https://i.imgur.com/as9Q2Dd.png)
```
from sklearn.metrics import mean_absolute_error
mae_train = mean_absolute_error(y_train, y_train_pred)
mae_test = mean_absolute_error(y_test, y_test_pred)
print(f'MAE train: {mae_train:.2f}')
print(f'MAE test: {mae_test:.2f}')
#OUTPUT
MAE train: 25983.03
MAE test: 24921.29
```
Based on the test set MAE, we can say that the model makes an error of approximately $25,000 on average.
![](https://i.imgur.com/Nr8flfI.png)
![](https://i.imgur.com/Sv7Bchp.png)
For the training dataset, R2 is bounded between 0 and 1, but it can become negative for the test dataset. A negative R2 means that the regression model fits the data worse than a horizontal line representing the sample mean. (In practice, this often happens in the case of extreme overfitting, or if we forget to scale the test set in the same manner we scaled the training set.) If R2 = 1, the model fits the data perfectly with a corresponding MSE = 0.
Evaluated on the training data, the R2 of our model is 0.77, which isnâ€™t great but also not too bad given that we only work with a small set of features. However, the R2 on the test dataset is only slightly smaller, at 0.75, which indicates that the model is only overfitting slightly:
```
from sklearn.metrics import r2_score
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
print(f'R^2 train: {train_r2:.3f}, {test_r2:.3f}')
```
# Using regularized methods for regression
regularization is one approach to tackling the problem of overfitting by adding additional information and thereby shrinking the parameter values of the model to induce a penalty against complexity. The most popular approaches to regularized linear regression are the so-called ridge regression, least absolute shrinkage and selection operator (LASSO), and elastic net.
Ridge regression is an L2 penalized model where we simply add the squared sum of the weights to the MSE loss function:
![](https://i.imgur.com/yrb9d1T.png)

Here, the L2 term is defined as follows:
![](https://i.imgur.com/x1VaJBp.png)
By increasing the value of hyperparameter ğœ†ğœ†, we increase the regularization strength and thereby shrink the weights of our model. Please note that, as mentioned in Chapter 3, the bias unit b is not regularized.

An alternative approach that can lead to sparse models is LASSO. Depending on the regularization strength, certain weights can become zero, which also makes LASSO useful as a supervised feature selection technique:![](https://i.imgur.com/rLu5C7V.png)
Here, the L1 penalty for LASSO is defined as the sum of the absolute magnitudes of the model weights

However, a limitation of LASSO is that it selects at most n features if m > n, where n is the number of training examples. This may be undesirable in certain applications of feature selection. In practice, however, this property of LASSO is often an advantage because it avoids saturated models. The saturation of a model occurs if the number of training examples is equal to the number of features, which is a form of overparameterization. As a consequence, a saturated model can always fit the training data perfectly but is merely a form of interpolation and thus is not expected to generalize well.

A compromise between ridge regression and LASSO is elastic net, which has an L1 penalty to generate sparsity and an L2 penalty such that it can be used for selecting more than n features if m > n:
![](https://i.imgur.com/5mEl1Yw.png)


Those regularized regression models are all available via scikit-learn, and their usage is similar to the regular regression model except that we have to specify the regularization strength via the parameter ğœ†ğœ†, for example, optimized via k-fold cross-validation.

A ridge regression model can be initialized via:
```
from sklearn.linear_model import Ridge
ridge = Ridge(alpha=1.0)
```
Note that the regularization strength is regulated by the parameter alpha, which is similar to the parameter ğœ†ğœ†. Likewise, we can initialize a LASSO regressor from the linear_model submodule: 
```
from sklearn.linear_model import Lasso
lasso = Lasso(alpha=1.0)
```
the ElasticNet implementation allows us to vary the L1 to L2 ratio:
```
from sklearn.linear_model import ElasticNet
elanet = ElasticNet(alpha=1.0, l1_ratio=0.5)
```
For example, if we set l1_ratio to 1.0, the ElasticNet regressor would be equal to LASSO regression.
