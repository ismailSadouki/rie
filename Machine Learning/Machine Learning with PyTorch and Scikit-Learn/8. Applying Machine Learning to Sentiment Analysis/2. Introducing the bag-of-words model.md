The idea behind bag-of-words:
1.We create a vocabulary of unique tokens—for example, words—from the entire set of documents.
2.We construct a feature vector from each document that contains the counts of how often each word occurs in the particular document.

Since the unique words in each document represent only a small subset of all the words in the bag-of-words vocabulary, the feature vectors will mostly consist of zeros, which is why we call them sparse.

# Transforming words into feature vectors
To construct a bag-of-words model based on the word counts in the respective documents, we can use the CountVectorizer class implemented in scikit-learn.
CountVectorizer takes an array of text data, which can be documents or sentences, and constructs the bag-of-words model for us:
```
from sklearn.feature_extraction.text import CountVectorizer
count = CountVectorizer()
docs = np.array(['The sun is shining','The weather is sweet','The sun is shining, the weather is sweet,''and one and one is two'])
bag = count.fit_transform(docs)
print(count.vocabulary_)
# {'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}
```
the vocabulary is stored in a Python dictionary that maps the unique words to integer indices. Next, let’s print the feature vectors that we just created:
```
print(bag.toarray())
#OUTPUT
[[0 1 0 1 1 0 1 0 0]
 [0 1 0 0 0 1 1 0 1]
 [2 3 2 1 1 1 2 1 1]]
```
Each index position in the feature vectors shown here corresponds to the integer values that are stored as dictionary items in the CountVectorizer vocabulary.

These values in the feature vectors are also called the **raw term frequencies**: tf(t, d)—the number of times a term, t, occurs in a document, d.


in the bag-of-words model, the word or term order in a sentence or document does not matter. The order in which the term frequencies appear in the feature vector is derived from the vocabulary indices, which are usually assigned alphabetically. 
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250618151049.png]]
# Assessing word relevancy via term frequency-inverse document frequency
When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. These frequently occurring words typically don’t contain useful or discriminatory information. In this subsection, you will learn about a useful technique called the term frequency-in-verse document frequency (tf-idf),which can be used to downweight these frequently occurring
words in the feature vectors.

![](https://i.imgur.com/IrBwETh.png)
![](https://i.imgur.com/GyV8Agv.png)
Here, nd is the total number of documents, and df(d, t) is the number of documents, d, that contain the term t.Note that adding the constant 1 to the denominator is optional and serves the purpose of assigning a non-zero value to terms that occur in none of the training examples; the log is used to ensure that low document frequencies are not given too much weight.

the TfidfTransformer class, which takes the raw term frequencies from the CountVectorizer class as input and transforms them into tf-idfs:
```
from sklearn.feature_extraction.text import TfidfTransformer
tfidf = TfidfTransformer(use_idf=True, norm="l2", smooth_idf=True)
np.set_printoptions(precision=2)
print(tfidf.fit_transform(count.fit_transform(docs)).toarray())

[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]
 [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]
 [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]
```
As you saw in the previous subsection, the word 'is' had the largest term frequency in the third document, being the most frequently occurring word. However, after transforming the same feature vector into tf-idfs, the word 'is' is now associated with a relatively small tf-idf (0.45) in the third document, since it is also present in the first and second document and thus is unlikely to contain any useful discriminatory information.

However, if we’d manually calculated the tf-idfs of the individual terms in our feature vectors, we would have noticed that TfidfTransformer calculates the tf-idfs slightly differently compared to the standard textbook equations that we defined previously. The equation for the inverse document frequency implemented in scikit-learn is computed as follows:
![](https://i.imgur.com/CLhIiNp.png)
Similarly, the tf-idf computed in scikit-learn deviates slightly from the default equation we defined earlier:
![](https://i.imgur.com/8isL7OY.png)
Note that the “+1” in the previous idf equation is due to setting smooth_idf=True in the previous code example, which is helpful for assigning zero weight (that is, idf(t, d) = log(1) = 0) to terms that occur in all documents.

While it is also more typical to normalize the raw term frequencies before calculating the tf-idfs, the TfidfTransformer class normalizes the tf-idfs directly. By default (norm='l2'), scikit-learn’s TfidfTransformer applies the L2-normalization, which returns a vector of length 1 by dividing an unnormalized feature vector, v, by its L2-norm:
![](https://i.imgur.com/IjAaYZw.png)
