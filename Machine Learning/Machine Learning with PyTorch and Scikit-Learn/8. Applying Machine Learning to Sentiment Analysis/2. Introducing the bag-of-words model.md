The idea behind bag-of-words:
1.We create a vocabulary of unique tokens—for example, words—from the entire set of documents.
2.We construct a feature vector from each document that contains the counts of how often each word occurs in the particular document.

Since the unique words in each document represent only a small subset of all the words in the bag-of-words vocabulary, the feature vectors will mostly consist of zeros, which is why we call them sparse.

# Transforming words into feature vectors
To construct a bag-of-words model based on the word counts in the respective documents, we can use the CountVectorizer class implemented in scikit-learn.
CountVectorizer takes an array of text data, which can be documents or sentences, and constructs the bag-of-words model for us:
```
from sklearn.feature_extraction.text import CountVectorizer
count = CountVectorizer()
docs = np.array(['The sun is shining','The weather is sweet','The sun is shining, the weather is sweet,''and one and one is two'])
bag = count.fit_transform(docs)
print(count.vocabulary_)
# {'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}
```
the vocabulary is stored in a Python dictionary that maps the unique words to integer indices. Next, let’s print the feature vectors that we just created:
```
print(bag.toarray())
#OUTPUT
[[0 1 0 1 1 0 1 0 0]
 [0 1 0 0 0 1 1 0 1]
 [2 3 2 1 1 1 2 1 1]]
```
Each index position in the feature vectors shown here corresponds to the integer values that are stored as dictionary items in the CountVectorizer vocabulary.

These values in the feature vectors are also called the **raw term frequencies**: tf(t, d)—the number of times a term, t, occurs in a document, d.


in the bag-of-words model, the word or term order in a sentence or document does not matter. The order in which the term frequencies appear in the feature vector is derived from the vocabulary indices, which are usually assigned alphabetically. 
<!--⚠️Imgur upload failed, check dev console-->
![[Pasted image 20250618151049.png]]
# Assessing word relevancy via term frequency-inverse document frequency
When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. These frequently occurring words typically don’t contain useful or discriminatory information. In this subsection, you will learn about a useful technique called the term frequency-in-verse document frequency (tf-idf),which can be used to downweight these frequently occurring
words in the feature vectors.

![](https://i.imgur.com/IrBwETh.png)
![](https://i.imgur.com/GyV8Agv.png)
Here, nd is the total number of documents, and df(d, t) is the number of documents, d, that contain the term t.Note that adding the constant 1 to the denominator is optional and serves the purpose of assigning a non-zero value to terms that occur in none of the training examples; the log is used to ensure that low document frequencies are not given too much weight.

the TfidfTransformer class, which takes the raw term frequencies from the CountVectorizer class as input and transforms them into tf-idfs:
```
from sklearn.feature_extraction.text import TfidfTransformer
tfidf = TfidfTransformer(use_idf=True, norm="l2", smooth_idf=True)
np.set_printoptions(precision=2)
print(tfidf.fit_transform(count.fit_transform(docs)).toarray())

[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]
 [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]
 [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]
```
As you saw in the previous subsection, the word 'is' had the largest term frequency in the third document, being the most frequently occurring word. However, after transforming the same feature vector into tf-idfs, the word 'is' is now associated with a relatively small tf-idf (0.45) in the third document, since it is also present in the first and second document and thus is unlikely to contain any useful discriminatory information.

However, if we’d manually calculated the tf-idfs of the individual terms in our feature vectors, we would have noticed that TfidfTransformer calculates the tf-idfs slightly differently compared to the standard textbook equations that we defined previously. The equation for the inverse document frequency implemented in scikit-learn is computed as follows:
![](https://i.imgur.com/CLhIiNp.png)
Similarly, the tf-idf computed in scikit-learn deviates slightly from the default equation we defined earlier:
![](https://i.imgur.com/8isL7OY.png)
Note that the “+1” in the previous idf equation is due to setting smooth_idf=True in the previous code example, which is helpful for assigning zero weight (that is, idf(t, d) = log(1) = 0) to terms that occur in all documents.

While it is also more typical to normalize the raw term frequencies before calculating the tf-idfs, the TfidfTransformer class normalizes the tf-idfs directly. By default (norm='l2'), scikit-learn’s TfidfTransformer applies the L2-normalization, which returns a vector of length 1 by dividing an unnormalized feature vector, v, by its L2-norm:
![](https://i.imgur.com/IjAaYZw.png)
![](https://i.imgur.com/W1c3j0E.png)

# Cleaning text data
the first important step—before we build our bag-of-words model—is to clean the text data by stripping it of all unwanted characters.
To illustrate why this is important, let’s display the last 50 characters from the first document in the reshuffled movie review dataset:
```
>>> df.loc[0, 'review'][-50:]
'is seven.<br /><br />Title (Brazil): Not Available'
```
the text contains HTML markup as well as punctuation and other non-letter
characters. While HTML markup does not contain many useful semantics, punctuation marks can represent useful, additional information in certain NLP contexts. However, for simplicity, we will now remove all punctuation marks except for emoticon characters, such as :), since those are certainly useful for sentiment analysis.
```
import re
def preprocessor(text):
    text = re.sub('<[^>]*>', '', text)
    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)',text)
    text = (re.sub('[\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', ''))
    return text
```
Via the first regex, `<[^>]*>`, in the preceding code section, we tried to remove all of the HTML markup from the movie reviews. Although many programmers generally advise against the use of regex to parse HTML, this regex should be sufficient to clean this particular dataset. Since we are only interested in removing HTML markup and do not plan to use the HTML markup further, using regex to do the job should be acceptable

Next, we removed all  non-word characters from the text via the regex [\W]+ and converted the text into lowercase characters.
Eventually, we added the temporarily stored emoticons to the end of the processed document string. Additionally, we removed the nose character (- in :-)) from the emoticons for consistency.

Although the addition of the emoticon characters to the end of the cleaned document strings may not look like the most elegant approach, we must note that the order of the words doesn’t matter in our bag-of-words model if our vocabulary consists of only one-word tokens. But before we talk more about the splitting of documents into individual terms, words, or tokens, let’s confirm that our preprocessor function works correctly:
```
>>> preprocessor(df.loc[0, 'review'][-50:])
'is seven title brazil not available'
>>> preprocessor("</a>This :) is :( a test :-)!")
'this is a test :) :( :)'
```
Lastly, since we will make use of the cleaned text data over and over again during the next sections, let’s now apply our preprocessor function to all the movie reviews in our DataFrame:
```
df['review'] = df['review'].apply(preprocessor)
```
# Processing documents into tokens
One way to tokenize documents is to split them into individual
words by splitting the cleaned documents at their whitespace characters:
```
def tokenizer(text):
    return text.split()
tokenizer('runners like running and thus they run')
# OUTPUT
['runners', 'like', 'running', 'and', 'thus', 'they', 'run']
```
In the context of tokenization, another useful technique is <mark>word stemming, which is the process of transforming a word into its root form</mark>.It allows us to map related words to the same stem

The original stemming algorithm was developed by Martin F. Porter in 1979 and is hence known as the Porter stemmer algorithm (An algorithm for suffix stripping by Martin F. Porter, Program: Electronic Library and Information Systems, 14(3): 130–137, 1980). The Natural Language Toolkit (NLTK, http://www.nltk.org) for Python implements the Porter stemming algorithm, which we will use in the following code section. To install the NLTK, you can simply execute conda install nltk or pip install nltk.

```
from nltk.stem.porter import PorterStemmer
porter = PorterStemmer()
def tokenizer_porter(text):
    return [porter.stem(word) for word in text.split()]
tokenizer_porter('runners like running and thus they run')
#OUTPUT
['runner', 'like', 'run', 'and', 'thu', 'they', 'run']
```
![](https://i.imgur.com/qGhfYSJ.png)

let’s briefly talk about another useful topic called <mark>stop word removal</mark>.
Stop words are simply those words that are extremely common in all sorts of texts and probably bear no (or only a little) useful information that can be used to distinguish between different classes of documents.
Removing stop words can be useful if we are working with raw or normalized term frequencies rather than tf-idfs, which already downweight the frequently occurring words.

To remove stop words from the movie reviews, we will use the set of 127 English stop words that is available from the NLTK library, which can be obtained by calling the nltk.download function:
```
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')
[w for w in tokenizer_porter('a runner likes'' running and runs a lot') if w not in stop]
#OUTPUT
['runner', 'like', 'run', 'run', 'lot']
```
