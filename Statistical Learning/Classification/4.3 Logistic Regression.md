## 4.3.1 The Logistic Model
In the logistic regression we use the logistic function to fit the model
$$ p(x)= \frac{exp(b_0+b_1x)}{1+exp(b_0+b_1x} \qquad (4.2)$$
after a bit of **manipulation** of (4.2) we find that
$$ \frac{p(X)}{1-p(X)}=exp(\beta_0+\beta_1X) \qquad (4.3)$$
The quantity in the right of the equation is called the `odds`, and can take on any values between 0 and infinity.
the `log odds` or `logit` is when we take the logarithm of (4.3).           (4.4)

* Recall from chapter of linear regression that $\beta_1$ gives the avg change in Y associated with a one-unit increase in X. In a logistic regression model, increasing X by one changes the log odds by $\beta_1$. in (4.4) and it multiplies the odds by $e^\beta_1$ in (4.3).
## 4.3.2 Estimating the regression coefficients
To estimate $\beta_0$ and $\beta_1$ we use the likelihood function since it has better statistical properties:
$$ l(\beta_0,\beta_1) = \prod_{i:y_i=1}p(x_i) \prod_{i^`:y_{i^`}=0}(1-p(x_{i^`})) \qquad (4.5)$$
The estimates $\hat{\beta_0}$ and $\hat{\beta_1}$ are chosen to maximize this likelihood function.
Maximum likelihood is a very general approach that is used to fit many of the non-linear models.

### 4.3.4 Multiple Logistic Regression
we can generalize (4.4) as follows:
$$ log(\frac{p(X)}{1-p(X)}) = \beta_0+\beta_1X_1+...+\beta_pX_p \qquad(4.6)$$
where X = ($X_1,...,X_p$) are p predictors. and it can be rewritten as:
$$ P(X)=\frac{exp(\beta_0+...+\beta_pX_p)}{1+exp(\beta_0+...+\beta_pX_p)}\qquad (4.7)$$

just as 4.3.2 we use the maximum likelihood method to estimate $\beta_0,...,\beta_p$.

## 4.3.5 Multinomial Logistic Regression
It turns out that it is possible to extend the two-class logistic regression approach to the setting of k > 2 classes. and its known as `multinomial logistic regression`. To do this, we first select a single class to serve as the baseline; without loss of generality, we select the `Kth` class for this role. Then we replace (4.7) with the model 
$$ Pr(Y=k|X=x) = \frac{exp(\beta_{k0}+...+\beta_{kp}X_p)}{1+\sum_{l=1}^{K-1}exp(\beta_{l0}+...+\beta_{lp}x_p)}\qquad (4.10) $$
for k=1,...,K-1 and 
$$ Pr(Y=K|X=x) = \frac{1}{1+\sum_{l=1}^{K-1}exp(\beta_{l0}+...+\beta_{lp}x_p)}\qquad (4.11) $$
It is not hard to show that for k=1,...,K-1
$$ log(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}) = \beta_{k0} +...+\beta_{kp}X_p \qquad (4.12) $$
Equation 4.12 indicates that once again, the log odds between any pair of classes is linear in the features.
- The decision to treat the `Kth` class as the baseline is unimportant. the log odds between any pair of class, and the other key model outputs will remain the same.
- Interpretation of the coefficients in a multinomial logistic regression model must be done with care, since it is tied to the choice of baseline.
#### softmax coding:
rather than selecting a baseline class, we treat all K classes symmetrically, and assume that for k=1,...,K
$$ Pr(Y=k|X=x) = \frac{exp(\beta_{k0}+...+\beta_{kp}X_p)}{\sum_{l=1}^{K}exp(\beta_{l0}+...+\beta_{lp}X_p)} \qquad (4.13) $$
Thus, rather than estimating coefficients for K-1 classes, we actually estimate coefficients for all K classes.I t is not hard to see that as a result of (4.13), the log odds ratio between the `kth` and `k'th` classes equals
$$ log(\frac{Pr(Y=k|X=x)}{Pr(Y=k^`|X=x)}) = (\beta_{k0} - \beta_{k^`0}) +...+(\beta_{kp}X_p-\beta_{k^`p}X_p) \qquad (4.14) $$
### 4.4 Generative Models for classification
In this new approach, we model the distribution of the predictors X separately in each of the response classes (i.e for each value of Y). We then use Bayes' theorem to flip these around to estimates for $Pr(Y=k|X=x)$. When the distribution of X within each class is assumed to be normal, it turns out that the model is very similar in form to logistic regression.
Why do we need another method, when we have logistic regression?
- When there is substantial separation between the two classes, the parameter of the logistic regression model are surprisingly unstable.
- If X is approximately normal in each of the classes and the sample size is small, then the approaches in this section may be more accurate than logistic regression.
- The methods in this section can be naturally extended to the case of more than two response classes.
Let $\Pi_k$ represent the overall or `prior` probability that a randomly chosen observation comes from the `kth` class.
Let $f_k(X)=Pr(X|Y=k)$ the density function of X for an observation that comes from the `kth` class.
Then Bayes' theorem states that
$$ Pr(Y=k|X=x) = \frac{\Pi_kf_k(x)}{\sum_{l=1}^K\Pi_lf_l(x)} \qquad (4.15) $$

**Notation**: $p_k(x)=Pr(Y=k|X=x)$; the posterior probability that an observation X=x belongs to the `kth` class. That is, it is the probability that the observation belongs to the `kth` class, given the predictor value for that observation.
**Explanation of 4.15:** It suggests that instead of directly computing the posterior probability $p_k(x)$ as in section 4.3.1, we can simply plug in **estimates** of $\Pi_k$ and $f_k(x)$ into (4.15).
**$\Pi_k$ and $f_k(x)$ :** Estimating $\Pi_k$ is easy if we have a random sample from the population: we simply compute the fraction of the training observation that belong to the `kth` class. However, Estimating the density function $f_k(x)$ is much more challenging. To estimate $f_k(x)$, we will typically have to make some simplifying assumptions.

In the following sections, we discuss three classifiers that use different estimates of $f_k(x)$ in (4.15) to approximate the Bayes classifier.
### 4.4.1 Linear Discriminant Analysis for p=1
Assume that p=1, To estimate $f_k(x)$, We will first make some assumptions about its form.
In particular, we assume that $f_k(x)$ is `normal`. In the one-dimensional setting.
$$ f_k(x)= \frac{1}{\sqrt{2\Pi\sigma_k}}exp(-\frac{1}{2\sigma_k^2}(x-\mu_k)^2) \qquad(4.16)$$
where $\mu_k$ and $\sigma_k^2$ are the mean and variance parameters for the `kth` class.
For now, let us assume that $\sigma_1^2=...=\sigma_K^2$ : that is, there is a shared variance term across all $K$ classes, which we will denote by $\sigma^2$. Plugging 4.16 into 4.15, we find that
$$ p_k(x) = \frac{
	\Pi_k
	\frac{1}{\sqrt{2\Pi\sigma_k}}exp(-\frac{1}{2\sigma_k^2}(x-\mu_k)^2)
	}{
		\sum_{l=1}^K\Pi_l \frac{1}{\sqrt{2\Pi\sigma_k}}exp(-\frac{1}{2\sigma_k^2}(x-\mu_k)^2)
	}  \qquad (4.17)$$
The Bayes classifier involves assigning an observation X=x to the class for which (4.17) is largest. Taking the log of (4.17), it is not hard to show that this is equivalent to assigning the observation to the class for which
$$
	\delta_k(x) = x.\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + log(\Pi_k)  \qquad (4.18)
$$
is largest.
For instance, if K = 2 and $\Pi_1=\Pi_2$, then the Bayes classifier assigns an observation to class 1 if $2x(\mu_1-\mu_2)>\mu_1^2-\mu_2^2$, and to class 2 otherwise.
The Bayes decision boundary is the point for which $\delta_1(x)=\delta_2(x)$ ;one can show that this amounts to
$$
	x = \frac{\mu_1^2-\mu_2^2}{2(\mu_1-\mu_2)} = \frac{\mu_1+\mu_2}{2} \qquad(4.19)
$$
![](https://i.imgur.com/wetfDee.png)
**(LDA)** method approximates the Bayes classifier by plugging estimates for $\Pi_k,\mu_k \,and\, \sigma^2$ into (4.18). In particular, the following estimates are used:
$$
\begin{align}
\hat{\mu_k} = \frac{1}{n_k}\sum_{i:y_i=k}x_i \qquad \qquad \qquad\\
\hat{\sigma^2} = \frac{1}{n-K}\sum_{k=1}^K\sum_{i:y_i=k}(x_i-\hat{\mu_k})^2 \qquad (4.20)
\end{align}
$$
where `n` is the total number of training observations, and $n_k$ is the number of training observations in the `kth` class. The estimate for $\mu_k$ is the avg of all training observations from the `kth` class, while $\hat{\sigma^2}$ can be seen as a weighted avg of the sample variance for each of the `K` classes. in the absence of any additional information, LDA estimates $\Pi_k$ using :
$$ \hat{\Pi_k} = \frac{n_k}{n} \qquad (4.21) $$
The LDA plugs the estimates given in 4.20 and 4.21 into 4.18, and assigns an observation X = x to the class for which
$$ 
	\hat{\delta}_k(x) = x . \frac{\hat{\mu}_k}{\hat{\sigma}^2} - \frac{\hat{\mu}_k^2}{2\hat{\sigma}^2} + log(\hat{\Pi}_k) \qquad (4.22) 
$$
is largest.

### 4.4.2 Linear Discriminant Analysis for p > 1
We will assume that $X=(X_1,...,X_p)$ is drawn from a `multivariate Gaussian` distribution.
**Brief review:** The `multi.gauss.dist` assumes that each individual predictor follows a one-dimensional normal distribution. as in 4.16, with some correlation between each pair of predictors.
![](https://i.imgur.com/VqltmhT.png)
To indicate that a p-dimensional r.v X has a `multi.gauss.dist` we write $X~N(\mu,\sum)$. Here $E(X)=\mu$ and $Cov(X)=\sum$ the $p.p$ covariance matrix of X. The multivariate Gaussian density is defined as
$$
	f(x) = \frac{1}{(2\Pi)^{p/2}|\sum|^{1/2}}exp(-\frac{1}{2}(x-\mu)^T\sum^{-1}(x-\mu)). \qquad (4.23)
$$
In the case of $p>1$, LDA classifier assumes that the observations in the `kth` class are drawn from a multivariate Gaussian distribution $N(\mu_k,\sum)$, where $\sum$ is a covariance matrix that is common to all $K$ classes.
Plugging $f_k(X=x)$ into 4.15 reveals that the Bayes classifier assigns an observation X = x to the class for which
$$
	\delta_k(x) = x^T\sum^{-1}\mu_k - \frac{1}{2}\mu_k^T\sum^{-1} + log\Pi_k \qquad (4.24)
$$
is largest. Note that $\delta_k(x)$ is a linear function of x; that is, the LDA decision rule depends on $x$ only through a linear combination of its elements. and this is the reason for the word `linear` in LDA.

#### ROC: 
The ROC curve is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds. 
![](https://i.imgur.com/0YkTZD7.png)

The overall performance of a classifier, summarized over all possible thresholds, is given by `the area under the ROC curve (AUC)`. An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier.
ROC curvers are useful for comparing different classifiers, since they take in account all possible thresholds.
As we have seen above, varying the classifier threshold changes its true positive and false positive rate. These are also called the  `sensitivity` and one minus the `specificity` of our classifier.

![](https://i.imgur.com/PVEpecD.png)

### 4.4.3 Quadratic Discriminant Analysis.

QDA classifier results from assuming that the observations from each class are drown form a Gaussian distribution, and plugging estimates for the parameters into Bayes' theorem in order to perform prediction.
QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the `kth` class is of the form $X~N(\mu_k, \sum_k)$ , where $\sum_k$ is the covariance matrix for the `kth` class. Under this assumption the Bayes classifier assigns an observation X = x to the class for which
$$
	\delta_k(x)=\frac{-1}{2}x^T\sum_k^{-1}x+x^T\sum_k^{-1}\mu_k-\frac{-1}{2}\mu_k^T\sum_k^{-1}\mu_k-\frac{-1}{2}log|\sum_k|+log\Pi_k \qquad (4.28)
$$
is `largest`. So the QDA classifier involves plugging estimates for $\sum_k,\mu_k\,and\,\Pi_k$ into 4.28, and then assigning an observation X =x to the class for which quantity is largest.

**Why does it  matter whether or not we assume that the K classes share a common covariance matrix? Why LDA over QDA or vice-versa.**  he answer lies in the bias-variance trade-off. When there are p predictors, then estimating a covariance matrix requires estimating p(p+1)/2 parameters. QDA estimates a separate covariance matrix
for each class, for a total of Kp(p+1)/2 parameters. With 50 predictors this
is some multiple of 1,275, which is a lot of parameters. By instead assum-
ing that the K classes share a common covariance matrix, the LDA model
becomes linear in x, which means there are Kp linear coefficients to esti-
mate. Consequently, LDA is a much less flexible classifier than QDA, and
so has substantially lower variance. This can potentially lead to improved
prediction performance. But there is a trade-off: if LDAâ€™s assumption that
the K classes share a common covariance matrix is badly off, then LDA
can suffer from high bias. Roughly speaking, LDA tends to be a better bet
than QDA if there are relatively few training observations and so reducing
variance is crucial. In contrast, QDA is recommended if the training set is
very large, so that the variance of the classifier is not a major concern, or if
the assumption of a common covariance matrix for the K classes is clearly
untenable.

![](https://i.imgur.com/uGYcwXX.png)

### 4.4.4 Naive Bayes.
In the naive Bayes classifier, Instead of assuming that these functions belong to a particular family of distributions, we instead make a single assumption:
		Within the `kth` class, the `p` predictors are independent. which means:
		$$ f_k(x)= f_{k1}(x_1).f_{k2}(x_2)...f_{kp}(x_p) \qquad (4.29) $$
		for $k=1,...,K$ where $f_{kj}$ is the density function of the $jth$ predictor among observations in the $kth$ class.

**Do we really believe the naive Bayes assumption that the p covariates are independent withinn each class?** In most settings, we do not. But even though this modeling assumption is made for convenience, it often leads to pretty decent results, especially in settings where $n$ is not large enough relative to $p$ for us to effectively estimate the joint distribution of the predictors within each class.
In fact, since estimating a joint distribution requires such a huge amount of data, naive Bayes is a good choice in a wide range of settings. Essentially, the naive Bayes assumption introduces some bias, but reduces variance, leading to a classifier that works quite well in practice as a result of the bias-variance trade-off.

Once we have made the naive Bayes assumption, we can plug (4.29) into (4.15) to obtain an expression for the posterior probability
$$
	Pr(Y=k|X=x)=\frac{
		\Pi_kf_{k1}(x_1)*...*f_{kp}(x_p)
	} {
		\sum_{l=1}^K\Pi_lf_{l1}(x_1)*...*f_{lp}(x_p)
	} \qquad (4.30)
$$
for $k=1,...,K.$

To estimate the one-dimensional density function $f_{kj}$ using training data $x_{1j},...x_{nj}$, we have a few options:
- If $X_j$ is quantitative, then we can assume that $X_j|Y=k~~N(\mu_{jk}, \sigma_{jk}^2)$  In other words, we assume that within each class, the $jth$ predictor is drawn from a (univariate) normal distribution.
- If $X_j$ is quantitative, then another option is to use a non-parametric estimate for $f_{kj}$. A very simple way to do this is by making a histogram for the observations of the $jth$ predictor within each class. Then we can estimate $f_{kj}(x_j)$ as the fraction of the training observations in the $kth$ class that belong to the same histogram bin as $x_j$ . Alternatively, we can use a `kernel density estimator`, which is kernel essentially a smoothed version of a histogram.
- If $X_j$ is qualitative, then we can simply count the proportion of training observations for the $jth$ predictor corresponding to each class. 

**NOTE:** The reduction in variance resulting from the naive Bayes assumption is not necessarily worthwhile. We expect to see a greater pay-off to using naive Bayes relative to LDA or QDA in instances where $p$ is larger or $n$ is smaller, so that reducing the variance is very important.


