## 4.5.1 An Analytical Comparison
We consider these approaches in a setting with K classes, so that we assign an observation to the class that maximizes $Pr(Y=x|X=x)$. Equivalently, we can set $K$ as the `baseline` class and assign an observation to the class that maximizes
$$
	log(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}) \qquad (4.31)
$$
for $k=1,...,K$. (4.31) provides a clear understanding of their similarities and differences.
First, for LDA, using Bayes' theorem (4.15), and the assumption of `multi.norm.density` (4.23) we show that 
$$
	log(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}) = a_k + \sum_{j=1}^pb_{kj}x_j \qquad (4.32)
$$
where $a_k = log(\Pi_k/\Pi_K) - \frac{1}{2}(\mu_k+\mu_K)^T\sum^{-1}(\mu_k-\mu_K)$ and $b_{kj}$ is the $jth$ component of $\sum^{-1}(\mu_k-\mu_K)$. 

In the QDA setting becomes 
$$
	log(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}) = a_k + \sum_{j=1}^pb_{kj}x_j + \sum_{j=1}^p\sum_{l=1}^pc_{kjl}x_jx_l \qquad (4.33)
$$
where $a_k,b_{kj}$ and $c_{kjl}$ are function of $\Pi_k,\Pi_K,\mu_k,\mu_K,\sum_k\,and\,\sum_K$.

For naive Bayes
$$
log(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}) = a_k + \sum_{j=1}^pg_{kj}(x_j) \qquad (4.34)
$$
where $a_k=log(\frac{\Pi_k}{\Pi_K})$ and $g_{kj}=log(\frac{f_{kj}(x_j)}{f_{Kj}(x_j)})$. Hence the right-hand side of 4.34 takes the form of a `generalized additive models`.

(4.32), (4.33), and (4.34) yields the following observations:
- LDA is a special case of QDA with $c_{kjl}=0$.
- Any classifier with a linear decision boundary is a special case of naive Bayes with $g_{kj}(x_j)=b_{kj}x_j$.
- Neither QDA nor naive Bayes is a special case of the other. Naive Bayes can produce a more flexible fit. However, it is restricted to a purely additive fit. Where QDA has the potential to be more accurate in settings where interactions among the predictors are important in discriminating between classes.
##### Note:
The choice of method will depend on the true distribution of the predictors in each of the K classes, as well as other considerations, such as the values of n and p, The latter ties into the bias-variance trade-off.
##### Note:
we expect LDA to outperform logistic regression when the normality assumption holds, and we expect logistic regression to perform better when it does not.

#### KNN
- Because KNN is non-parametric, we can expect this approach to dominate LDA and logistic regression when the decision boundary is highly non-linear, provided that n is very large and p is small.
- KNN requires a lot of observation relative to the number of predictors. This has to do with the fact that KNN is non-parametric, and thus tends to reduce the bias while incurring a lot of variance.
- In settings where the decision boundary is non-linear but n is only modes, or p is not very small, then QDA may be preferred to KNN. This is because QDA can provide a non-linear decision boundary.
- Unlike logistic regression KNN does not tell us which predictors are important.
## 4.5.2 An Empirical (practical) Comparison
we have six different scenarios each with two-class,  Three scenarios with as linear Bayes decision boundary, and the remaining are non-linear. For each scenario we produced 100 random training data sets. In each of the six scenarios there were $p=2$.

![](https://i.imgur.com/9HS38ol.png)
![](https://i.imgur.com/qJdh4M5.png)

**<mark>For the full explanation get back to the book.</mark>

**Recall** from chapter 3 that in the regression setting we can accommodate a non-linear relationship between the predictors and the response by performing regression using transformations of the predictors. A similar approach could be taken in the classification setting. For instance, we could create a more flexible version of logistic regression by including $X^2, X^3,\,and\,even\,X^4$ as  predictors.This may or may not increase in variance due to the added flexibility is offset by a sufficiently large reduction in bias. We could do the same for LDA. if we added all possible quadratic terms and cross-products to LDA. the form of the model will be the same as the QDA model. although the parameter estimates would be different. This device allows us to move somewhere between an LDA and a QDA model.